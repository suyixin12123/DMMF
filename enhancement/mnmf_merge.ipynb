{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie: 206\n",
      "user: 3550\n",
      "director: 194\n",
      "genre: 20\n",
      "topic: 50\n",
      "cast: 474\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nMOVIE_NUM = 1682\\nDIRECTOR_NUM = 1139\\nGENRE_NUM = 24\\nTOPIC_NUM = 50\\nCAST_NUM = 2894\\nUSER_NUM = 943\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Global values\n",
    "\"\"\"\n",
    "folder = 'boxoffice_10'\n",
    "\n",
    "len_dtcgum = pickle.load(open('../data/'+folder+'/len_dtcgum.pkl', 'rb'))\n",
    "\n",
    "FEATURE_LEN = 5\n",
    "\n",
    "MOVIE_NUM = len_dtcgum[5]\n",
    "DIRECTOR_NUM = len_dtcgum[0]\n",
    "GENRE_NUM = len_dtcgum[3]\n",
    "TOPIC_NUM = 50\n",
    "CAST_NUM = len_dtcgum[2]\n",
    "USER_NUM = len_dtcgum[4]\n",
    "\n",
    "print('movie:', MOVIE_NUM)\n",
    "print('user:', USER_NUM)\n",
    "print('director:', DIRECTOR_NUM)\n",
    "print('genre:', GENRE_NUM)\n",
    "print('topic:', TOPIC_NUM)\n",
    "print('cast:', CAST_NUM)\n",
    "\n",
    "\"\"\"\n",
    "MOVIE_NUM = 1682\n",
    "DIRECTOR_NUM = 1139\n",
    "GENRE_NUM = 24\n",
    "TOPIC_NUM = 50\n",
    "CAST_NUM = 2894\n",
    "USER_NUM = 943\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movie_rating = pickle.load(open('../data/'+folder+'/movie_rating.pkl', 'rb'))\n",
    "movie_rating_test = pickle.load(open('../data/'+folder+'/movie_rating_test.pkl', 'rb'))\n",
    "movie_rating_validation = pickle.load(open('../data/'+folder+'/movie_rating_validation.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "def load_data(rating_file):\n",
    "    user_list = []\n",
    "    movie_list = []\n",
    "    rating_list = []\n",
    "    for movie_id in rating_file:\n",
    "        for tup in rating_file[movie_id]:\n",
    "            user_id = tup[0]\n",
    "            rating = tup[1]\n",
    "            user_list.append(user_id)\n",
    "            movie_list.append(movie_id)\n",
    "            rating_list.append(rating)\n",
    "    \n",
    "    return user_list, movie_list, rating_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load training data\n",
    "total_train_user, total_train_movie, total_train_rating = load_data(movie_rating)\n",
    "total_valid_user, total_valid_movie, total_valid_rating = load_data(movie_rating_validation)\n",
    "total_test_user, total_test_movie, total_test_rating = load_data(movie_rating_test)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load validation and test data\n",
    "total_train_user_v = []\n",
    "total_train_movie_v = []\n",
    "total_train_rating_v = []\n",
    "for movie_id in movie_rating:\n",
    "    for tup in movie_rating[movie_id]:\n",
    "        user_id = tup[0]\n",
    "        rating = tup[1]\n",
    "        total_train_user.append(user_id)\n",
    "        total_train_movie.append(movie_id)\n",
    "        total_train_rating.append(rating)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(user_name, movie_name, rating_name, last_same):\n",
    "    user = pickle.load(open('../data/'+folder+'/'+user_name+last_same+'.pkl', 'rb'))\n",
    "    movie = pickle.load(open('../data/'+folder+'/'+movie_name+last_same+'.pkl', 'rb'))\n",
    "    rating = pickle.load(open('../data/'+folder+'/'+rating_name+last_same+'.pkl', 'rb'))\n",
    "    return shuffle(user, movie, rating)\n",
    "\n",
    "#load training data\n",
    "d_user, d_movie, d_rating = load_data(\"train_user\", \"train_movie\", \"train_rating\", '_d')\n",
    "t_user, t_movie, t_rating = load_data(\"train_user\", \"train_movie\", \"train_rating\", '_t')\n",
    "g_user, g_movie, g_rating = load_data(\"train_user\", \"train_movie\", \"train_rating\", '_g')   \n",
    "c_user, c_movie, c_rating = load_data(\"train_user\", \"train_movie\", \"train_rating\", '_c')\n",
    "\n",
    "#load validation data\n",
    "d_user_v, d_movie_v, d_rating_v = load_data(\"train_user\", \"train_movie\", \"train_rating\", '_d_v')\n",
    "t_user_v, t_movie_v, t_rating_v = load_data(\"train_user\", \"train_movie\", \"train_rating\", '_t_v')\n",
    "g_user_v, g_movie_v, g_rating_v = load_data(\"train_user\", \"train_movie\", \"train_rating\", '_g_v')        \n",
    "c_user_v, c_movie_v, c_rating_v = load_data(\"train_user\", \"train_movie\", \"train_rating\", '_c_v')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nminvalue = 0\\nmaxvalue = 10\\nU = tf.clip_by_value(U, minvalue, maxvalue)\\nD = tf.clip_by_value(D, minvalue, maxvalue)\\nT = tf.clip_by_value(T, minvalue, maxvalue)\\nG = tf.clip_by_value(G, minvalue, maxvalue)\\nC = tf.clip_by_value(C, minvalue, maxvalue)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U = tf.Variable(initial_value=tf.truncated_normal([USER_NUM, FEATURE_LEN]), name='users')\n",
    "D = tf.Variable(initial_value=tf.truncated_normal([FEATURE_LEN, DIRECTOR_NUM]), name='directors')\n",
    "T = tf.Variable(initial_value=tf.truncated_normal([FEATURE_LEN, TOPIC_NUM]), name='topics')\n",
    "G = tf.Variable(initial_value=tf.truncated_normal([FEATURE_LEN, GENRE_NUM]), name='genures')\n",
    "C = tf.Variable(initial_value=tf.truncated_normal([FEATURE_LEN, CAST_NUM]), name='casts')\n",
    "\n",
    "\"\"\"\n",
    "minvalue = 0\n",
    "maxvalue = 10\n",
    "U = tf.clip_by_value(U, minvalue, maxvalue)\n",
    "D = tf.clip_by_value(D, minvalue, maxvalue)\n",
    "T = tf.clip_by_value(T, minvalue, maxvalue)\n",
    "G = tf.clip_by_value(G, minvalue, maxvalue)\n",
    "C = tf.clip_by_value(C, minvalue, maxvalue)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate the in the film matrix\n",
    "\n",
    "### if the information is in the movie, the value will be 1, otherwise will be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_result = tf.matmul(U, D)\n",
    "t_result = tf.matmul(U, T)\n",
    "g_result = tf.matmul(U, G)\n",
    "c_result = tf.matmul(U, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MatMul:0' shape=(3550, 194) dtype=float32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_var = tf.Variable(initial_value=tf.random_normal([USER_NUM, DIRECTOR_NUM], mean=0,stddev=0.05), name='d_var')\n",
    "t_var = tf.Variable(initial_value=tf.random_normal([USER_NUM, TOPIC_NUM], mean=0, stddev=0.05), name='t_var')\n",
    "g_var = tf.Variable(initial_value=tf.random_normal([USER_NUM, GENRE_NUM], mean=0,stddev=0.05), name='g_var')\n",
    "c_var = tf.Variable(initial_value=tf.random_normal([USER_NUM, CAST_NUM], mean=0,stddev=0.05), name='c_var')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_result = d_result + d_var\n",
    "t_result = t_result + t_var\n",
    "g_result = g_result + g_var\n",
    "c_result = c_result + c_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movie_feature_dict = pickle.load(open('../data/'+folder+'/movie_feature_dict.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#build is_in_matrix, if feature is in the movie, the value in matrix will be 1, otherwise will be 0\n",
    "d_isin = np.zeros([MOVIE_NUM+1, DIRECTOR_NUM])\n",
    "t_isin = np.zeros([MOVIE_NUM+1, TOPIC_NUM])\n",
    "c_isin = np.zeros([MOVIE_NUM+1, CAST_NUM])\n",
    "g_isin = np.zeros([MOVIE_NUM+1, GENRE_NUM])\n",
    "\n",
    "for key in movie_feature_dict:\n",
    "    for f_id in movie_feature_dict[key]['d']:\n",
    "        d_isin[key,f_id] = 1\n",
    "    \n",
    "    for f_id in movie_feature_dict[key]['t']:\n",
    "        t_isin[key,f_id] = 1\n",
    "    \n",
    "    for f_id in movie_feature_dict[key]['c']:\n",
    "        c_isin[key,f_id] = 1\n",
    "    \n",
    "    for f_id in movie_feature_dict[key]['g']:\n",
    "        g_isin[key,f_id] = 1\n",
    "    \n",
    "d_isin = tf.constant(d_isin, tf.float32)\n",
    "t_isin = tf.constant(t_isin, tf.float32)\n",
    "c_isin = tf.constant(c_isin, tf.float32)\n",
    "g_isin = tf.constant(g_isin, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#md_var = tf.Variable(initial_value=tf.truncated_normal([MOVIE_NUM+1, DIRECTOR_NUM]), name='md_var')\n",
    "#mt_var = tf.Variable(initial_value=tf.truncated_normal([MOVIE_NUM+1, TOPIC_NUM]), name='mt_var')\n",
    "#mg_var = tf.Variable(initial_value=tf.truncated_normal([MOVIE_NUM+1, GENRE_NUM]), name='mg_var')\n",
    "#mc_var = tf.Variable(initial_value=tf.truncated_normal([MOVIE_NUM+1, CAST_NUM]), name='mc_var')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "md_var = tf.Variable(initial_value=tf.random_normal([MOVIE_NUM+1, DIRECTOR_NUM], mean=1,stddev=0.05), name='md_var')\n",
    "mt_var = tf.Variable(initial_value=tf.random_normal([MOVIE_NUM+1, TOPIC_NUM], mean=1,stddev=0.05), name='mt_var')\n",
    "mg_var = tf.Variable(initial_value=tf.random_normal([MOVIE_NUM+1, GENRE_NUM], mean=1,stddev=0.05), name='mg_var')\n",
    "mc_var = tf.Variable(initial_value=tf.random_normal([MOVIE_NUM+1, CAST_NUM], mean=1,stddev=0.05), name='mc_var')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_isin = d_isin * md_var\n",
    "t_isin = t_isin * mt_var\n",
    "g_isin = g_isin * mg_var\n",
    "c_isin = c_isin * mc_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count how many feature in each movie\n",
    "movie_feature_num = np.zeros([MOVIE_NUM+1,1])\n",
    "for key in movie_feature_dict:\n",
    "    total_num = 0\n",
    "    for f_key in movie_feature_dict[key]:\n",
    "        total_num += len(movie_feature_dict[key][f_key])\n",
    "    movie_feature_num[key] = 1/total_num\n",
    "feature_num_tensor = tf.tile(movie_feature_num, [1, USER_NUM])\n",
    "feature_num_tensor = tf.cast(feature_num_tensor, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Cast:0' shape=(207, 3550) dtype=float32>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_num_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_nm = tf.matmul(d_result, tf.transpose(d_isin))\n",
    "t_nm = tf.matmul(t_result, tf.transpose(t_isin))\n",
    "c_nm = tf.matmul(c_result, tf.transpose(c_isin))\n",
    "g_nm = tf.matmul(g_result, tf.transpose(g_isin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add:0' shape=(3550, 194) dtype=float32>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Total_Rate = (d_nm + t_nm + c_nm + g_nm)*tf.transpose(feature_num_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clip_U = U.assign(tf.maximum(tf.zeros_like(U), U))\n",
    "clip_D = D.assign(tf.maximum(tf.zeros_like(D), D))\n",
    "clip_T = T.assign(tf.maximum(tf.zeros_like(T), T))\n",
    "clip_C = C.assign(tf.maximum(tf.zeros_like(C), C))\n",
    "clip_G = G.assign(tf.maximum(tf.zeros_like(G), G))\n",
    "clip = tf.group(clip_U, clip_D,clip_T,clip_C,clip_G)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_result_flatten = tf.reshape(Total_Rate, [-1])\n",
    "TR_R = tf.gather(total_result_flatten, total_train_user * tf.shape(Total_Rate)[1] + total_train_movie, name='total_user_rate')\n",
    "TR_R_test = tf.gather(total_result_flatten, total_test_user * tf.shape(Total_Rate)[1] + total_test_movie, name='total_user_rate_test')\n",
    "TR_R_valid = tf.gather(total_result_flatten, total_valid_user * tf.shape(Total_Rate)[1] + total_valid_movie, name='total_user_rate_valid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "d_result_flatten = tf.reshape(d_result, [-1])\n",
    "t_result_flatten = tf.reshape(t_result, [-1])\n",
    "g_result_flatten = tf.reshape(g_result, [-1])\n",
    "c_result_flatten = tf.reshape(c_result, [-1])\n",
    "\n",
    "D_R = tf.gather(d_result_flatten, d_user * tf.shape(d_result)[1] + d_movie, name='director_user_rate')\n",
    "T_R = tf.gather(t_result_flatten, t_user * tf.shape(t_result)[1] + t_movie, name='topic_user_rate')\n",
    "G_R = tf.gather(g_result_flatten, g_user * tf.shape(g_result)[1] + g_movie, name='genre_user_rate')\n",
    "C_R = tf.gather(c_result_flatten, c_user * tf.shape(c_result)[1] + c_movie, name='cast_user_rate')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_diff_op = tf.subtract(TR_R, total_train_rating, name='total_trainig_diff')\n",
    "#total_diff_op_squared = tf.abs(total_diff_op, name=\"total_squared_difference\")\n",
    "#total_cost = tf.reduce_sum(total_diff_op_squared, name=\"total_sum_squared_error\")/10000 \n",
    "\n",
    "total_cost = tf.nn.l2_loss(total_diff_op)\n",
    "\n",
    "\n",
    "d_diff_op = tf.subtract(D_R, d_rating, name='d_trainig_diff')\n",
    "t_diff_op = tf.subtract(T_R, t_rating, name='t_trainig_diff')\n",
    "g_diff_op = tf.subtract(G_R, g_rating, name='g_trainig_diff')\n",
    "c_diff_op = tf.subtract(C_R, c_rating, name='c_trainig_diff')\n",
    "\n",
    "d_diff_op_squared = tf.abs(d_diff_op, name=\"d_squared_difference\")\n",
    "t_diff_op_squared = tf.abs(t_diff_op, name=\"t_squared_difference\")\n",
    "g_diff_op_squared = tf.abs(g_diff_op, name=\"g_squared_difference\")\n",
    "c_diff_op_squared = tf.abs(c_diff_op, name=\"c_squared_difference\")\n",
    "\n",
    "d_base_cost = tf.reduce_sum(d_diff_op_squared, name=\"d_sum_squared_error\")/ tf.cast(tf.shape(d_diff_op_squared)[0], tf.float32) * 10000 \n",
    "t_base_cost = tf.reduce_sum(t_diff_op_squared, name=\"t_sum_squared_error\")/ tf.cast(tf.shape(t_diff_op_squared)[0], tf.float32) * 10000\n",
    "g_base_cost = tf.reduce_sum(g_diff_op_squared, name=\"g_sum_squared_error\")/ tf.cast(tf.shape(g_diff_op_squared)[0], tf.float32) * 10000\n",
    "c_base_cost = tf.reduce_sum(c_diff_op_squared, name=\"c_sum_squared_error\")/ tf.cast(tf.shape(c_diff_op_squared)[0], tf.float32) * 10000\n",
    "\n",
    "base_cost = d_base_cost + t_base_cost + g_base_cost + c_base_cost + total_cost/10\n",
    "#base_cost = total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = tf.constant(.001, name='lambda')\n",
    "\n",
    "\"\"\"\n",
    "u_norm = tf.reduce_sum(tf.abs(U, name='user_abs'), name='user_norm') / tf.cast(tf.shape(U)[0], tf.float32)\n",
    "d_norm = tf.reduce_sum(tf.abs(D, name='director_abs'), name='director_norm')/ tf.cast(tf.shape(D)[1], tf.float32)\n",
    "t_norm = tf.reduce_sum(tf.abs(T, name='topic_abs'), name='topic_norm')/ tf.cast(tf.shape(T)[1], tf.float32)\n",
    "g_norm = tf.reduce_sum(tf.abs(G, name='genre_abs'), name='genre_norm')/ tf.cast(tf.shape(G)[1], tf.float32)\n",
    "c_norm = tf.reduce_sum(tf.abs(C, name='cast_abs'), name='cast_norm')/ tf.cast(tf.shape(C)[1], tf.float32)\n",
    "\"\"\"\n",
    "u_norm = tf.reduce_sum(tf.abs(U, name='user_abs'), name='user_norm') \n",
    "d_norm = tf.reduce_sum(tf.abs(D, name='director_abs'), name='director_norm')\n",
    "t_norm = tf.reduce_sum(tf.abs(T, name='topic_abs'), name='topic_norm')\n",
    "g_norm = tf.reduce_sum(tf.abs(G, name='genre_abs'), name='genre_norm')\n",
    "c_norm = tf.reduce_sum(tf.abs(C, name='cast_abs'), name='cast_norm')\n",
    "\n",
    "norm_sums = u_norm + d_norm + t_norm + g_norm + c_norm\n",
    "regularizer = tf.multiply(norm_sums, lda, 'regularizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cost = tf.add(base_cost, regularizer)\n",
    "cost = base_cost + regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = tf.constant(.0001, name='learning_rate')\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = tf.train.exponential_decay(lr, global_step, 10000, 0.96, staircase=True)\n",
    "#learning_rate = 0.01\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "training_step = optimizer.minimize(cost, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(TR_R, total_train_rating))))\n",
    "rmse_test = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(TR_R_test, total_test_rating))))\n",
    "rmse_valid = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(TR_R_valid, total_valid_rating))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 250788.0 15.4658 106714.3625 3.66469 3.66284 3.64961\n",
      "1 187810.0 7.77584 72815.5625 3.02718 3.03681 3.0077\n",
      "2 183533.0 7.80483 69992.8875 2.96792 2.97744 2.94749\n",
      "3 179423.0 7.83454 67298.1625 2.91023 2.91964 2.88891\n",
      "4 175480.0 7.8649 64724.9 2.85405 2.86333 2.83192\n",
      "5 171706.0 7.89582 62270.2125 2.79941 2.80856 2.77654\n",
      "6 168109.0 7.92723 59933.1875 2.74637 2.75539 2.72285\n",
      "7 164694.0 7.95905 57713.3125 2.69503 2.7039 2.67093\n",
      "8 161464.0 7.99116 55611.21875 2.64549 2.65422 2.62091\n",
      "9 158407.0 8.02354 53619.29375 2.59768 2.60626 2.5727\n",
      "10 155532.0 8.05605 51739.2625 2.55173 2.56016 2.52644\n",
      "11 152833.0 8.08866 49968.3875 2.50768 2.51596 2.48216\n",
      "12 150300.0 8.12129 48302.2375 2.46552 2.47365 2.43986\n",
      "13 147920.0 8.15387 46734.278125 2.42518 2.43315 2.39946\n",
      "14 145681.0 8.18639 45258.63125 2.38658 2.39441 2.3609\n",
      "15 143575.0 8.21878 43870.921875 2.3497 2.35741 2.32414\n",
      "16 141589.0 8.25101 42565.33125 2.31448 2.32205 2.28909\n",
      "17 139713.0 8.28304 41337.0 2.28084 2.2883 2.2557\n",
      "18 137939.0 8.31484 40180.6875 2.24871 2.25607 2.22389\n",
      "19 136255.0 8.34642 39090.80625 2.218 2.22528 2.19356\n",
      "20 134656.0 8.37772 38064.4125 2.18869 2.19589 2.16468\n",
      "21 133133.0 8.40876 37095.7 2.16066 2.1678 2.13713\n",
      "22 131678.0 8.43951 36180.809375 2.13386 2.14095 2.11084\n",
      "23 130288.0 8.46997 35316.434375 2.10821 2.11528 2.08577\n",
      "24 128955.0 8.50014 34498.753125 2.08366 2.09072 2.06182\n",
      "25 127674.0 8.53 33723.74375 2.06012 2.06719 2.03892\n",
      "26 126443.0 8.55956 32989.2 2.03757 2.04465 2.01702\n",
      "27 125255.0 8.5888 32291.740625 2.01591 2.02304 1.99604\n",
      "28 124107.0 8.61776 31628.0625 1.99509 2.00228 1.97592\n",
      "29 122994.0 8.64643 30995.4625 1.97503 1.9823 1.95658\n",
      "30 121915.0 8.67482 30391.75 1.9557 1.96307 1.93798\n",
      "31 120866.0 8.70289 29815.265625 1.93707 1.94454 1.92007\n",
      "32 119846.0 8.73068 29263.6 1.91906 1.92666 1.90281\n",
      "33 118852.0 8.75818 28735.346875 1.90166 1.9094 1.88616\n",
      "34 117881.0 8.78541 28228.725 1.88482 1.89271 1.87007\n",
      "35 116932.0 8.81236 27741.8625 1.8685 1.87655 1.85449\n",
      "36 116004.0 8.83904 27273.471875 1.85266 1.86089 1.83939\n",
      "37 115094.0 8.86547 26822.046875 1.83726 1.84569 1.82473\n",
      "38 114201.0 8.89164 26386.625 1.82229 1.83092 1.81049\n",
      "39 113325.0 8.91759 25965.9875 1.80771 1.81655 1.79664\n",
      "40 112465.0 8.94328 25559.7609375 1.79351 1.80258 1.78317\n",
      "41 111619.0 8.96873 25166.7875 1.77967 1.78897 1.77005\n",
      "42 110785.0 8.99397 24785.3875 1.76613 1.77568 1.75722\n",
      "43 109964.0 9.01899 24415.740625 1.75291 1.76272 1.74471\n",
      "44 109155.0 9.04381 24056.634375 1.73997 1.75004 1.73247\n",
      "45 108356.0 9.06843 23707.675 1.72731 1.73764 1.72049\n",
      "46 107567.0 9.09285 23367.9890625 1.71489 1.7255 1.70876\n",
      "47 106788.0 9.11709 23037.1921875 1.70271 1.7136 1.69725\n",
      "48 106019.0 9.14114 22714.86875 1.69076 1.70194 1.68596\n",
      "49 105259.0 9.165 22400.7984375 1.67902 1.6905 1.67489\n",
      "50 104508.0 9.18868 22094.4265625 1.6675 1.67928 1.66402\n",
      "51 103765.0 9.21219 21795.671875 1.65619 1.66827 1.65335\n",
      "52 103030.0 9.23555 21503.784375 1.64506 1.65745 1.64286\n",
      "53 102303.0 9.25874 21218.675 1.63412 1.64683 1.63255\n",
      "54 101583.0 9.28179 20939.825 1.62335 1.63637 1.62239\n",
      "55 100871.0 9.30467 20667.240625 1.61275 1.62609 1.61241\n",
      "56 100165.0 9.32742 20400.5515625 1.60231 1.61598 1.60258\n",
      "57 99466.7 9.35003 20139.6015625 1.59203 1.60603 1.5929\n",
      "58 98774.9 9.3725 19884.059375 1.5819 1.59623 1.58336\n",
      "59 98089.3 9.39484 19633.55625 1.5719 1.58658 1.57396\n",
      "60 97410.4 9.41704 19388.090625 1.56204 1.57706 1.56469\n",
      "61 96737.9 9.43911 19147.5609375 1.55232 1.56768 1.55555\n",
      "62 96072.0 9.46104 18911.8625 1.54274 1.55844 1.54654\n",
      "63 95412.2 9.48285 18680.75 1.53328 1.54934 1.53766\n",
      "64 94758.4 9.50455 18453.965625 1.52395 1.54035 1.52889\n",
      "65 94110.4 9.52611 18231.45 1.51473 1.53148 1.52023\n",
      "66 93468.7 9.54757 18013.103125 1.50563 1.52273 1.51169\n",
      "67 92833.2 9.5689 17798.9484375 1.49666 1.51411 1.50327\n",
      "68 92203.6 9.59012 17588.815625 1.4878 1.5056 1.49495\n",
      "69 91579.7 9.61123 17382.45 1.47904 1.4972 1.48674\n",
      "70 90961.5 9.63223 17179.828125 1.4704 1.4889 1.47863\n",
      "71 90349.1 9.65312 16980.9 1.46186 1.48071 1.47062\n",
      "72 89742.3 9.67389 16785.525 1.45342 1.47263 1.46271\n",
      "73 89141.2 9.69458 16593.5640625 1.44509 1.46464 1.4549\n",
      "74 88545.5 9.71517 16404.8875 1.43685 1.45675 1.44717\n",
      "75 87955.1 9.73566 16219.53125 1.42871 1.44895 1.43954\n",
      "76 87370.3 9.75603 16037.3578125 1.42066 1.44126 1.432\n",
      "77 86790.7 9.77633 15858.259375 1.41271 1.43365 1.42454\n",
      "78 86216.3 9.79651 15682.209375 1.40484 1.42613 1.41718\n",
      "79 85647.2 9.8166 15509.1625 1.39707 1.4187 1.40989\n",
      "80 85083.4 9.83659 15339.084375 1.38939 1.41136 1.4027\n",
      "81 84525.0 9.85648 15171.825 1.38179 1.40411 1.39559\n",
      "82 83971.5 9.87628 15007.309375 1.37428 1.39693 1.38856\n",
      "83 83423.4 9.89598 14845.4875 1.36686 1.38984 1.38161\n",
      "84 82880.2 9.91559 14686.2859375 1.3595 1.38282 1.37474\n",
      "85 82341.7 9.93511 14529.6625 1.35224 1.37589 1.36794\n",
      "86 81807.8 9.95454 14375.5140625 1.34505 1.36902 1.36121\n",
      "87 81278.8 9.97388 14223.965625 1.33794 1.36224 1.35457\n",
      "88 80754.6 9.99312 14074.778125 1.3309 1.35554 1.34799\n",
      "89 80235.2 10.0123 13928.103125 1.32395 1.34891 1.34149\n",
      "90 79720.5 10.0313 13783.8203125 1.31707 1.34235 1.33506\n",
      "91 79210.6 10.0503 13641.825 1.31027 1.33587 1.3287\n",
      "92 78705.9 10.0692 13502.1 1.30354 1.32946 1.32241\n",
      "93 78205.9 10.088 13364.440625 1.29688 1.3231 1.31618\n",
      "94 77711.0 10.1067 13228.8875 1.29029 1.31682 1.31002\n",
      "95 77220.6 10.1253 13095.5265625 1.28377 1.31061 1.30393\n",
      "96 76735.1 10.1438 12964.1984375 1.27732 1.30446 1.2979\n",
      "97 76254.2 10.1622 12834.859375 1.27093 1.29837 1.29193\n",
      "98 75777.8 10.1805 12707.4875 1.26461 1.29234 1.28602\n",
      "99 75305.9 10.1988 12582.0070312 1.25835 1.28638 1.28017\n",
      "100 74838.4 10.217 12458.475 1.25215 1.28047 1.27438\n",
      "101 74375.2 10.235 12336.7546875 1.24602 1.27463 1.26864\n",
      "102 73916.5 10.253 12216.8515625 1.23995 1.26884 1.26297\n",
      "103 73462.2 10.271 12098.7671875 1.23394 1.26312 1.25736\n",
      "104 73012.2 10.2888 11982.4492188 1.228 1.25746 1.2518\n",
      "105 72566.8 10.3066 11867.9734375 1.22212 1.25186 1.24631\n",
      "106 72125.7 10.3242 11755.2734375 1.2163 1.24632 1.24087\n",
      "107 71688.7 10.3418 11644.3 1.21055 1.24084 1.2355\n",
      "108 71256.1 10.3593 11534.984375 1.20485 1.23541 1.23018\n",
      "109 70827.8 10.3767 11427.3109375 1.19921 1.23005 1.22491\n",
      "110 70403.6 10.394 11321.303125 1.19364 1.22474 1.2197\n",
      "111 69983.1 10.4112 11216.884375 1.18812 1.21949 1.21455\n",
      "112 69567.0 10.4283 11113.990625 1.18266 1.21429 1.20944\n",
      "113 69155.0 10.4454 11012.615625 1.17726 1.20914 1.2044\n",
      "114 68747.3 10.4623 10912.8125 1.17191 1.20405 1.1994\n",
      "115 68343.5 10.4792 10814.3976562 1.16661 1.19901 1.19445\n",
      "116 67943.7 10.496 10717.4625 1.16137 1.19402 1.18956\n",
      "117 67547.9 10.5128 10621.990625 1.15619 1.18908 1.18471\n",
      "118 67156.1 10.5294 10527.9171875 1.15105 1.1842 1.17991\n",
      "119 66768.3 10.5459 10435.2109375 1.14597 1.17936 1.17516\n",
      "120 66384.4 10.5624 10343.9132812 1.14095 1.17458 1.17046\n",
      "121 66004.2 10.5788 10254.0359375 1.13598 1.16984 1.16581\n",
      "122 65627.8 10.595 10165.5171875 1.13107 1.16516 1.16121\n",
      "123 65255.2 10.6113 10078.3390625 1.12621 1.16053 1.15666\n",
      "124 64886.5 10.6274 9992.4328125 1.1214 1.15595 1.15216\n",
      "125 64521.4 10.6434 9907.8296875 1.11664 1.15142 1.14771\n",
      "126 64160.0 10.6594 9824.446875 1.11193 1.14693 1.14331\n",
      "127 63802.5 10.6752 9742.2828125 1.10728 1.14249 1.13895\n",
      "128 63448.5 10.691 9661.334375 1.10267 1.13809 1.13464\n",
      "129 63098.2 10.7067 9581.590625 1.0981 1.13375 1.13037\n",
      "130 62751.4 10.7223 9503.0296875 1.09359 1.12945 1.12615\n",
      "131 62408.1 10.7378 9425.603125 1.08913 1.12519 1.12197\n",
      "132 62068.1 10.7533 9349.3 1.08471 1.12098 1.11784\n",
      "133 61731.4 10.7686 9274.0890625 1.08034 1.11681 1.11375\n",
      "134 61398.1 10.7839 9200.0125 1.07602 1.11269 1.10971\n",
      "135 61067.9 10.7991 9127.021875 1.07174 1.10861 1.10571\n",
      "136 60741.1 10.8142 9055.1 1.06751 1.10457 1.10175\n",
      "137 60417.7 10.8292 8984.1921875 1.06332 1.10058 1.09784\n",
      "138 60097.4 10.8442 8914.2671875 1.05918 1.09662 1.09396\n",
      "139 59780.1 10.8591 8845.3640625 1.05507 1.09271 1.09012\n",
      "140 59465.9 10.8739 8777.521875 1.05102 1.08884 1.08633\n",
      "141 59155.1 10.8886 8710.65546875 1.04701 1.08501 1.08258\n",
      "142 58847.3 10.9032 8644.759375 1.04304 1.08122 1.07886\n",
      "143 58542.4 10.9178 8579.825 1.03912 1.07747 1.07519\n",
      "144 58240.7 10.9322 8515.84296875 1.03523 1.07376 1.07156\n",
      "145 57942.0 10.9466 8452.8 1.0314 1.07009 1.06796\n",
      "146 57646.4 10.9609 8390.68359375 1.0276 1.06646 1.06441\n",
      "147 57353.7 10.9752 8329.4734375 1.02384 1.06287 1.06089\n",
      "148 57064.0 10.9893 8269.15703125 1.02013 1.05932 1.05741\n",
      "149 56777.3 11.0034 8209.76015625 1.01646 1.0558 1.05398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 56493.4 11.0174 8151.1625 1.01283 1.05233 1.05057\n",
      "151 56212.5 11.0313 8093.4 1.00923 1.04888 1.04721\n",
      "152 55934.5 11.0452 8036.446875 1.00567 1.04548 1.04387\n",
      "153 55659.2 11.0589 7980.3796875 1.00216 1.04211 1.04058\n",
      "154 55386.8 11.0726 7925.1359375 0.998684 1.03878 1.03732\n",
      "155 55117.0 11.0862 7870.72421875 0.995249 1.03549 1.0341\n",
      "156 54850.0 11.0998 7817.0921875 0.991852 1.03223 1.03092\n",
      "157 54585.6 11.1132 7764.21484375 0.988494 1.02901 1.02777\n",
      "158 54324.0 11.1266 7712.159375 0.985174 1.02583 1.02465\n",
      "159 54065.2 11.1399 7660.8390625 0.98189 1.02268 1.02157\n",
      "160 53808.9 11.1531 7610.28671875 0.978647 1.01957 1.01853\n",
      "161 53555.2 11.1663 7560.4734375 0.975436 1.01649 1.01552\n",
      "162 53304.0 11.1794 7511.37421875 0.972265 1.01345 1.01254\n",
      "163 53055.3 11.1924 7462.9734375 0.969127 1.01044 1.00959\n",
      "164 52808.9 11.2053 7415.3015625 0.966028 1.00746 1.00668\n",
      "165 52565.0 11.2182 7368.32734375 0.962962 1.00452 1.0038\n",
      "166 52323.6 11.231 7322.0515625 0.959934 1.00161 1.00095\n",
      "167 52084.6 11.2437 7276.4796875 0.956941 0.998733 0.998136\n",
      "168 51848.0 11.2563 7231.5625 0.953983 0.995887 0.995353\n",
      "169 51613.8 11.2689 7187.3515625 0.951063 0.993074 0.992602\n",
      "170 51381.8 11.2814 7143.7640625 0.948176 0.990292 0.989883\n",
      "171 51152.1 11.2938 7100.83671875 0.945322 0.987544 0.987197\n",
      "172 50924.7 11.3062 7058.5578125 0.942504 0.984825 0.984543\n",
      "173 50699.5 11.3185 7016.9203125 0.939719 0.98214 0.98192\n",
      "174 50476.5 11.3307 6975.8890625 0.936968 0.979486 0.979328\n",
      "175 50255.6 11.3429 6935.45 0.934249 0.976862 0.976764\n",
      "176 50037.0 11.355 6895.5734375 0.931558 0.974267 0.974228\n",
      "177 49820.5 11.3671 6856.2578125 0.928899 0.971698 0.97172\n",
      "178 49606.2 11.379 6817.53671875 0.926272 0.969162 0.969242\n",
      "179 49394.0 11.3909 6779.36796875 0.923674 0.966653 0.966793\n",
      "180 49184.1 11.4027 6741.75 0.92111 0.964171 0.964371\n",
      "181 48976.3 11.4145 6704.709375 0.918576 0.96172 0.96198\n",
      "182 48770.4 11.4262 6668.22265625 0.916073 0.959299 0.959617\n",
      "183 48566.6 11.4378 6632.2625 0.9136 0.956905 0.957282\n",
      "184 48364.8 11.4494 6596.834375 0.911156 0.954541 0.954977\n",
      "185 48165.0 11.4609 6561.9265625 0.908741 0.952204 0.952698\n",
      "186 47967.2 11.4723 6527.50703125 0.906355 0.949894 0.950442\n",
      "187 47771.2 11.4837 6493.590625 0.903999 0.94761 0.948214\n",
      "188 47577.2 11.495 6460.17773438 0.90167 0.945351 0.946012\n",
      "189 47385.1 11.5062 6427.25078125 0.899369 0.94312 0.943836\n",
      "190 47194.9 11.5174 6394.79414063 0.897095 0.940911 0.941687\n",
      "191 47006.5 11.5285 6362.821875 0.894849 0.938729 0.939565\n",
      "192 46820.0 11.5396 6331.30742187 0.89263 0.936573 0.937467\n",
      "193 46635.3 11.5506 6300.271875 0.89044 0.934443 0.935395\n",
      "194 46452.5 11.5615 6269.70078125 0.888277 0.932338 0.933348\n",
      "195 46271.5 11.5723 6239.56875 0.88614 0.930258 0.931325\n",
      "196 46092.3 11.5832 6209.87539063 0.884029 0.928205 0.929325\n",
      "197 45914.9 11.5939 6180.60976563 0.881944 0.926176 0.927345\n",
      "198 45739.1 11.6046 6151.75703125 0.879883 0.924172 0.925389\n",
      "199 45565.2 11.6152 6123.28320312 0.877844 0.922189 0.923453\n",
      "200 45392.9 11.6257 6095.24296875 0.875831 0.920229 0.921541\n",
      "201 45222.3 11.6362 6067.60117187 0.873843 0.918294 0.919651\n",
      "202 45053.4 11.6467 6040.3828125 0.87188 0.916382 0.917784\n",
      "203 44886.2 11.657 6013.54921875 0.869942 0.914493 0.915938\n",
      "204 44720.6 11.6674 5987.1171875 0.868029 0.912626 0.914117\n",
      "205 44556.8 11.6776 5961.09570312 0.866139 0.910782 0.912321\n",
      "206 44394.6 11.6879 5935.45898438 0.864276 0.90896 0.910548\n",
      "207 44233.9 11.698 5910.2046875 0.862434 0.907159 0.908797\n",
      "208 44074.6 11.7081 5885.32148438 0.860618 0.905382 0.907067\n",
      "209 43916.8 11.7182 5860.79609375 0.858823 0.903628 0.905357\n",
      "210 43760.5 11.7282 5836.61835938 0.857049 0.901893 0.903667\n",
      "211 43605.8 11.7382 5812.79140625 0.855298 0.90018 0.902\n",
      "212 43452.5 11.748 5789.31015625 0.853569 0.898488 0.900353\n",
      "213 43300.7 11.7579 5766.175 0.851861 0.896814 0.898726\n",
      "214 43150.4 11.7677 5743.36679687 0.850174 0.895161 0.897118\n",
      "215 43001.5 11.7774 5720.878125 0.848508 0.893528 0.895529\n",
      "216 42854.0 11.7871 5698.684375 0.846861 0.891912 0.893956\n",
      "217 42708.0 11.7967 5676.83046875 0.845235 0.890316 0.892405\n",
      "218 42563.2 11.8063 5655.27539062 0.843629 0.888739 0.890873\n",
      "219 42419.8 11.8159 5634.05507813 0.842045 0.88718 0.88936\n",
      "220 42277.7 11.8253 5613.1375 0.840481 0.885642 0.887869\n",
      "221 42136.9 11.8348 5592.50351562 0.838933 0.88412 0.886393\n",
      "222 41997.5 11.8441 5572.12734375 0.837405 0.882615 0.884934\n",
      "223 41859.4 11.8534 5552.03203125 0.835893 0.881126 0.883492\n",
      "224 41722.7 11.8627 5532.253125 0.834403 0.879656 0.88207\n",
      "225 41587.3 11.872 5512.78085937 0.832933 0.878206 0.880667\n",
      "226 41453.2 11.8811 5493.55078125 0.831479 0.876771 0.879278\n",
      "227 41320.3 11.8902 5474.6 0.830044 0.875353 0.877908\n",
      "228 41188.6 11.8993 5455.88984375 0.828625 0.873951 0.87655\n",
      "229 41058.1 11.9083 5437.43984375 0.827223 0.872568 0.875209\n",
      "230 40928.8 11.9173 5419.25546875 0.825838 0.871201 0.873885\n",
      "231 40800.6 11.9262 5401.3359375 0.824472 0.86985 0.872575\n",
      "232 40673.7 11.9351 5383.64375 0.82312 0.868517 0.871281\n",
      "233 40548.1 11.9439 5366.24453125 0.821788 0.8672 0.870006\n",
      "234 40423.5 11.9527 5349.0421875 0.82047 0.865898 0.868744\n",
      "235 40300.1 11.9615 5332.125 0.819172 0.864613 0.867503\n",
      "236 40177.8 11.9702 5315.44609375 0.81789 0.863343 0.866276\n",
      "237 40056.5 11.9788 5298.99140625 0.816623 0.862087 0.865064\n",
      "238 39936.4 11.9874 5282.7734375 0.815372 0.860847 0.863868\n",
      "239 39817.3 11.996 5266.76484375 0.814135 0.859622 0.862685\n",
      "240 39699.4 12.0045 5250.99960937 0.812916 0.858414 0.861519\n",
      "241 39582.6 12.013 5235.3984375 0.811707 0.857219 0.860363\n",
      "242 39466.9 12.0214 5220.05195313 0.810517 0.856041 0.859225\n",
      "243 39352.3 12.0298 5204.9515625 0.809344 0.854879 0.858105\n",
      "244 39238.6 12.0381 5190.034375 0.808183 0.853728 0.856996\n",
      "245 39126.0 12.0464 5175.34882813 0.807039 0.852592 0.855903\n",
      "246 39014.6 12.0547 5160.83046875 0.805907 0.851469 0.85482\n",
      "247 38904.1 12.0629 5146.58203125 0.804793 0.850364 0.853759\n",
      "248 38794.7 12.071 5132.46757812 0.803688 0.84927 0.852705\n",
      "249 38686.2 12.0791 5118.53125 0.802596 0.848189 0.851663\n",
      "250 38578.7 12.0872 5104.80390625 0.801518 0.847121 0.850634\n",
      "251 38472.2 12.0953 5091.2265625 0.800454 0.846064 0.849614\n",
      "252 38366.6 12.1033 5077.8578125 0.799402 0.845022 0.84861\n",
      "253 38262.0 12.1112 5064.621875 0.798359 0.843992 0.847613\n",
      "254 38158.4 12.1192 5051.6109375 0.797333 0.842976 0.846632\n",
      "255 38055.7 12.127 5038.775 0.796319 0.841974 0.845662\n",
      "256 37954.0 12.1349 5026.07773437 0.795315 0.84098 0.844701\n",
      "257 37853.1 12.1427 5013.5765625 0.794326 0.84 0.843755\n",
      "258 37753.3 12.1504 5001.1875 0.793344 0.839029 0.842814\n",
      "259 37654.3 12.1581 4989.03671875 0.792379 0.838074 0.841892\n",
      "260 37556.3 12.1658 4977.0265625 0.791425 0.837129 0.840981\n",
      "261 37459.1 12.1734 4965.1828125 0.790483 0.836197 0.840081\n",
      "262 37362.7 12.181 4953.4875 0.789552 0.835274 0.839191\n",
      "263 37267.2 12.1885 4941.97929687 0.788634 0.834364 0.838316\n",
      "264 37172.6 12.1961 4930.609375 0.787726 0.833462 0.83745\n",
      "265 37078.7 12.2035 4919.40390625 0.78683 0.832573 0.836597\n",
      "266 36985.6 12.211 4908.32890625 0.785944 0.831692 0.835751\n",
      "267 36893.4 12.2184 4897.37578125 0.785067 0.83082 0.834913\n",
      "268 36802.0 12.2257 4886.60546875 0.784203 0.829961 0.834091\n",
      "269 36711.4 12.2331 4875.9984375 0.783352 0.829112 0.833279\n",
      "270 36621.6 12.2404 4865.4765625 0.782506 0.82827 0.832472\n",
      "271 36532.6 12.2476 4855.14375 0.781673 0.827442 0.831681\n",
      "272 36444.3 12.2548 4844.9234375 0.780851 0.826621 0.830896\n",
      "273 36356.8 12.262 4834.840625 0.780038 0.825811 0.830123\n",
      "274 36270.1 12.2691 4824.90859375 0.779236 0.82501 0.82936\n",
      "275 36184.1 12.2763 4815.07421875 0.778443 0.824219 0.828604\n",
      "276 36098.9 12.2833 4805.40625 0.777661 0.823439 0.827859\n",
      "277 36014.4 12.2904 4795.84609375 0.776886 0.822668 0.827122\n",
      "278 35930.6 12.2974 4786.36796875 0.776118 0.821904 0.826392\n",
      "279 35847.5 12.3043 4777.0828125 0.775364 0.821153 0.825673\n",
      "280 35765.2 12.3113 4767.86875 0.774617 0.820408 0.82496\n",
      "281 35683.5 12.3181 4758.75078125 0.773875 0.81967 0.824254\n",
      "282 35602.6 12.325 4749.84453125 0.773152 0.818945 0.823566\n",
      "283 35522.4 12.3318 4740.99609375 0.772431 0.818225 0.822879\n",
      "284 35442.9 12.3386 4732.27890625 0.77172 0.817516 0.822206\n",
      "285 35364.1 12.3454 4723.71796875 0.771022 0.816815 0.82154\n",
      "286 35285.9 12.3521 4715.15820312 0.770322 0.816117 0.820877\n",
      "287 35208.5 12.3588 4706.85546875 0.769644 0.815434 0.820232\n",
      "288 35131.7 12.3655 4698.5828125 0.768968 0.814754 0.819589\n",
      "289 35055.5 12.3721 4690.41953125 0.768299 0.814083 0.818956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290 34980.0 12.3787 4682.3359375 0.767637 0.813417 0.818325\n",
      "291 34905.1 12.3853 4674.4484375 0.76699 0.812765 0.817712\n",
      "292 34830.9 12.3918 4666.50546875 0.766337 0.812111 0.817093\n",
      "293 34757.3 12.3983 4658.79257812 0.765705 0.811473 0.816492\n",
      "294 34684.3 12.4047 4651.06953125 0.76507 0.810836 0.81589\n",
      "295 34611.9 12.4112 4643.5046875 0.764448 0.81021 0.815299\n",
      "296 34540.1 12.4176 4635.99921875 0.763829 0.809587 0.81471\n",
      "297 34468.9 12.424 4628.61796875 0.763222 0.808979 0.814132\n",
      "298 34398.3 12.4303 4621.29140625 0.762616 0.808371 0.813556\n",
      "299 34328.3 12.4366 4614.07265625 0.762022 0.807771 0.812988\n",
      "300 34258.9 12.4429 4606.9625 0.761434 0.807179 0.812429\n",
      "301 34190.1 12.4492 4599.99960937 0.760859 0.806598 0.811881\n",
      "302 34121.9 12.4554 4593.03476562 0.760282 0.806019 0.811333\n",
      "303 34054.3 12.4616 4586.19921875 0.759717 0.80545 0.810792\n",
      "304 33987.2 12.4677 4579.42851562 0.759155 0.804886 0.810259\n",
      "305 33920.8 12.4738 4572.75546875 0.758601 0.804332 0.809731\n",
      "306 33854.8 12.4799 4566.1640625 0.758055 0.803783 0.80921\n",
      "307 33789.4 12.486 4559.68359375 0.757516 0.803243 0.808697\n",
      "308 33724.4 12.4921 4553.2671875 0.756983 0.802709 0.80819\n",
      "309 33660.0 12.4981 4546.89023437 0.756452 0.802177 0.807683\n",
      "310 33596.2 12.5041 4540.61953125 0.755932 0.801655 0.807188\n",
      "311 33532.8 12.51 4534.45625 0.755419 0.801136 0.806697\n",
      "312 33469.9 12.516 4528.29765625 0.754904 0.800622 0.806209\n",
      "313 33407.6 12.5219 4522.28398437 0.754404 0.800115 0.805732\n",
      "314 33345.7 12.5277 4516.29140625 0.753903 0.799612 0.805254\n",
      "315 33284.4 12.5336 4510.51953125 0.75342 0.799122 0.804795\n",
      "316 33223.6 12.5394 4504.56367188 0.752923 0.798623 0.804322\n",
      "317 33163.2 12.5452 4498.98671875 0.752458 0.798148 0.803881\n",
      "318 33103.4 12.551 4493.2140625 0.751975 0.797663 0.803422\n",
      "319 33044.0 12.5567 4487.74140625 0.751517 0.797198 0.802988\n",
      "320 32985.1 12.5624 4482.11914062 0.751045 0.796724 0.80254\n",
      "321 32926.7 12.5681 4476.78515625 0.750599 0.79627 0.802117\n",
      "322 32868.8 12.5738 4471.31875 0.750141 0.795809 0.801681\n",
      "323 32811.3 12.5794 4466.146875 0.749705 0.795368 0.801271\n",
      "324 32754.3 12.585 4460.7703125 0.749256 0.794914 0.800841\n",
      "325 32697.9 12.5906 4455.7234375 0.748831 0.794483 0.80044\n",
      "326 32641.8 12.5961 4450.56328125 0.748397 0.794044 0.800026\n",
      "327 32586.2 12.6017 4445.59648437 0.747979 0.793618 0.799631\n",
      "328 32531.0 12.6072 4440.55625 0.747555 0.793188 0.799227\n",
      "329 32476.3 12.6127 4435.71953125 0.747148 0.792774 0.798841\n",
      "330 32421.9 12.6181 4430.80898438 0.746735 0.792352 0.798446\n",
      "331 32368.1 12.6235 4426.146875 0.746341 0.79195 0.798074\n",
      "332 32314.6 12.6289 4421.26640625 0.74593 0.791533 0.79768\n",
      "333 32261.6 12.6343 4416.74296875 0.745548 0.791142 0.797319\n",
      "334 32209.0 12.6397 4412.00820313 0.745148 0.790737 0.796938\n",
      "335 32156.8 12.645 4407.56601562 0.744773 0.790355 0.796584\n",
      "336 32105.0 12.6503 4402.99296875 0.744387 0.789965 0.796218\n",
      "337 32053.7 12.6556 4398.59375 0.744015 0.789587 0.795869\n",
      "338 32002.7 12.6608 4394.15429688 0.74364 0.789206 0.795513\n",
      "339 31952.3 12.6661 4389.8671875 0.743277 0.788837 0.795172\n",
      "340 31902.1 12.6713 4385.5828125 0.742913 0.788466 0.794828\n",
      "341 31852.4 12.6764 4381.3515625 0.742555 0.788103 0.794489\n",
      "342 31803.2 12.6816 4377.1578125 0.7422 0.78774 0.794152\n",
      "343 31754.2 12.6867 4373.09296875 0.741855 0.787389 0.793826\n",
      "344 31705.7 12.6918 4368.98085938 0.741507 0.787036 0.793495\n",
      "345 31657.6 12.6969 4365.0140625 0.741169 0.786694 0.793178\n",
      "346 31609.8 12.702 4360.98125 0.740827 0.786345 0.792852\n",
      "347 31562.4 12.707 4357.13359375 0.740501 0.786014 0.792545\n",
      "348 31515.3 12.7121 4353.190625 0.740165 0.785673 0.792225\n",
      "349 31468.7 12.7171 4349.45546875 0.739848 0.78535 0.791926\n",
      "350 31422.4 12.722 4345.6203125 0.739521 0.78502 0.791615\n",
      "351 31376.5 12.727 4341.94375 0.739208 0.784701 0.791321\n",
      "352 31330.9 12.732 4338.259375 0.738893 0.784383 0.791021\n",
      "353 31285.7 12.7369 4334.61953125 0.738584 0.784071 0.790729\n",
      "354 31240.8 12.7418 4330.96992188 0.738273 0.783754 0.790434\n",
      "355 31196.2 12.7467 4327.54296875 0.737982 0.783459 0.79016\n",
      "356 31152.0 12.7515 4323.9046875 0.73767 0.783147 0.789864\n",
      "357 31108.1 12.7564 4320.54609375 0.737384 0.782858 0.789595\n",
      "358 31064.7 12.7612 4317.07578125 0.737088 0.782559 0.789313\n",
      "359 31021.5 12.766 4313.75234375 0.736803 0.782275 0.789045\n",
      "360 30978.6 12.7707 4310.38203125 0.736517 0.781983 0.788771\n",
      "361 30936.1 12.7755 4307.165625 0.736242 0.781707 0.788514\n",
      "362 30893.9 12.7802 4303.82890625 0.735956 0.781419 0.78824\n",
      "363 30852.1 12.7849 4300.6953125 0.735688 0.781151 0.787989\n",
      "364 30810.5 12.7896 4297.41757812 0.735408 0.780868 0.787722\n",
      "365 30769.3 12.7942 4294.38671875 0.735148 0.780609 0.787479\n",
      "366 30728.4 12.7989 4291.19257813 0.734877 0.780334 0.78722\n",
      "367 30687.8 12.8035 4288.21484375 0.734621 0.78008 0.786981\n",
      "368 30647.4 12.8081 4285.07890625 0.734352 0.779813 0.786726\n",
      "369 30607.4 12.8127 4282.26054687 0.734111 0.77957 0.786503\n",
      "370 30567.7 12.8172 4279.05234375 0.733835 0.7793 0.78624\n",
      "371 30528.3 12.8217 4276.35625 0.733604 0.779065 0.786027\n",
      "372 30489.1 12.8262 4273.26953125 0.733338 0.778803 0.785775\n",
      "373 30450.3 12.8308 4270.590625 0.733109 0.778571 0.785562\n",
      "374 30411.7 12.8352 4267.61484375 0.732854 0.778316 0.78532\n",
      "375 30373.5 12.8397 4264.94765625 0.732625 0.778085 0.785108\n",
      "376 30335.5 12.8441 4262.07382812 0.732379 0.777839 0.784872\n",
      "377 30297.8 12.8486 4259.49453125 0.732156 0.777615 0.784667\n",
      "378 30260.3 12.8529 4256.61757813 0.731909 0.777371 0.784431\n",
      "379 30223.2 12.8573 4254.146875 0.731696 0.777153 0.784235\n",
      "380 30186.2 12.8617 4251.34765625 0.731456 0.776915 0.784006\n",
      "381 30149.5 12.866 4248.87265625 0.731243 0.776699 0.783809\n",
      "382 30113.2 12.8703 4246.221875 0.731015 0.776471 0.783592\n",
      "383 30077.0 12.8747 4243.68632812 0.730796 0.776251 0.783388\n",
      "384 30041.2 12.8789 4241.1890625 0.730581 0.776037 0.783184\n",
      "385 30005.6 12.8832 4238.68828125 0.730365 0.775819 0.782982\n",
      "386 29970.2 12.8875 4236.25507813 0.730156 0.77561 0.782784\n",
      "387 29935.2 12.8917 4233.74179687 0.729939 0.775393 0.782581\n",
      "388 29900.3 12.8959 4231.44921875 0.729742 0.775195 0.782397\n",
      "389 29865.7 12.9001 4228.99140625 0.729529 0.774984 0.782202\n",
      "390 29831.3 12.9043 4226.70351563 0.729332 0.774786 0.782018\n",
      "391 29797.3 12.9085 4224.33984375 0.729128 0.774583 0.781829\n",
      "392 29763.4 12.9126 4222.0578125 0.728932 0.774384 0.781644\n",
      "393 29729.8 12.9167 4219.7703125 0.728733 0.774188 0.781462\n",
      "394 29696.5 12.9208 4217.58203125 0.728545 0.773995 0.781288\n",
      "395 29663.4 12.9249 4215.27539062 0.728345 0.773798 0.781104\n",
      "396 29630.5 12.929 4213.22148437 0.728168 0.773616 0.780942\n",
      "397 29597.8 12.9331 4210.9 0.727968 0.773418 0.780757\n",
      "398 29565.4 12.9371 4208.8921875 0.727794 0.773242 0.780597\n",
      "399 29533.2 12.9411 4206.66640625 0.727601 0.773051 0.78042\n",
      "400 29501.2 12.9452 4204.6640625 0.72743 0.772876 0.780262\n",
      "401 29469.5 12.9491 4202.49453125 0.727241 0.772691 0.780089\n",
      "402 29438.0 12.9531 4200.5125 0.727069 0.772518 0.779931\n",
      "403 29406.7 12.9571 4198.4453125 0.726891 0.772343 0.779766\n",
      "404 29375.6 12.961 4196.440625 0.726717 0.772168 0.779606\n",
      "405 29344.7 12.965 4194.4203125 0.726543 0.771995 0.779447\n",
      "406 29314.1 12.9689 4192.57148438 0.726382 0.771832 0.779301\n",
      "407 29283.7 12.9728 4190.471875 0.726201 0.771654 0.779134\n",
      "408 29253.5 12.9767 4188.74375 0.72605 0.771499 0.778998\n",
      "409 29223.4 12.9805 4186.71601563 0.725874 0.771327 0.778837\n",
      "410 29193.6 12.9844 4184.95585937 0.725722 0.77117 0.778699\n",
      "411 29163.9 12.9882 4182.94921875 0.725548 0.771 0.778537\n",
      "412 29134.5 12.992 4181.3484375 0.725409 0.770855 0.778412\n",
      "413 29105.3 12.9958 4179.25507813 0.725228 0.770679 0.778242\n",
      "414 29076.2 12.9996 4177.7046875 0.725094 0.77054 0.778123\n",
      "415 29047.3 13.0034 4175.76640625 0.724924 0.770376 0.777967\n",
      "416 29018.7 13.0071 4174.1484375 0.724784 0.770235 0.777842\n",
      "417 28990.2 13.0108 4172.29726563 0.724623 0.770078 0.777693\n",
      "418 28962.0 13.0146 4170.7328125 0.724488 0.769937 0.777573\n",
      "419 28933.8 13.0183 4168.86328125 0.724325 0.769782 0.777425\n",
      "420 28905.9 13.022 4167.3171875 0.724191 0.769643 0.777304\n",
      "421 28878.2 13.0257 4165.48125 0.724032 0.769488 0.777157\n",
      "422 28850.7 13.0294 4164.03828125 0.723907 0.769358 0.777046\n",
      "423 28823.4 13.033 4162.17304688 0.723744 0.769199 0.776894\n",
      "424 28796.2 13.0367 4160.81054688 0.723625 0.769078 0.776793\n",
      "425 28769.2 13.0403 4159.01835937 0.723468 0.768922 0.776648\n",
      "426 28742.4 13.0439 4157.58046875 0.723344 0.768795 0.776539\n",
      "427 28715.8 13.0475 4155.88515625 0.723197 0.768646 0.776405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "428 28689.3 13.0511 4154.5078125 0.723078 0.768524 0.7763\n",
      "429 28663.0 13.0546 4152.74453125 0.722923 0.768372 0.776157\n",
      "430 28636.9 13.0582 4151.45195312 0.72281 0.768256 0.776059\n",
      "431 28611.0 13.0617 4149.7375 0.722662 0.768109 0.77592\n",
      "432 28585.2 13.0653 4148.47890625 0.722551 0.767998 0.775826\n",
      "433 28559.6 13.0687 4146.76171875 0.722403 0.76785 0.775683\n",
      "434 28534.1 13.0723 4145.53984375 0.722297 0.76774 0.775592\n",
      "435 28508.9 13.0757 4143.86328125 0.722151 0.767598 0.775457\n",
      "436 28483.7 13.0792 4142.68046875 0.722048 0.76749 0.775369\n",
      "437 28458.7 13.0827 4141.01015625 0.721901 0.767351 0.775234\n",
      "438 28433.9 13.0861 4139.84453125 0.7218 0.767246 0.775147\n",
      "439 28409.2 13.0895 4138.275 0.721663 0.767112 0.775019\n",
      "440 28384.7 13.093 4137.07851563 0.721559 0.767007 0.774931\n",
      "441 28360.3 13.0964 4135.53125 0.721425 0.766873 0.774804\n",
      "442 28336.1 13.0998 4134.38867188 0.721324 0.766776 0.77472\n",
      "443 28312.1 13.1031 4132.85546875 0.721191 0.766644 0.774596\n",
      "444 28288.2 13.1065 4131.78515625 0.721097 0.766549 0.774518\n",
      "445 28264.4 13.1099 4130.2609375 0.720964 0.76642 0.774394\n",
      "446 28240.8 13.1132 4129.18671875 0.720871 0.766324 0.774316\n",
      "447 28217.4 13.1165 4127.66132813 0.720738 0.766198 0.774191\n",
      "448 28194.1 13.1199 4126.6359375 0.720647 0.766106 0.774117\n",
      "449 28170.9 13.1231 4125.1296875 0.720515 0.765983 0.773993\n",
      "450 28147.8 13.1264 4124.14335937 0.72043 0.765894 0.773925\n",
      "451 28124.9 13.1297 4122.671875 0.720301 0.765776 0.773804\n",
      "452 28102.1 13.133 4121.71289062 0.720217 0.76569 0.773736\n",
      "453 28079.5 13.1362 4120.21796875 0.720087 0.765567 0.773616\n",
      "454 28057.0 13.1395 4119.30585938 0.720007 0.765487 0.773554\n",
      "455 28034.6 13.1427 4117.828125 0.719878 0.765363 0.773435\n",
      "456 28012.4 13.1459 4116.91328125 0.719797 0.765284 0.773372\n",
      "457 27990.3 13.1491 4115.5390625 0.719678 0.765167 0.773262\n",
      "458 27968.3 13.1523 4114.5859375 0.719595 0.765085 0.773197\n",
      "459 27946.5 13.1554 4113.26328125 0.719479 0.764973 0.773088\n",
      "460 27924.8 13.1586 4112.28710938 0.719392 0.764888 0.773021\n",
      "461 27903.2 13.1618 4111.06445312 0.719286 0.764784 0.772922\n",
      "462 27881.7 13.1649 4110.03320312 0.719196 0.764696 0.772848\n",
      "463 27860.4 13.1681 4108.86015625 0.719094 0.764596 0.772755\n",
      "464 27839.1 13.1712 4107.89414062 0.719008 0.764517 0.772686\n",
      "465 27818.1 13.1743 4106.65546875 0.7189 0.764409 0.772588\n",
      "466 27797.1 13.1774 4105.82070313 0.718827 0.764337 0.77253\n",
      "467 27776.3 13.1805 4104.48515625 0.71871 0.764225 0.77242\n",
      "468 27755.6 13.1836 4103.79453125 0.718651 0.764162 0.772379\n",
      "469 27735.0 13.1866 4102.4078125 0.718529 0.76405 0.772261\n",
      "470 27714.6 13.1897 4101.68984375 0.718466 0.763987 0.772218\n",
      "471 27694.3 13.1927 4100.4375 0.718356 0.763882 0.772114\n",
      "472 27674.1 13.1958 4099.63789063 0.718285 0.763813 0.772061\n",
      "473 27654.0 13.1987 4098.5046875 0.718187 0.763717 0.771971\n",
      "474 27634.0 13.2018 4097.6484375 0.718112 0.763646 0.771913\n",
      "475 27614.2 13.2047 4096.54296875 0.718016 0.763553 0.771825\n",
      "476 27594.5 13.2077 4095.68867188 0.71794 0.763482 0.771766\n",
      "477 27574.9 13.2107 4094.63789063 0.717848 0.763395 0.771685\n",
      "478 27555.4 13.2137 4093.83242187 0.717778 0.763328 0.771631\n",
      "479 27536.0 13.2166 4092.675 0.717677 0.763234 0.771539\n",
      "480 27516.7 13.2196 4092.00625 0.717617 0.763176 0.771497\n",
      "481 27497.5 13.2224 4090.7453125 0.717507 0.763079 0.771392\n",
      "482 27478.4 13.2254 4090.21875 0.717459 0.763029 0.771365\n",
      "483 27459.5 13.2283 4088.96835938 0.71735 0.762931 0.771261\n",
      "484 27440.6 13.2312 4088.346875 0.717296 0.762876 0.771225\n",
      "485 27421.8 13.2341 4087.20039063 0.717195 0.762785 0.771131\n",
      "486 27403.2 13.237 4086.603125 0.717143 0.762735 0.771096\n",
      "487 27384.7 13.2398 4085.42890625 0.71704 0.76264 0.770999\n",
      "488 27366.2 13.2427 4084.8625 0.71699 0.76259 0.770967\n",
      "489 27347.9 13.2455 4083.68828125 0.716888 0.762497 0.770869\n",
      "490 27329.6 13.2485 4083.15664062 0.716841 0.762449 0.770841\n",
      "491 27311.4 13.2512 4082.0203125 0.716741 0.76236 0.770749\n",
      "492 27293.3 13.2541 4081.4546875 0.716691 0.762308 0.770715\n",
      "493 27275.4 13.2569 4080.40625 0.716599 0.762228 0.770633\n",
      "494 27257.4 13.2597 4079.79921875 0.716547 0.762172 0.770595\n",
      "495 27239.6 13.2625 4078.77265625 0.716456 0.762091 0.770514\n",
      "496 27222.0 13.2653 4078.21992188 0.716408 0.762043 0.770483\n",
      "497 27204.4 13.268 4077.12929687 0.716311 0.761953 0.770393\n",
      "498 27186.9 13.2709 4076.6515625 0.716269 0.761913 0.770368\n",
      "499 27169.4 13.2736 4075.54453125 0.716172 0.761823 0.770278\n",
      "500 27152.1 13.2764 4075.10234375 0.716133 0.761785 0.770258\n",
      "501 27134.9 13.2791 4073.9890625 0.716036 0.761696 0.770167\n",
      "502 27117.7 13.2819 4073.51875 0.715994 0.761654 0.770144\n",
      "503 27100.7 13.2845 4072.48125 0.715903 0.761575 0.770061\n",
      "504 27083.7 13.2873 4071.98515625 0.71586 0.761529 0.770036\n",
      "505 27066.9 13.2899 4070.9765625 0.715771 0.761454 0.769956\n",
      "506 27050.1 13.2927 4070.50078125 0.71573 0.761409 0.769932\n",
      "507 27033.5 13.2953 4069.49765625 0.715641 0.761334 0.769854\n",
      "508 27016.9 13.2981 4069.04375 0.715601 0.761296 0.769831\n",
      "509 27000.4 13.3007 4068.02695313 0.715512 0.761214 0.769749\n",
      "510 26984.0 13.3034 4067.63671875 0.715477 0.761182 0.769734\n",
      "511 26967.7 13.306 4066.56210938 0.715383 0.761098 0.769647\n",
      "512 26951.4 13.3087 4066.23554688 0.715354 0.76107 0.769638\n",
      "513 26935.3 13.3112 4065.15703125 0.715259 0.760987 0.769549\n",
      "514 26919.2 13.3139 4064.81523437 0.715228 0.760956 0.769541\n",
      "515 26903.2 13.3165 4063.79453125 0.715139 0.760879 0.769457\n",
      "516 26887.3 13.3192 4063.3890625 0.715104 0.760842 0.769441\n",
      "517 26871.4 13.3217 4062.49921875 0.715025 0.760776 0.769374\n",
      "518 26855.7 13.3243 4062.00820312 0.714982 0.760733 0.769345\n",
      "519 26840.0 13.3269 4061.14804688 0.714905 0.760667 0.769282\n",
      "520 26824.5 13.3295 4060.72304687 0.714868 0.760632 0.769259\n",
      "521 26809.0 13.332 4059.775 0.714785 0.760559 0.769184\n",
      "522 26793.5 13.3347 4059.49101562 0.71476 0.760537 0.76918\n",
      "523 26778.2 13.3371 4058.43359375 0.714668 0.760456 0.769091\n",
      "524 26762.9 13.3397 4058.2 0.714647 0.760436 0.769094\n",
      "525 26747.7 13.3422 4057.146875 0.714554 0.760354 0.769003\n",
      "526 26732.6 13.3448 4056.8921875 0.714532 0.760332 0.769002\n",
      "527 26717.6 13.3472 4055.93476563 0.714448 0.760259 0.768923\n",
      "528 26702.7 13.3498 4055.58046875 0.714415 0.760228 0.768909\n",
      "529 26687.8 13.3522 4054.7515625 0.714343 0.760166 0.768843\n",
      "530 26673.1 13.3548 4054.34726563 0.714307 0.760132 0.768823\n",
      "531 26658.3 13.3572 4053.55820313 0.714238 0.760074 0.768764\n",
      "532 26643.7 13.3597 4053.1375 0.714201 0.760038 0.768741\n",
      "533 26629.2 13.3621 4052.33085937 0.71413 0.759979 0.768679\n",
      "534 26614.7 13.3647 4051.9828125 0.714099 0.759949 0.768667\n",
      "535 26600.3 13.367 4051.12773437 0.714024 0.759883 0.768595\n",
      "536 26586.0 13.3696 4050.87890625 0.714002 0.759862 0.768596\n",
      "537 26571.7 13.3719 4049.89414063 0.713915 0.759788 0.768509\n",
      "538 26557.5 13.3744 4049.77382812 0.713904 0.759775 0.768522\n",
      "539 26543.4 13.3767 4048.7359375 0.713813 0.759701 0.768432\n",
      "540 26529.4 13.3793 4048.61054688 0.713802 0.759684 0.768442\n",
      "541 26515.4 13.3816 4047.64296875 0.713717 0.759615 0.76836\n",
      "542 26501.6 13.384 4047.42890625 0.713697 0.759594 0.768362\n",
      "543 26487.7 13.3863 4046.5734375 0.713623 0.759532 0.768294\n",
      "544 26474.0 13.3888 4046.29101562 0.713598 0.759508 0.768285\n",
      "545 26460.3 13.3911 4045.46015625 0.713524 0.759448 0.76822\n",
      "546 26446.7 13.3935 4045.2546875 0.713506 0.759432 0.76822\n",
      "547 26433.2 13.3958 4044.40078125 0.713431 0.759368 0.768149\n",
      "548 26419.7 13.3982 4044.14765625 0.713408 0.759347 0.768147\n",
      "549 26406.3 13.4005 4043.34140625 0.713336 0.759286 0.768078\n",
      "550 26392.9 13.4029 4043.10390625 0.713316 0.759268 0.768079\n",
      "551 26379.6 13.4051 4042.2453125 0.71324 0.759206 0.768003\n",
      "552 26366.4 13.4075 4042.11367187 0.713229 0.759191 0.768014\n",
      "553 26353.2 13.4098 4041.175 0.713145 0.759124 0.767932\n",
      "554 26340.1 13.4122 4041.06015625 0.713136 0.759111 0.767941\n",
      "555 26327.0 13.4144 4040.17460937 0.713058 0.759046 0.767868\n",
      "556 26314.0 13.4167 4040.02851563 0.713045 0.759035 0.767871\n",
      "557 26301.1 13.419 4039.2015625 0.712973 0.758972 0.767802\n",
      "558 26288.2 13.4213 4038.990625 0.712953 0.758956 0.767798\n",
      "559 26275.4 13.4235 4038.21679688 0.712885 0.7589 0.767736\n",
      "560 26262.6 13.4259 4038.05195313 0.71287 0.758885 0.767742\n",
      "561 26249.9 13.428 4037.2265625 0.712798 0.758826 0.767671\n",
      "562 26237.3 13.4304 4037.0765625 0.712784 0.758812 0.767681\n",
      "563 26224.7 13.4325 4036.2765625 0.712713 0.758756 0.767613\n",
      "564 26212.2 13.4349 4036.13164062 0.712701 0.758743 0.767621\n",
      "565 26199.7 13.437 4035.29570312 0.712626 0.758683 0.767551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "566 26187.4 13.4393 4035.215625 0.712619 0.758676 0.76756\n",
      "567 26175.0 13.4414 4034.34453125 0.712544 0.758613 0.767488\n",
      "568 26162.8 13.4438 4034.296875 0.712538 0.758608 0.767501\n",
      "569 26150.6 13.4459 4033.45390625 0.712465 0.758548 0.767432\n",
      "570 26138.4 13.4482 4033.35195312 0.712455 0.758539 0.767441\n",
      "571 26126.4 13.4503 4032.54296875 0.712383 0.758482 0.767373\n",
      "572 26114.4 13.4526 4032.46640625 0.712376 0.758475 0.767387\n",
      "573 26102.4 13.4546 4031.61914062 0.712302 0.758415 0.767315\n",
      "574 26090.5 13.4569 4031.565625 0.712297 0.758411 0.767332\n",
      "575 26078.6 13.459 4030.7328125 0.712224 0.75835 0.76726\n",
      "576 26066.8 13.4612 4030.7015625 0.712221 0.758348 0.767279\n",
      "577 26055.1 13.4633 4029.86875 0.712147 0.758287 0.767205\n",
      "578 26043.4 13.4655 4029.828125 0.712144 0.758285 0.767224\n",
      "579 26031.8 13.4676 4029.00703125 0.712072 0.758224 0.767153\n",
      "580 26020.2 13.4698 4028.96015625 0.712067 0.75822 0.767169\n",
      "581 26008.7 13.4718 4028.15078125 0.711996 0.758161 0.767103\n",
      "582 25997.3 13.474 4028.1046875 0.711992 0.758157 0.767117\n",
      "583 25985.9 13.4761 4027.32265625 0.711923 0.758102 0.767054\n",
      "584 25974.6 13.4783 4027.2203125 0.711914 0.758092 0.767064\n",
      "585 25963.3 13.4803 4026.47734375 0.711848 0.758042 0.767001\n",
      "586 25952.0 13.4825 4026.41328125 0.711843 0.758033 0.767018\n",
      "587 25940.8 13.4844 4025.6265625 0.711773 0.757981 0.76695\n",
      "588 25929.7 13.4866 4025.61796875 0.711773 0.757979 0.766975\n",
      "589 25918.6 13.4886 4024.81210938 0.7117 0.757923 0.766902\n",
      "590 25907.6 13.4908 4024.803125 0.711701 0.757923 0.766924\n",
      "591 25896.6 13.4927 4024.00664062 0.711629 0.757867 0.766856\n",
      "592 25885.7 13.4949 4024.003125 0.711629 0.757868 0.766877\n",
      "593 25874.8 13.4968 4023.20234375 0.711558 0.757812 0.766809\n",
      "594 25864.0 13.499 4023.1984375 0.711557 0.757812 0.766831\n",
      "595 25853.2 13.5009 4022.4125 0.711489 0.757759 0.766766\n",
      "596 25842.5 13.503 4022.38242188 0.711486 0.757755 0.766785\n",
      "597 25831.9 13.505 4021.66171875 0.711421 0.757707 0.766725\n",
      "598 25821.3 13.5071 4021.5796875 0.711414 0.757701 0.766739\n",
      "599 25810.7 13.509 4020.84726563 0.71135 0.757652 0.766673\n",
      "600 25800.2 13.5111 4020.83828125 0.711349 0.757654 0.766697\n",
      "601 25789.7 13.513 4020.0765625 0.711282 0.757599 0.766628\n",
      "602 25779.3 13.5151 4020.09335938 0.711284 0.757601 0.766653\n",
      "603 25768.9 13.517 4019.26953125 0.71121 0.757544 0.766579\n",
      "604 25758.6 13.5191 4019.3703125 0.71122 0.757552 0.766612\n",
      "605 25748.3 13.5209 4018.53515625 0.711145 0.757495 0.766537\n",
      "606 25738.1 13.523 4018.5515625 0.711147 0.757496 0.766563\n",
      "607 25727.8 13.5249 4017.83046875 0.711083 0.757447 0.766499\n",
      "608 25717.7 13.5269 4017.77265625 0.711077 0.757444 0.766518\n",
      "609 25707.6 13.5288 4017.1078125 0.711019 0.7574 0.766457\n",
      "610 25697.5 13.5308 4017.0875 0.711017 0.757398 0.766481\n",
      "611 25687.5 13.5327 4016.35390625 0.710953 0.757352 0.766413\n",
      "612 25677.6 13.5347 4016.37890625 0.710955 0.757355 0.76644\n",
      "613 25667.6 13.5365 4015.6171875 0.710888 0.757305 0.76637\n",
      "614 25657.8 13.5386 4015.6265625 0.710888 0.757308 0.766391\n",
      "615 25647.9 13.5404 4014.94648438 0.710827 0.757262 0.766334\n",
      "616 25638.2 13.5424 4014.90546875 0.710824 0.757264 0.766347\n",
      "617 25628.4 13.5442 4014.26992187 0.710767 0.757217 0.766294\n",
      "618 25618.7 13.5462 4014.16796875 0.710758 0.757215 0.766304\n",
      "619 25609.0 13.548 4013.56171875 0.710705 0.757172 0.766251\n",
      "620 25599.4 13.55 4013.48789063 0.710699 0.757168 0.766269\n",
      "621 25589.8 13.5518 4012.86015625 0.710643 0.757125 0.766211\n",
      "622 25580.2 13.5538 4012.80859375 0.710639 0.757122 0.766231\n",
      "623 25570.7 13.5555 4012.17890625 0.710582 0.757082 0.766172\n",
      "624 25561.3 13.5575 4012.16875 0.710583 0.75708 0.766197\n",
      "625 25551.9 13.5592 4011.46640625 0.71052 0.757035 0.766131\n",
      "626 25542.5 13.5612 4011.5140625 0.710525 0.75704 0.76616\n",
      "627 25533.1 13.563 4010.8109375 0.710462 0.75699 0.766095\n",
      "628 25523.9 13.5649 4010.8578125 0.710467 0.756996 0.766122\n",
      "629 25514.6 13.5666 4010.16640625 0.710405 0.756948 0.766062\n",
      "630 25505.4 13.5686 4010.15859375 0.710405 0.75695 0.766081\n",
      "631 25496.2 13.5703 4009.57265625 0.710353 0.756911 0.766034\n",
      "632 25487.2 13.5722 4009.44179687 0.710341 0.756903 0.766037\n",
      "633 25478.1 13.574 4008.93125 0.710296 0.75687 0.765997\n",
      "634 25469.1 13.5759 4008.80078125 0.710284 0.756861 0.766003\n",
      "635 25460.1 13.5776 4008.24960938 0.710236 0.756829 0.765959\n",
      "636 25451.2 13.5795 4008.21640625 0.710232 0.756827 0.765976\n",
      "637 25442.3 13.5811 4007.5734375 0.710175 0.756788 0.765919\n",
      "638 25433.5 13.5831 4007.640625 0.710181 0.756794 0.76595\n",
      "639 25424.6 13.5847 4006.8984375 0.710115 0.756744 0.76588\n",
      "640 25415.9 13.5866 4007.0984375 0.710134 0.756765 0.765929\n",
      "641 25407.0 13.5883 4006.30390625 0.710063 0.756707 0.765852\n",
      "642 25398.3 13.5902 4006.42109375 0.710073 0.756723 0.76589\n",
      "643 25389.6 13.5918 4005.67734375 0.710008 0.756671 0.765821\n",
      "644 25381.0 13.5937 4005.78359375 0.710017 0.756679 0.765856\n",
      "645 25372.4 13.5953 4005.10234375 0.709957 0.756637 0.765794\n",
      "646 25363.9 13.5972 4005.10625 0.709956 0.756638 0.765816\n",
      "647 25355.3 13.5988 4004.5359375 0.709907 0.756605 0.765767\n",
      "648 25346.8 13.6006 4004.4484375 0.709898 0.7566 0.765777\n",
      "649 25338.4 13.6023 4003.95664063 0.709855 0.756571 0.765739\n",
      "650 25330.0 13.6041 4003.89804688 0.709849 0.756565 0.76575\n",
      "651 25321.6 13.6058 4003.34648437 0.7098 0.756532 0.765704\n",
      "652 25313.3 13.6076 4003.3453125 0.7098 0.756535 0.765725\n",
      "653 25305.0 13.6092 4002.69101563 0.709741 0.756491 0.765665\n",
      "654 25296.7 13.611 4002.83203125 0.709755 0.756503 0.765707\n",
      "655 25288.4 13.6126 4002.08671875 0.709689 0.756453 0.765634\n",
      "656 25280.2 13.6144 4002.27734375 0.709705 0.75647 0.765682\n",
      "657 25272.0 13.616 4001.56054688 0.709642 0.756422 0.765613\n",
      "658 25263.8 13.6178 4001.5578125 0.709642 0.756426 0.765638\n",
      "659 25255.7 13.6194 4001.07265625 0.709599 0.756392 0.765596\n",
      "660 25247.6 13.6211 4000.90625 0.709584 0.756384 0.765601\n",
      "661 25239.6 13.6228 4000.46914062 0.709545 0.756354 0.76557\n",
      "662 25231.6 13.6245 4000.39296875 0.709538 0.756356 0.765578\n",
      "663 25223.6 13.6261 3999.94179687 0.709499 0.756322 0.765548\n",
      "664 25215.7 13.6279 3999.8421875 0.70949 0.756324 0.765554\n",
      "665 25207.8 13.6294 3999.31328125 0.709442 0.756283 0.765511\n",
      "666 25200.0 13.6312 3999.346875 0.709445 0.756295 0.765536\n",
      "667 25192.1 13.6327 3998.7515625 0.709393 0.756249 0.765479\n",
      "668 25184.3 13.6345 3998.80234375 0.709397 0.756262 0.76551\n",
      "669 25176.5 13.636 3998.1296875 0.709338 0.756212 0.765444\n",
      "670 25168.8 13.6378 3998.28984375 0.709352 0.756229 0.765486\n",
      "671 25161.0 13.6393 3997.5859375 0.70929 0.75618 0.765418\n",
      "672 25153.3 13.6411 3997.740625 0.709303 0.756191 0.765459\n",
      "673 25145.6 13.6425 3997.07304687 0.709244 0.756152 0.765394\n",
      "674 25138.0 13.6443 3997.16015625 0.709252 0.756158 0.765429\n",
      "675 25130.4 13.6458 3996.5484375 0.709198 0.756121 0.765372\n",
      "676 25122.8 13.6475 3996.64257812 0.709206 0.75613 0.765405\n",
      "677 25115.2 13.649 3996.04765625 0.709153 0.756094 0.765354\n",
      "678 25107.7 13.6507 3996.03476563 0.709152 0.756097 0.76537\n",
      "679 25100.2 13.6523 3995.55976563 0.70911 0.756065 0.765335\n",
      "680 25092.8 13.6539 3995.503125 0.709104 0.756066 0.765345\n",
      "681 25085.4 13.6554 3995.00703125 0.70906 0.756034 0.765305\n",
      "682 25078.1 13.6571 3995.0421875 0.709063 0.756041 0.765331\n",
      "683 25070.7 13.6586 3994.45117188 0.709012 0.756002 0.765275\n",
      "684 25063.4 13.6603 3994.54609375 0.70902 0.756013 0.765314\n",
      "685 25056.1 13.6617 3993.9203125 0.708965 0.755972 0.765249\n",
      "686 25048.8 13.6634 3994.04882812 0.708977 0.755984 0.765291\n",
      "687 25041.5 13.6648 3993.396875 0.708918 0.755944 0.765226\n",
      "688 25034.3 13.6665 3993.51328125 0.708928 0.755955 0.765264\n",
      "689 25027.0 13.668 3992.91875 0.708876 0.755921 0.765209\n",
      "690 25019.8 13.6696 3992.953125 0.708879 0.755926 0.765235\n",
      "691 25012.6 13.6711 3992.45703125 0.708834 0.755897 0.765194\n",
      "692 25005.5 13.6727 3992.40390625 0.708829 0.755899 0.765208\n",
      "693 24998.4 13.6742 3991.96953125 0.708791 0.755872 0.765173\n",
      "694 24991.4 13.6758 3991.90234375 0.708785 0.755875 0.765186\n",
      "695 24984.4 13.6772 3991.45234375 0.708745 0.755844 0.765149\n",
      "696 24977.4 13.6788 3991.43359375 0.708743 0.755852 0.765167\n",
      "697 24970.4 13.6803 3990.928125 0.708699 0.755817 0.765122\n",
      "698 24963.5 13.6819 3990.98359375 0.708703 0.755829 0.765153\n",
      "699 24956.5 13.6833 3990.39101562 0.708651 0.755789 0.765093\n",
      "700 24949.7 13.6849 3990.54882812 0.708665 0.755805 0.765139\n",
      "701 24942.7 13.6863 3989.89335937 0.708607 0.755764 0.76507\n",
      "702 24935.9 13.6879 3990.05234375 0.708621 0.75578 0.765116\n",
      "703 24929.0 13.6893 3989.415625 0.708564 0.75574 0.765052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "704 24922.1 13.6909 3989.56640625 0.708578 0.755757 0.765095\n",
      "705 24915.3 13.6923 3988.93945312 0.708522 0.755716 0.765034\n",
      "706 24908.5 13.6939 3989.09375 0.708536 0.755734 0.765076\n",
      "707 24901.8 13.6953 3988.46875 0.708481 0.755692 0.765016\n",
      "708 24895.0 13.6969 3988.63085938 0.708495 0.755708 0.765057\n",
      "709 24888.3 13.6982 3988.00898438 0.70844 0.755669 0.764996\n",
      "710 24881.7 13.6998 3988.159375 0.708453 0.755681 0.765038\n",
      "711 24875.0 13.7012 3987.51640625 0.708396 0.755641 0.764972\n",
      "712 24868.5 13.7028 3987.72617188 0.708415 0.755658 0.765023\n",
      "713 24861.9 13.7041 3987.05429688 0.708354 0.755614 0.76495\n",
      "714 24855.3 13.7057 3987.24140625 0.708371 0.755632 0.765001\n",
      "715 24848.7 13.707 3986.63242188 0.708317 0.755592 0.764935\n",
      "716 24842.2 13.7086 3986.74453125 0.708327 0.755607 0.764975\n",
      "717 24835.7 13.7099 3986.2015625 0.708278 0.75557 0.76492\n",
      "718 24829.1 13.7115 3986.2828125 0.708287 0.755585 0.764955\n",
      "719 24822.6 13.7128 3985.728125 0.708236 0.755548 0.764901\n",
      "720 24816.2 13.7144 3985.846875 0.708247 0.755561 0.764937\n",
      "721 24809.7 13.7157 3985.3125 0.708199 0.755528 0.764887\n",
      "722 24803.4 13.7172 3985.3671875 0.708205 0.755536 0.764915\n",
      "723 24797.0 13.7185 3984.85546875 0.70816 0.755505 0.764867\n",
      "724 24790.7 13.7201 3984.928125 0.708166 0.755513 0.764898\n",
      "725 24784.4 13.7214 3984.3609375 0.708115 0.755478 0.764842\n",
      "726 24778.1 13.7229 3984.5421875 0.708131 0.755496 0.764888\n",
      "727 24771.9 13.7242 3983.89296875 0.708073 0.755453 0.764823\n",
      "728 24765.6 13.7257 3984.12539062 0.708094 0.755473 0.764874\n",
      "729 24759.3 13.727 3983.4421875 0.708034 0.75543 0.764804\n",
      "730 24753.1 13.7286 3983.70039063 0.708056 0.755449 0.764863\n",
      "731 24746.9 13.7298 3983.003125 0.707995 0.755406 0.764786\n",
      "732 24740.7 13.7314 3983.26015625 0.708017 0.755426 0.764848\n",
      "733 24734.5 13.7325 3982.59140625 0.707958 0.755384 0.764772\n",
      "734 24728.4 13.7341 3982.78046875 0.707976 0.755401 0.764824\n",
      "735 24722.3 13.7353 3982.1796875 0.707922 0.755362 0.764758\n",
      "736 24716.2 13.7369 3982.35429687 0.707936 0.755378 0.764807\n",
      "737 24710.2 13.7381 3981.7453125 0.707882 0.75534 0.764744\n",
      "738 24704.1 13.7396 3981.90585938 0.707896 0.755355 0.764786\n",
      "739 24698.1 13.7408 3981.33046875 0.707845 0.75532 0.764732\n",
      "740 24692.2 13.7423 3981.46640625 0.707858 0.755331 0.764765\n",
      "741 24686.2 13.7435 3980.88828125 0.707807 0.7553 0.764711\n",
      "742 24680.3 13.745 3981.05820313 0.707821 0.75531 0.764751\n",
      "743 24674.4 13.7462 3980.41484375 0.707763 0.755275 0.764686\n",
      "744 24668.5 13.7478 3980.72421875 0.707792 0.755294 0.764749\n",
      "745 24662.6 13.7489 3979.96640625 0.707725 0.755247 0.764666\n",
      "746 24656.7 13.7505 3980.3453125 0.707758 0.755274 0.76474\n",
      "747 24650.8 13.7516 3979.56601563 0.707689 0.75522 0.76465\n",
      "748 24645.0 13.7531 3979.91015625 0.707719 0.755252 0.764723\n",
      "749 24639.1 13.7542 3979.1671875 0.707654 0.755199 0.764638\n",
      "750 24633.3 13.7558 3979.47539062 0.707682 0.75523 0.764703\n",
      "751 24627.5 13.7569 3978.74648437 0.707617 0.75518 0.764623\n",
      "752 24621.7 13.7584 3979.06679687 0.707645 0.755206 0.764685\n",
      "753 24616.0 13.7596 3978.378125 0.707585 0.755163 0.764613\n",
      "754 24610.2 13.7611 3978.6140625 0.707604 0.755177 0.764663\n",
      "755 24604.5 13.7622 3978.02421875 0.707553 0.755147 0.764605\n",
      "756 24598.9 13.7637 3978.15351563 0.707563 0.755151 0.764642\n",
      "757 24593.2 13.7648 3977.6484375 0.707519 0.755126 0.764593\n",
      "758 24587.6 13.7663 3977.74882812 0.707528 0.755133 0.764625\n",
      "759 24581.9 13.7674 3977.21054688 0.70748 0.755103 0.764572\n",
      "760 24576.4 13.7689 3977.41484375 0.707498 0.75512 0.76462\n",
      "761 24570.8 13.77 3976.7359375 0.707438 0.755076 0.764547\n",
      "762 24565.2 13.7715 3977.0984375 0.707471 0.755102 0.764615\n",
      "763 24559.7 13.7726 3976.29921875 0.707399 0.755052 0.764525\n",
      "764 24554.2 13.7742 3976.7328125 0.707437 0.755085 0.764606\n",
      "765 24548.6 13.7752 3975.9171875 0.707365 0.755032 0.764514\n",
      "766 24543.0 13.7767 3976.275 0.707398 0.755061 0.764584\n",
      "767 24537.5 13.7777 3975.590625 0.707336 0.755018 0.764508\n",
      "768 24532.0 13.7792 3975.79921875 0.707354 0.755036 0.764559\n",
      "769 24526.5 13.7803 3975.26953125 0.707307 0.755002 0.764503\n",
      "770 24521.1 13.7818 3975.3765625 0.707316 0.755012 0.764538\n",
      "771 24515.7 13.7829 3974.91328125 0.707275 0.754985 0.764494\n",
      "772 24510.4 13.7843 3975.00546875 0.707283 0.754994 0.764524\n",
      "773 24505.0 13.7854 3974.490625 0.707238 0.754965 0.764473\n",
      "774 24499.6 13.7868 3974.653125 0.707252 0.75498 0.764514\n",
      "775 24494.2 13.7879 3974.08203125 0.707201 0.754944 0.764453\n",
      "776 24488.9 13.7893 3974.32890625 0.707223 0.754964 0.764506\n",
      "777 24483.6 13.7904 3973.64453125 0.707162 0.754922 0.764428\n",
      "778 24478.4 13.7919 3974.04765625 0.707198 0.754952 0.764506\n",
      "779 24473.2 13.7928 3973.20703125 0.707123 0.754898 0.764404\n",
      "780 24467.9 13.7944 3973.67734375 0.707165 0.754933 0.764493\n",
      "781 24462.6 13.7953 3972.84101563 0.707091 0.75488 0.764391\n",
      "782 24457.4 13.7968 3973.25742188 0.707129 0.754911 0.764472\n",
      "783 24452.1 13.7978 3972.51328125 0.707062 0.754866 0.764385\n",
      "784 24446.9 13.7993 3972.80976563 0.707089 0.754889 0.764444\n",
      "785 24441.6 13.8003 3972.2203125 0.707036 0.754855 0.76438\n",
      "786 24436.5 13.8017 3972.37578125 0.70705 0.754867 0.764419\n",
      "787 24431.3 13.8027 3971.88945312 0.707007 0.754838 0.76437\n",
      "788 24426.2 13.8041 3971.98046875 0.707014 0.754845 0.7644\n",
      "789 24421.1 13.8052 3971.54609375 0.706976 0.754822 0.764359\n",
      "790 24416.0 13.8065 3971.61796875 0.706982 0.754827 0.764385\n",
      "791 24411.0 13.8076 3971.159375 0.706941 0.754803 0.764342\n",
      "792 24405.9 13.809 3971.278125 0.706952 0.754815 0.764374\n",
      "793 24400.9 13.81 3970.74648437 0.706904 0.754785 0.764319\n",
      "794 24395.9 13.8114 3970.965625 0.706925 0.754803 0.764368\n",
      "795 24390.9 13.8124 3970.34570312 0.706869 0.754765 0.764298\n",
      "796 24385.9 13.8138 3970.6734375 0.706899 0.754789 0.764367\n",
      "797 24380.9 13.8147 3969.93671875 0.706832 0.754742 0.764279\n",
      "798 24376.0 13.8162 3970.35234375 0.706869 0.754775 0.764358\n",
      "799 24371.0 13.8171 3969.571875 0.7068 0.754724 0.764264\n",
      "800 24366.0 13.8186 3969.97421875 0.706836 0.754758 0.764342\n",
      "801 24361.0 13.8195 3969.2609375 0.706773 0.754714 0.764259\n",
      "802 24356.1 13.8209 3969.56640625 0.706802 0.754738 0.764322\n",
      "803 24351.2 13.8218 3968.93476563 0.706744 0.7547 0.764251\n",
      "804 24346.3 13.8233 3969.18085938 0.706766 0.754719 0.764304\n",
      "805 24341.4 13.8242 3968.62109375 0.706715 0.754686 0.764245\n",
      "806 24336.5 13.8256 3968.8 0.706732 0.754701 0.764289\n",
      "807 24331.7 13.8265 3968.27421875 0.706684 0.754668 0.764232\n",
      "808 24326.9 13.8279 3968.46875 0.706701 0.754687 0.764282\n",
      "809 24322.2 13.8289 3967.88671875 0.706649 0.754651 0.764214\n",
      "810 24317.4 13.8302 3968.19921875 0.706677 0.754674 0.764282\n",
      "811 24312.6 13.8311 3967.51132813 0.706616 0.754634 0.764198\n",
      "812 24307.8 13.8325 3967.865625 0.706648 0.75466 0.76427\n",
      "813 24303.0 13.8334 3967.18125 0.706587 0.754618 0.764187\n",
      "814 24298.2 13.8348 3967.49023438 0.706614 0.75464 0.764254\n",
      "815 24293.5 13.8357 3966.865625 0.706559 0.754604 0.76418\n",
      "816 24288.8 13.8371 3967.11757812 0.706581 0.754622 0.764237\n",
      "817 24284.1 13.838 3966.521875 0.706527 0.754589 0.764169\n",
      "818 24279.4 13.8394 3966.79296875 0.706552 0.754607 0.764227\n",
      "819 24274.7 13.8402 3966.18125 0.706497 0.754575 0.764157\n",
      "820 24270.1 13.8416 3966.4296875 0.70652 0.754592 0.764209\n",
      "821 24265.4 13.8425 3965.88671875 0.706471 0.754562 0.76415\n",
      "822 24260.7 13.8439 3966.06875 0.706488 0.754578 0.764191\n",
      "823 24256.2 13.8448 3965.55078125 0.706442 0.754546 0.764138\n",
      "824 24251.7 13.8461 3965.7609375 0.706461 0.754566 0.764182\n",
      "825 24247.2 13.847 3965.23359375 0.706413 0.754534 0.764127\n",
      "826 24242.7 13.8483 3965.45234375 0.706433 0.754552 0.764176\n",
      "827 24238.2 13.8492 3964.8640625 0.70638 0.754518 0.764108\n",
      "828 24233.7 13.8506 3965.15351563 0.706406 0.754539 0.764173\n",
      "829 24229.2 13.8514 3964.53515625 0.706351 0.754504 0.764098\n",
      "830 24224.6 13.8528 3964.76484375 0.706373 0.754521 0.764155\n",
      "831 24220.1 13.8536 3964.2265625 0.706324 0.754493 0.764089\n",
      "832 24215.6 13.8549 3964.43515625 0.706342 0.754508 0.764143\n",
      "833 24211.1 13.8558 3963.89765625 0.706294 0.754478 0.76408\n",
      "834 24206.7 13.8571 3964.140625 0.706316 0.7545 0.764135\n",
      "835 24202.2 13.858 3963.5734375 0.706265 0.754463 0.764071\n",
      "836 24197.8 13.8593 3963.86796875 0.706292 0.754491 0.764132\n",
      "837 24193.4 13.8601 3963.21210938 0.706233 0.754446 0.764055\n",
      "838 24189.1 13.8615 3963.59882813 0.706268 0.75448 0.764128\n",
      "839 24184.7 13.8623 3962.8796875 0.706203 0.754435 0.764043\n",
      "840 24180.4 13.8637 3963.284375 0.706241 0.754468 0.76412\n",
      "841 24176.0 13.8644 3962.54882812 0.706174 0.754423 0.764029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "842 24171.7 13.8658 3962.940625 0.70621 0.754453 0.764108\n",
      "843 24167.3 13.8666 3962.28046875 0.70615 0.754414 0.764025\n",
      "844 24162.9 13.8679 3962.52578125 0.706172 0.754436 0.764084\n",
      "845 24158.6 13.8688 3962.01132813 0.706127 0.754404 0.764023\n",
      "846 24154.3 13.87 3962.13867188 0.706138 0.75442 0.764063\n",
      "847 24150.0 13.8709 3961.75117188 0.706103 0.754398 0.764018\n",
      "848 24145.6 13.8721 3961.82851562 0.706112 0.754406 0.764053\n",
      "849 24141.3 13.873 3961.41914063 0.706074 0.754389 0.764006\n",
      "850 24137.0 13.8743 3961.56601563 0.706087 0.754398 0.764048\n",
      "851 24132.8 13.8751 3961.07265625 0.706043 0.754377 0.763995\n",
      "852 24128.7 13.8764 3961.31328125 0.706065 0.75439 0.764046\n",
      "853 24124.5 13.8772 3960.70273438 0.70601 0.754361 0.763979\n",
      "854 24120.3 13.8785 3961.07578125 0.706043 0.754386 0.764045\n",
      "855 24116.0 13.8792 3960.36054688 0.70598 0.754347 0.763965\n",
      "856 24112.0 13.8806 3960.77929688 0.706018 0.754376 0.764037\n",
      "857 24107.6 13.8813 3960.07070312 0.705954 0.754333 0.763957\n",
      "858 24103.5 13.8826 3960.42070312 0.705985 0.754362 0.764019\n",
      "859 24099.3 13.8834 3959.78710938 0.705929 0.754324 0.763947\n",
      "860 24095.2 13.8847 3960.05117187 0.705952 0.754348 0.764002\n",
      "861 24091.0 13.8855 3959.50703125 0.705904 0.754313 0.763941\n",
      "862 24086.9 13.8867 3959.70585937 0.705921 0.754335 0.763989\n",
      "863 24082.9 13.8875 3959.22578125 0.705878 0.754305 0.763931\n",
      "864 24078.8 13.8887 3959.36484375 0.705891 0.754322 0.763976\n",
      "865 24074.7 13.8896 3958.98125 0.705856 0.754296 0.763928\n",
      "866 24070.7 13.8908 3959.053125 0.705863 0.754313 0.763965\n",
      "867 24066.6 13.8916 3958.67578125 0.705829 0.754286 0.763917\n",
      "868 24062.6 13.8928 3958.771875 0.705839 0.754303 0.763955\n",
      "869 24058.5 13.8936 3958.37109375 0.705802 0.754274 0.763905\n",
      "870 24054.5 13.8948 3958.49375 0.705814 0.754295 0.763948\n",
      "871 24050.4 13.8956 3958.05234375 0.705774 0.754262 0.763895\n",
      "872 24046.4 13.8968 3958.18828125 0.705786 0.754281 0.763936\n",
      "873 24042.3 13.8976 3957.70859375 0.705743 0.754248 0.763881\n",
      "874 24038.4 13.8989 3957.93203125 0.705763 0.754271 0.76393\n",
      "875 24034.4 13.8996 3957.37929687 0.705713 0.754239 0.763868\n",
      "876 24030.5 13.9009 3957.6328125 0.705737 0.754259 0.763923\n",
      "877 24026.5 13.9016 3957.08359375 0.705687 0.754229 0.763857\n",
      "878 24022.6 13.9029 3957.3390625 0.705711 0.754247 0.763918\n",
      "879 24018.7 13.9035 3956.78984375 0.705663 0.754217 0.763846\n",
      "880 24014.8 13.9049 3957.0578125 0.705685 0.75424 0.763915\n",
      "881 24010.8 13.9055 3956.50546875 0.705635 0.754202 0.763837\n",
      "882 24006.9 13.9068 3956.7703125 0.705661 0.754233 0.763907\n",
      "883 24003.0 13.9075 3956.22109375 0.705611 0.754193 0.763831\n",
      "884 23999.2 13.9088 3956.45898438 0.705632 0.754223 0.763892\n",
      "885 23995.2 13.9095 3955.96875 0.705588 0.754183 0.76383\n",
      "886 23991.3 13.9107 3956.1046875 0.705599 0.754208 0.763872\n",
      "887 23987.4 13.9115 3955.74492187 0.705569 0.754175 0.76383\n",
      "888 23983.5 13.9126 3955.75625 0.705569 0.754189 0.763853\n",
      "889 23979.6 13.9134 3955.4515625 0.705542 0.754166 0.763819\n",
      "890 23975.8 13.9145 3955.47421875 0.705544 0.754176 0.763843\n",
      "891 23972.0 13.9153 3955.1328125 0.705514 0.754156 0.763806\n",
      "892 23968.2 13.9165 3955.2140625 0.70552 0.754164 0.763837\n",
      "893 23964.5 13.9173 3954.83203125 0.705486 0.754146 0.763796\n",
      "894 23960.8 13.9185 3954.99765625 0.705501 0.754155 0.763837\n",
      "895 23956.9 13.9192 3954.52109375 0.705459 0.754135 0.763784\n",
      "896 23953.2 13.9204 3954.77617187 0.705481 0.754152 0.76384\n",
      "897 23949.4 13.921 3954.16796875 0.705428 0.754117 0.763764\n",
      "898 23945.8 13.9223 3954.54609375 0.70546 0.754149 0.763844\n",
      "899 23942.0 13.9229 3953.86171875 0.7054 0.754101 0.76375\n",
      "900 23938.3 13.9242 3954.25703125 0.705435 0.754141 0.763837\n",
      "901 23934.4 13.9248 3953.5921875 0.705376 0.754089 0.763743\n",
      "902 23930.7 13.9261 3953.92226562 0.705405 0.754125 0.763822\n",
      "903 23926.9 13.9267 3953.328125 0.705352 0.75408 0.763741\n",
      "904 23923.3 13.928 3953.61992187 0.705378 0.754112 0.763813\n",
      "905 23919.6 13.9286 3953.04921875 0.705328 0.754069 0.763731\n",
      "906 23916.0 13.9299 3953.3515625 0.705354 0.754097 0.763808\n",
      "907 23912.4 13.9305 3952.77265625 0.705302 0.754062 0.763724\n",
      "908 23908.8 13.9318 3953.0859375 0.705331 0.754085 0.763804\n",
      "909 23905.1 13.9323 3952.48046875 0.705277 0.754051 0.763716\n",
      "910 23901.5 13.9336 3952.821875 0.705307 0.754078 0.763797\n",
      "911 23897.8 13.9342 3952.21132812 0.705253 0.75404 0.763708\n",
      "912 23894.2 13.9355 3952.5625 0.705284 0.75407 0.763792\n",
      "913 23890.5 13.9361 3951.915625 0.705227 0.754029 0.763699\n",
      "914 23886.9 13.9373 3952.27421875 0.705259 0.754058 0.763782\n",
      "915 23883.3 13.9379 3951.621875 0.705201 0.754017 0.763693\n",
      "916 23879.7 13.9392 3951.984375 0.705232 0.754046 0.763773\n",
      "917 23876.0 13.9397 3951.3984375 0.70518 0.754008 0.763696\n",
      "918 23872.5 13.941 3951.68125 0.705206 0.754031 0.763762\n",
      "919 23868.8 13.9416 3951.1265625 0.705156 0.753994 0.763692\n",
      "920 23865.2 13.9428 3951.36875 0.705179 0.754018 0.76375\n",
      "921 23861.6 13.9434 3950.8578125 0.705132 0.753986 0.763687\n",
      "922 23858.1 13.9446 3951.0953125 0.705154 0.754004 0.763746\n",
      "923 23854.6 13.9452 3950.5390625 0.705104 0.753969 0.763673\n",
      "924 23851.2 13.9465 3950.90234375 0.705136 0.754 0.763754\n",
      "925 23847.8 13.947 3950.21796875 0.705075 0.753955 0.763655\n",
      "926 23844.4 13.9483 3950.71015625 0.705119 0.753992 0.763761\n",
      "927 23840.9 13.9488 3949.93867188 0.70505 0.753946 0.763645\n",
      "928 23837.5 13.9501 3950.45078125 0.705095 0.75398 0.763754\n",
      "929 23834.0 13.9506 3949.64414063 0.705024 0.753935 0.763636\n",
      "930 23830.6 13.952 3950.203125 0.705073 0.753972 0.76375\n",
      "931 23827.0 13.9524 3949.39765625 0.705002 0.753929 0.763635\n",
      "932 23823.5 13.9537 3949.8890625 0.705046 0.753959 0.763736\n",
      "933 23820.0 13.9542 3949.134375 0.704979 0.75392 0.763628\n",
      "934 23816.5 13.9555 3949.60898438 0.705021 0.753947 0.763729\n",
      "935 23813.1 13.956 3948.86523438 0.704953 0.753908 0.763624\n",
      "936 23809.7 13.9573 3949.35585937 0.704998 0.75394 0.763724\n",
      "937 23806.1 13.9578 3948.6265625 0.704932 0.753899 0.763623\n",
      "938 23802.7 13.9591 3949.04335937 0.704971 0.753927 0.763711\n",
      "939 23799.2 13.9596 3948.403125 0.704912 0.753891 0.763623\n",
      "940 23795.9 13.9608 3948.7578125 0.704944 0.753914 0.763702\n",
      "941 23792.4 13.9613 3948.14609375 0.70489 0.753877 0.763621\n",
      "942 23789.2 13.9626 3948.51835937 0.704923 0.753911 0.7637\n",
      "943 23785.7 13.9631 3947.8640625 0.704865 0.753865 0.763613\n",
      "944 23782.5 13.9644 3948.25898438 0.704901 0.753902 0.763696\n",
      "945 23779.1 13.9648 3947.57421875 0.704839 0.753853 0.763601\n",
      "946 23775.8 13.9661 3947.99023438 0.704877 0.75389 0.763692\n",
      "947 23772.5 13.9666 3947.28398437 0.704813 0.753843 0.763589\n",
      "948 23769.3 13.9679 3947.75 0.704854 0.753875 0.76369\n",
      "949 23766.1 13.9683 3947.00976562 0.704788 0.753839 0.763579\n",
      "950 23762.7 13.9696 3947.48125 0.704831 0.753863 0.763684\n",
      "951 23759.3 13.97 3946.7828125 0.704769 0.753834 0.763579\n",
      "952 23756.0 13.9714 3947.18359375 0.704804 0.753851 0.763672\n",
      "953 23752.6 13.9718 3946.5328125 0.704746 0.753827 0.763576\n",
      "954 23749.4 13.9731 3946.9375 0.704782 0.753846 0.763669\n",
      "955 23746.0 13.9735 3946.25390625 0.704721 0.753816 0.763571\n",
      "956 23742.8 13.9748 3946.72226563 0.704762 0.753847 0.763668\n",
      "957 23739.5 13.9752 3945.9828125 0.704697 0.753803 0.763566\n",
      "958 23736.4 13.9765 3946.50390625 0.704743 0.753846 0.763669\n",
      "959 23732.9 13.977 3945.7609375 0.704677 0.753794 0.763566\n",
      "960 23729.7 13.9782 3946.20039063 0.704716 0.753835 0.763657\n",
      "961 23726.3 13.9787 3945.53515625 0.704657 0.753787 0.763565\n",
      "962 23723.3 13.9799 3945.9296875 0.704692 0.753824 0.763647\n",
      "963 23719.9 13.9804 3945.2796875 0.704633 0.753781 0.763559\n",
      "964 23716.9 13.9816 3945.67773438 0.704669 0.753809 0.763642\n",
      "965 23713.6 13.982 3945.01796875 0.70461 0.753775 0.76355\n",
      "966 23710.4 13.9833 3945.3875 0.704643 0.753795 0.763634\n",
      "967 23707.2 13.9837 3944.74609375 0.704586 0.753768 0.763543\n",
      "968 23704.2 13.985 3945.1859375 0.704625 0.753789 0.763638\n",
      "969 23701.0 13.9853 3944.45625 0.70456 0.753756 0.763531\n",
      "970 23697.8 13.9866 3944.92265625 0.704602 0.753781 0.763634\n",
      "971 23694.7 13.987 3944.20664063 0.704539 0.753749 0.763528\n",
      "972 23691.5 13.9883 3944.65351563 0.704578 0.753774 0.763627\n",
      "973 23688.3 13.9887 3943.95664063 0.704516 0.753741 0.763524\n",
      "974 23685.2 13.99 3944.41601562 0.704557 0.753771 0.763626\n",
      "975 23682.0 13.9904 3943.70234375 0.704494 0.75373 0.763518\n",
      "976 23679.0 13.9916 3944.2234375 0.704539 0.753769 0.763627\n",
      "977 23675.8 13.992 3943.440625 0.70447 0.753719 0.763513\n",
      "978 23672.8 13.9933 3943.98164063 0.704517 0.753762 0.763625\n",
      "979 23669.4 13.9936 3943.20703125 0.70445 0.75371 0.763512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "980 23666.4 13.9949 3943.68867188 0.704492 0.75375 0.763615\n",
      "981 23663.2 13.9953 3942.9734375 0.704428 0.753705 0.76351\n",
      "982 23660.3 13.9965 3943.42226562 0.704468 0.753739 0.763609\n",
      "983 23657.2 13.9969 3942.7328125 0.704408 0.753701 0.763506\n",
      "984 23654.1 13.9982 3943.16132813 0.704445 0.75373 0.763606\n",
      "985 23651.0 13.9985 3942.50039062 0.704386 0.753697 0.763507\n",
      "986 23648.0 13.9998 3942.884375 0.70442 0.753723 0.763599\n",
      "987 23644.9 14.0001 3942.25625 0.704365 0.753692 0.763502\n",
      "988 23641.9 14.0014 3942.62421875 0.704396 0.753715 0.763593\n",
      "989 23638.9 14.0017 3941.99609375 0.704341 0.753684 0.763496\n",
      "990 23635.8 14.003 3942.38242188 0.704375 0.753711 0.763588\n",
      "991 23632.8 14.0033 3941.74023438 0.704318 0.75368 0.763491\n",
      "992 23629.9 14.0046 3942.1953125 0.704359 0.75371 0.763594\n",
      "993 23626.8 14.0049 3941.46289062 0.704293 0.753675 0.763485\n",
      "994 23623.9 14.0062 3942.00351563 0.704342 0.753707 0.7636\n",
      "995 23620.8 14.0065 3941.20625 0.70427 0.753669 0.763482\n",
      "996 23617.8 14.0078 3941.74492187 0.704319 0.753704 0.763592\n",
      "997 23614.6 14.0081 3940.99921875 0.704252 0.753662 0.763485\n",
      "998 23611.6 14.0093 3941.44765625 0.704291 0.753697 0.763577\n",
      "999 23608.6 14.0097 3940.78359375 0.704232 0.753658 0.763484\n",
      "1000 23605.6 14.0109 3941.17578125 0.704267 0.75369 0.763568\n",
      "1001 23602.5 14.0113 3940.55078125 0.704211 0.75365 0.76348\n",
      "1002 23599.6 14.0125 3940.93515625 0.704245 0.753679 0.763564\n",
      "1003 23596.6 14.0128 3940.290625 0.704188 0.753642 0.763474\n",
      "1004 23593.8 14.014 3940.7453125 0.704228 0.753675 0.763569\n",
      "1005 23590.8 14.0144 3940.03359375 0.704165 0.753637 0.763467\n",
      "1006 23588.1 14.0156 3940.50273437 0.704206 0.753669 0.76357\n",
      "1007 23585.0 14.0159 3939.79257813 0.704144 0.75363 0.763462\n",
      "1008 23582.2 14.0172 3940.2125 0.704181 0.753658 0.763562\n",
      "1009 23579.2 14.0175 3939.546875 0.704122 0.753622 0.763457\n",
      "1010 23576.3 14.0187 3939.96757812 0.704159 0.753648 0.76356\n",
      "1011 23573.6 14.019 3939.27539062 0.704098 0.753616 0.763449\n",
      "1012 23570.8 14.0203 3939.7640625 0.70414 0.753641 0.763561\n",
      "1013 23567.9 14.0205 3939.0421875 0.704077 0.753614 0.763447\n",
      "1014 23564.9 14.0218 3939.50625 0.704117 0.753636 0.763553\n",
      "1015 23562.0 14.0221 3938.81953125 0.704057 0.753607 0.763447\n",
      "1016 23559.0 14.0233 3939.26875 0.704097 0.753635 0.763546\n",
      "1017 23556.1 14.0236 3938.60273438 0.704037 0.7536 0.763445\n",
      "1018 23553.2 14.0249 3939.021875 0.704075 0.753631 0.76354\n",
      "1019 23550.3 14.0252 3938.38085938 0.704018 0.753595 0.763447\n",
      "1020 23547.4 14.0264 3938.77265625 0.704053 0.753625 0.763535\n",
      "1021 23544.5 14.0267 3938.13984375 0.703996 0.753588 0.763442\n",
      "1022 23541.8 14.0279 3938.60039062 0.704037 0.753626 0.763542\n",
      "1023 23538.8 14.0282 3937.88867188 0.703974 0.753582 0.763435\n",
      "1024 23536.3 14.0295 3938.39609375 0.704019 0.753623 0.763546\n",
      "1025 23533.3 14.0297 3937.6203125 0.703951 0.753576 0.763425\n",
      "1026 23530.7 14.031 3938.175 0.703999 0.753617 0.763547\n",
      "1027 23527.8 14.0312 3937.35078125 0.703925 0.753569 0.763414\n",
      "1028 23525.1 14.0325 3937.94804687 0.703978 0.753609 0.763545\n",
      "1029 23522.3 14.0327 3937.09921875 0.703902 0.753565 0.763409\n",
      "1030 23519.4 14.034 3937.66953125 0.703954 0.753602 0.763534\n",
      "1031 23516.6 14.0342 3936.88203125 0.703883 0.753561 0.763409\n",
      "1032 23513.7 14.0355 3937.403125 0.703931 0.753598 0.763526\n",
      "1033 23511.0 14.0357 3936.66796875 0.703865 0.753555 0.763409\n",
      "1034 23508.0 14.0369 3937.13828125 0.703907 0.75359 0.763516\n",
      "1035 23505.2 14.0372 3936.46640625 0.703846 0.75355 0.76341\n",
      "1036 23502.5 14.0384 3936.9453125 0.703889 0.753588 0.763514\n",
      "1037 23499.6 14.0387 3936.2390625 0.703827 0.753548 0.76341\n",
      "1038 23496.9 14.0399 3936.72421875 0.703869 0.753585 0.763514\n",
      "1039 23494.0 14.0402 3936.02734375 0.703808 0.753544 0.76341\n",
      "1040 23491.2 14.0414 3936.43242187 0.703844 0.753579 0.763503\n",
      "1041 23488.3 14.0417 3935.825 0.703789 0.753543 0.763409\n",
      "1042 23485.6 14.0428 3936.19609375 0.703822 0.753571 0.763496\n",
      "1043 23482.9 14.0431 3935.578125 0.703767 0.753541 0.763402\n",
      "1044 23480.4 14.0443 3936.00546875 0.703805 0.753568 0.763501\n",
      "1045 23477.6 14.0446 3935.31523437 0.703743 0.753536 0.763392\n",
      "1046 23475.1 14.0458 3935.81015625 0.703788 0.753566 0.763503\n",
      "1047 23472.4 14.046 3935.06367188 0.703721 0.753531 0.763384\n",
      "1048 23469.6 14.0472 3935.50703125 0.70376 0.753557 0.763492\n",
      "1049 23467.0 14.0475 3934.828125 0.7037 0.753524 0.763378\n",
      "1050 23464.1 14.0487 3935.28632813 0.703742 0.753553 0.76349\n",
      "1051 23461.7 14.0489 3934.59296875 0.703679 0.753516 0.763375\n",
      "1052 23459.0 14.0501 3935.1140625 0.703726 0.753554 0.763495\n",
      "1053 23456.4 14.0503 3934.34609375 0.703657 0.753509 0.763367\n",
      "1054 23453.6 14.0516 3934.91953125 0.703707 0.75355 0.763494\n",
      "1055 23450.9 14.0518 3934.11796875 0.703637 0.753504 0.763364\n",
      "1056 23448.2 14.0531 3934.678125 0.703687 0.753546 0.763491\n",
      "1057 23445.5 14.0532 3933.91875 0.703619 0.753501 0.763366\n",
      "1058 23442.7 14.0545 3934.40273437 0.703661 0.753541 0.763479\n",
      "1059 23439.9 14.0547 3933.7421875 0.703603 0.7535 0.763374\n",
      "1060 23437.3 14.0559 3934.12421875 0.703637 0.753532 0.763466\n",
      "1061 23434.5 14.0561 3933.53671875 0.703584 0.753498 0.763375\n",
      "1062 23431.9 14.0573 3933.90390625 0.703617 0.753527 0.763462\n",
      "1063 23429.2 14.0575 3933.31132812 0.703564 0.753494 0.763374\n",
      "1064 23426.7 14.0587 3933.69453125 0.703598 0.753523 0.76346\n",
      "1065 23423.9 14.0589 3933.09140625 0.703545 0.753494 0.76337\n",
      "1066 23421.5 14.0601 3933.47734375 0.703579 0.753516 0.763458\n",
      "1067 23418.9 14.0603 3932.8203125 0.703521 0.753488 0.76336\n",
      "1068 23416.4 14.0615 3933.29648438 0.703563 0.753515 0.763465\n",
      "1069 23413.8 14.0617 3932.5671875 0.703497 0.753481 0.76335\n",
      "1070 23411.2 14.0629 3933.053125 0.703541 0.753514 0.763461\n",
      "1071 23408.6 14.0631 3932.32929688 0.703477 0.753474 0.763343\n",
      "1072 23406.0 14.0643 3932.81796875 0.703521 0.753513 0.763458\n",
      "1073 23403.5 14.0645 3932.1 0.703456 0.753467 0.763337\n",
      "1074 23400.9 14.0657 3932.62578125 0.703504 0.75351 0.763463\n",
      "1075 23398.3 14.0659 3931.88125 0.703436 0.753463 0.763335\n",
      "1076 23395.8 14.0671 3932.43476563 0.703486 0.753506 0.763465\n",
      "1077 23393.2 14.0673 3931.64609375 0.703415 0.753459 0.76333\n",
      "1078 23390.8 14.0685 3932.22890625 0.703468 0.7535 0.763461\n",
      "1079 23388.2 14.0686 3931.43515625 0.703397 0.753458 0.763329\n",
      "1080 23385.4 14.0699 3931.9671875 0.703444 0.753493 0.763449\n",
      "1081 23382.9 14.07 3931.25234375 0.70338 0.753457 0.763334\n",
      "1082 23380.2 14.0712 3931.7078125 0.703421 0.753486 0.76344\n",
      "1083 23377.7 14.0714 3931.06679687 0.703364 0.753455 0.763336\n",
      "1084 23375.1 14.0726 3931.45234375 0.703398 0.753482 0.763431\n",
      "1085 23372.6 14.0728 3930.86679688 0.703346 0.753452 0.763336\n",
      "1086 23370.1 14.0739 3931.23203125 0.703379 0.753476 0.763426\n",
      "1087 23367.5 14.0741 3930.67109375 0.703329 0.75345 0.76334\n",
      "1088 23365.0 14.0753 3930.97734375 0.703356 0.753471 0.763416\n",
      "1089 23362.4 14.0755 3930.45390625 0.70331 0.753454 0.763338\n",
      "1090 23360.1 14.0766 3930.83125 0.703343 0.753471 0.763425\n",
      "1091 23357.6 14.0768 3930.19140625 0.703286 0.753448 0.763324\n",
      "1092 23355.3 14.078 3930.65859375 0.703327 0.75347 0.763434\n",
      "1093 23353.0 14.0781 3929.90234375 0.70326 0.753439 0.763307\n",
      "1094 23350.5 14.0794 3930.49140625 0.703313 0.753475 0.763442\n",
      "1095 23348.3 14.0794 3929.64765625 0.703237 0.753429 0.763296\n",
      "1096 23345.6 14.0807 3930.28125 0.703294 0.753475 0.763442\n",
      "1097 23343.2 14.0808 3929.459375 0.70322 0.753425 0.763296\n",
      "1098 23340.4 14.082 3930.01289063 0.703268 0.753469 0.76343\n",
      "1099 23337.9 14.0821 3929.26835937 0.703202 0.753423 0.763301\n",
      "1100 23335.3 14.0833 3929.7609375 0.703246 0.753464 0.763422\n",
      "1101 23332.8 14.0835 3929.06953125 0.703185 0.753421 0.763304\n",
      "1102 23330.2 14.0846 3929.5109375 0.703224 0.753457 0.763411\n",
      "1103 23327.7 14.0848 3928.87890625 0.703167 0.753418 0.763304\n",
      "1104 23325.3 14.0859 3929.31289062 0.703206 0.753453 0.763408\n",
      "1105 23322.9 14.0861 3928.65585938 0.703147 0.753416 0.763303\n",
      "1106 23320.7 14.0873 3929.153125 0.703192 0.753451 0.763413\n",
      "1107 23318.1 14.0874 3928.43984375 0.703128 0.753415 0.763301\n",
      "1108 23315.9 14.0886 3928.94140625 0.703173 0.753451 0.76341\n",
      "1109 23313.3 14.0887 3928.23125 0.70311 0.753416 0.7633\n",
      "1110 23311.0 14.0899 3928.7 0.703151 0.753447 0.763405\n",
      "1111 23308.6 14.09 3928.00429687 0.703089 0.753411 0.763296\n",
      "1112 23306.3 14.0912 3928.50390625 0.703133 0.753448 0.763408\n",
      "1113 23303.8 14.0913 3927.78320312 0.703069 0.753408 0.763293\n",
      "1114 23301.5 14.0925 3928.29804688 0.703115 0.753446 0.763413\n",
      "1115 23299.1 14.0926 3927.5671875 0.70305 0.753403 0.763288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1116 23296.7 14.0938 3928.08203125 0.703096 0.753443 0.763413\n",
      "1117 23294.3 14.0939 3927.34804687 0.70303 0.753398 0.76328\n",
      "1118 23291.9 14.0951 3927.81953125 0.703072 0.753439 0.763402\n",
      "1119 23289.4 14.0952 3927.1640625 0.703013 0.753397 0.763284\n",
      "1120 23286.9 14.0963 3927.54765625 0.703048 0.753433 0.763393\n",
      "1121 23284.6 14.0965 3926.946875 0.702995 0.753394 0.763284\n",
      "1122 23282.1 14.0976 3927.3390625 0.70303 0.753429 0.763391\n",
      "1123 23280.0 14.0977 3926.72265625 0.702974 0.753392 0.763281\n",
      "1124 23277.3 14.0989 3927.1359375 0.703011 0.753421 0.763388\n",
      "1125 23275.1 14.099 3926.50078125 0.702955 0.753391 0.763278\n",
      "1126 23272.8 14.1002 3926.9765625 0.702998 0.75342 0.763393\n",
      "1127 23270.6 14.1003 3926.24921875 0.702932 0.753386 0.763272\n",
      "1128 23268.3 14.1015 3926.81757813 0.702983 0.753421 0.7634\n",
      "1129 23265.9 14.1015 3926.03203125 0.702913 0.753383 0.763272\n",
      "1130 23263.6 14.1027 3926.5875 0.702962 0.75342 0.763397\n",
      "1131 23261.2 14.1028 3925.82421875 0.702894 0.753379 0.763269\n",
      "1132 23258.9 14.104 3926.36640625 0.702942 0.753417 0.763394\n",
      "1133 23256.6 14.1041 3925.6046875 0.702874 0.753376 0.763264\n",
      "1134 23254.3 14.1053 3926.15234375 0.702922 0.753416 0.763392\n",
      "1135 23252.0 14.1053 3925.39609375 0.702856 0.753375 0.763259\n",
      "1136 23249.7 14.1065 3925.9265625 0.702903 0.753413 0.76339\n",
      "1137 23247.6 14.1065 3925.178125 0.702836 0.753373 0.763255\n",
      "1138 23245.1 14.1078 3925.7140625 0.702884 0.75341 0.763387\n",
      "1139 23243.1 14.1078 3924.95078125 0.702815 0.753368 0.76325\n",
      "1140 23240.5 14.109 3925.48984375 0.702864 0.753406 0.763386\n",
      "1141 23238.4 14.109 3924.7703125 0.7028 0.75337 0.763254\n",
      "1142 23235.8 14.1103 3925.2234375 0.70284 0.753403 0.763374\n",
      "1143 23233.6 14.1103 3924.5953125 0.702784 0.753369 0.76326\n",
      "1144 23231.0 14.1115 3924.96484375 0.702817 0.753396 0.763364\n",
      "1145 23228.9 14.1116 3924.41953125 0.702768 0.753367 0.763264\n",
      "1146 23226.4 14.1127 3924.77539062 0.7028 0.753391 0.763362\n",
      "1147 23224.2 14.1128 3924.2109375 0.70275 0.753367 0.763268\n",
      "1148 23221.9 14.114 3924.5875 0.702783 0.753388 0.763367\n",
      "1149 23219.8 14.114 3923.97890625 0.702728 0.753366 0.763262\n",
      "1150 23217.7 14.1152 3924.45078125 0.70277 0.75339 0.763379\n",
      "1151 23215.7 14.1152 3923.71796875 0.702705 0.753362 0.76325\n",
      "1152 23213.5 14.1165 3924.29375 0.702757 0.753395 0.763385\n",
      "1153 23211.3 14.1164 3923.49140625 0.702684 0.753359 0.763243\n",
      "1154 23208.9 14.1177 3924.06484375 0.702737 0.753395 0.763378\n",
      "1155 23206.7 14.1177 3923.29960937 0.702668 0.753357 0.763244\n",
      "1156 23204.3 14.1189 3923.8234375 0.702715 0.753394 0.763371\n",
      "1157 23201.9 14.1189 3923.11328125 0.70265 0.753355 0.763246\n",
      "1158 23199.6 14.1201 3923.59140625 0.702694 0.753389 0.763363\n",
      "1159 23197.4 14.1201 3922.9109375 0.702633 0.753354 0.763245\n",
      "1160 23195.1 14.1213 3923.37460938 0.702675 0.753389 0.763357\n",
      "1161 23192.8 14.1213 3922.7046875 0.702615 0.753351 0.763242\n",
      "1162 23190.6 14.1225 3923.15234375 0.702655 0.753387 0.763354\n",
      "1163 23188.4 14.1225 3922.49570313 0.702596 0.753344 0.763236\n",
      "1164 23186.2 14.1237 3922.940625 0.702636 0.753384 0.763352\n",
      "1165 23184.0 14.1237 3922.28320312 0.702577 0.753341 0.763233\n",
      "1166 23181.8 14.1249 3922.740625 0.702618 0.753383 0.763351\n",
      "1167 23179.6 14.1249 3922.0703125 0.702558 0.753338 0.763232\n",
      "1168 23177.4 14.1261 3922.5484375 0.702601 0.75338 0.763351\n",
      "1169 23175.1 14.1261 3921.87617188 0.702541 0.753338 0.763233\n",
      "1170 23173.0 14.1273 3922.33203125 0.702581 0.753376 0.76335\n",
      "1171 23170.9 14.1273 3921.67070312 0.702522 0.753339 0.763233\n",
      "1172 23168.9 14.1285 3922.171875 0.702567 0.753377 0.763358\n",
      "1173 23166.8 14.1285 3921.4515625 0.702503 0.753337 0.763225\n",
      "1174 23164.7 14.1297 3921.978125 0.702549 0.753376 0.763359\n",
      "1175 23162.6 14.1296 3921.23125 0.702482 0.753334 0.763218\n",
      "1176 23160.4 14.1309 3921.77382812 0.702531 0.753373 0.763358\n",
      "1177 23158.4 14.1308 3921.00703125 0.702463 0.753334 0.763213\n",
      "1178 23156.0 14.1321 3921.55507812 0.702511 0.753366 0.763352\n",
      "1179 23154.0 14.132 3920.81328125 0.702446 0.753333 0.763214\n",
      "1180 23151.5 14.1332 3921.30820313 0.702489 0.753359 0.763341\n",
      "1181 23149.4 14.1332 3920.64414063 0.702431 0.753334 0.763222\n",
      "1182 23146.9 14.1343 3921.04921875 0.702467 0.753354 0.763326\n",
      "1183 23144.9 14.1344 3920.47070312 0.702415 0.753333 0.763228\n",
      "1184 23142.4 14.1355 3920.81328125 0.702445 0.753353 0.763316\n",
      "1185 23140.4 14.1356 3920.28671875 0.702398 0.753334 0.763228\n",
      "1186 23138.3 14.1366 3920.64179688 0.70243 0.753353 0.763318\n",
      "1187 23136.4 14.1367 3920.04296875 0.702377 0.753329 0.763215\n",
      "1188 23134.2 14.1378 3920.49882812 0.702418 0.753359 0.763328\n",
      "1189 23132.4 14.1378 3919.803125 0.702355 0.753325 0.763202\n",
      "1190 23130.4 14.1391 3920.37539062 0.702406 0.753362 0.763345\n",
      "1191 23128.7 14.1389 3919.54882812 0.702332 0.753317 0.763185\n",
      "1192 23126.5 14.1403 3920.221875 0.702393 0.753364 0.763352\n",
      "1193 23124.5 14.1401 3919.33515625 0.702313 0.753313 0.763183\n",
      "1194 23122.2 14.1414 3920.0140625 0.702374 0.753361 0.763349\n",
      "1195 23120.0 14.1412 3919.1484375 0.702297 0.753311 0.763186\n",
      "1196 23117.7 14.1425 3919.75625 0.70235 0.753358 0.763337\n",
      "1197 23115.4 14.1424 3918.9765625 0.702281 0.753311 0.763193\n",
      "1198 23113.3 14.1436 3919.53554687 0.702331 0.753357 0.763331\n",
      "1199 23111.1 14.1435 3918.78828125 0.702265 0.75331 0.763196\n",
      "1200 23108.8 14.1447 3919.28671875 0.702309 0.753352 0.763321\n",
      "1201 23106.6 14.1447 3918.60976562 0.702248 0.753311 0.763199\n",
      "1202 23104.6 14.1458 3919.07734375 0.702289 0.753346 0.763313\n",
      "1203 23102.5 14.1458 3918.40546875 0.70223 0.753315 0.763196\n",
      "1204 23100.3 14.147 3918.85078125 0.70227 0.753339 0.763305\n",
      "1205 23098.2 14.147 3918.22382813 0.702213 0.753314 0.7632\n",
      "1206 23096.0 14.1481 3918.621875 0.702249 0.753336 0.763297\n",
      "1207 23093.9 14.1481 3918.04609375 0.702197 0.753313 0.763201\n",
      "1208 23091.9 14.1492 3918.41523438 0.70223 0.753332 0.763291\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-9b5b0ef11eeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_rmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_rmse_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_rmse_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_cost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_cost\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrmse_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrmse_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_rmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_rmse_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_rmse_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Suyixin/anaconda/envs/ws/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Suyixin/anaconda/envs/ws/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Suyixin/anaconda/envs/ws/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/Suyixin/anaconda/envs/ws/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Suyixin/anaconda/envs/ws/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "for i in range(100001):\n",
    "    _, c, r, t, _rmse, _rmse_v, _rmse_t = sess.run([training_step, base_cost, regularizer, total_cost,rmse, rmse_valid, rmse_test])\n",
    "    sess.run(clip)\n",
    "    print(i, c,r,t/10, _rmse, _rmse_v, _rmse_t)\n",
    "    #print(sess.run(md_var)[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(d_var)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id_2_genre = pickle.load(open('../data/'+folder+'/id_2_genre.pkl', 'rb'))\n",
    "id_2_director = pickle.load(open('../data/'+folder+'/id_2_director.pkl', 'rb'))\n",
    "id_2_cast = pickle.load(open('../data/'+folder+'/id_2_cast.pkl', 'rb'))\n",
    "genre_2_id = pickle.load(open('../data/'+folder+'/genre_2_id.pkl', 'rb'))\n",
    "director_2_id = pickle.load(open('../data/'+folder+'/director_2_id.pkl', 'rb'))\n",
    "cast_2_id = pickle.load(open('../data/'+folder+'/cast_2_id.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d_list = sess.run(d_result)\n",
    "mean_list = np.mean(d_list, axis=0)\n",
    "\n",
    "with open('director_dtcg.csv','w') as f:\n",
    "    f_csv = csv.writer(f)\n",
    "    headers = ['name', 'score']\n",
    "    f_csv.writerow(headers)\n",
    "    for i in id_2_director:\n",
    "        rows = [id_2_director[i], mean_list[i]]\n",
    "        f_csv.writerow(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_list = sess.run(c_result)\n",
    "mean_list = np.mean(c_list, axis=0)\n",
    "\n",
    "with open('cast_dtcg.csv','w') as f:\n",
    "    f_csv = csv.writer(f)\n",
    "    headers = ['name', 'score']\n",
    "    f_csv.writerow(headers)\n",
    "    for i in id_2_cast:\n",
    "        rows = [id_2_cast[i], mean_list[i]]\n",
    "        f_csv.writerow(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g_list = sess.run(g_result)\n",
    "mean_list = np.mean(g_list, axis=0)\n",
    "\n",
    "with open('genre_dtcg.csv','w') as f:\n",
    "    f_csv = csv.writer(f)\n",
    "    headers = ['name', 'score']\n",
    "    f_csv.writerow(headers)\n",
    "    for i in id_2_genre:\n",
    "        rows = [id_2_genre[i], mean_list[i]]\n",
    "        f_csv.writerow(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate Training RMSE\n",
    "\"\"\"\n",
    "g_user_test = g_user\n",
    "g_item_test = g_movie\n",
    "g_rate_test = g_rating\n",
    "g_result_flatten = g_result_flatten\n",
    "#genre rmse\n",
    "g_test = tf.gather(g_result_flatten, g_user_test * tf.shape(g_result)[1] + g_item_test, name='g_test')\n",
    "g_rmse = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(g_rate_test, g_test))))\n",
    "\n",
    "d_user_test = d_user\n",
    "d_item_test = d_movie\n",
    "d_rate_test = d_rating\n",
    "d_result_flatten = d_result_flatten\n",
    "#genre rmse\n",
    "d_test = tf.gather(d_result_flatten, d_user_test * tf.shape(d_result)[1] + d_item_test, name='d_test')\n",
    "d_rmse = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(d_rate_test, d_test))))\n",
    "\n",
    "t_user_test = t_user\n",
    "t_item_test = t_movie\n",
    "t_rate_test = t_rating\n",
    "t_result_flatten = t_result_flatten\n",
    "#genre rmse\n",
    "t_test = tf.gather(t_result_flatten, t_user_test * tf.shape(t_result)[1] + t_item_test, name='t_test')\n",
    "t_rmse = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(t_rate_test, t_test))))\n",
    "\n",
    "c_user_test = c_user\n",
    "c_item_test = c_movie\n",
    "c_rate_test = c_rating\n",
    "c_result_flatten = c_result_flatten\n",
    "#genre rmse\n",
    "c_test = tf.gather(c_result_flatten, c_user_test * tf.shape(c_result)[1] + c_item_test, name='c_test')\n",
    "c_rmse = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(c_rate_test, c_test))))\n",
    "\n",
    "\n",
    "rmse = tf.divide((g_rmse + d_rmse + t_rmse + c_rmse), 4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate Validation RMSE\n",
    "\"\"\"\n",
    "g_user_v = g_user_v\n",
    "g_item_v = g_movie_v\n",
    "g_rate_v = g_rating_v\n",
    "g_result_flatten = g_result_flatten\n",
    "#genre rmse\n",
    "g_v = tf.gather(g_result_flatten, g_user_v * tf.shape(g_result)[1] + g_item_v, name='g_v')\n",
    "g_rmse_v = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(g_rate_v, g_v))))\n",
    "\n",
    "d_user_v = d_user_v\n",
    "d_item_v = d_movie_v\n",
    "d_rate_v = d_rating_v\n",
    "d_result_flatten = d_result_flatten\n",
    "#genre rmse\n",
    "d_v = tf.gather(d_result_flatten, d_user_v * tf.shape(d_result)[1] + d_item_v, name='d_v')\n",
    "d_rmse_v = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(d_rate_v, d_v))))\n",
    "\n",
    "t_user_v = t_user_v\n",
    "t_item_v = t_movie_v\n",
    "t_rate_v = t_rating_v\n",
    "t_result_flatten = t_result_flatten\n",
    "#genre rmse\n",
    "t_v = tf.gather(t_result_flatten, t_user_v * tf.shape(t_result)[1] + t_item_v, name='t_v')\n",
    "t_rmse_v = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(t_rate_v, t_v))))\n",
    "\n",
    "c_user_v = c_user_v\n",
    "c_item_v = c_movie_v\n",
    "c_rate_v = c_rating_v\n",
    "c_result_flatten = c_result_flatten\n",
    "#genre rmse\n",
    "c_v = tf.gather(c_result_flatten, c_user_v * tf.shape(c_result)[1] + c_item_v, name='c_v')\n",
    "c_rmse_v = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(c_rate_v, c_v))))\n",
    "\n",
    "\n",
    "rmse_v = tf.divide((g_rmse_v + d_rmse_v + t_rmse_v + c_rmse_v), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#training_trend_f = open('../data/'+folder+'/trends.csv', 'w')\n",
    "#csv_trend = csv.writer(training_trend_f)\n",
    "#headers = ['round','train_total', 'train_g', 'train_d', 'train_t', 'train_c', 'valid_total', 'valid_g', 'valid_d', 'valid_t', 'valid_c']\n",
    "#csv_trend.writerow(headers)\n",
    "lowest_num = 0\n",
    "lowest_rmse = 9999999\n",
    "not_dec_num = 0\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "for i in range(100001):\n",
    "    _, c, r = sess.run([training_step, cost, regularizer])\n",
    "    if i%20 == 0:\n",
    "        _rmse_v, _g_rmse_v, _d_rmse_v, _t_rmse_v, _c_rmse_v  = sess.run([rmse_v, g_rmse_v, d_rmse_v, t_rmse_v, c_rmse_v])\n",
    "        sess.run(clip)\n",
    "        if lowest_rmse > _rmse_v + 0.001:\n",
    "            lowest_rmse = _rmse_v\n",
    "            lowest_num = i\n",
    "            not_dec_num = 0\n",
    "        else:\n",
    "            not_dec_num += 1\n",
    "            if not_dec_num > 10:\n",
    "                break\n",
    "            \n",
    "        print(i, c, \"current rmsr: \", _rmse_v,  \"lowest rmse: \", lowest_rmse, \"lowest num: \", lowest_num)\n",
    "        print(_g_rmse_v, _d_rmse_v, _t_rmse_v, _c_rmse_v)\n",
    "        _rmse, _g_rmse, _d_rmse, _t_rmse, _c_rmse = sess.run([rmse, g_rmse, d_rmse, t_rmse, c_rmse])\n",
    "        #w_row = [i, _rmse, _g_rmse, _d_rmse, _t_rmse, _c_rmse, _rmse_v, _g_rmse_v, _d_rmse_v, _t_rmse_v, _c_rmse_v]\n",
    "        #csv_trend.writerow(w_row)\n",
    "    if i%200 == 0:\n",
    "        print()\n",
    "        print(\"Traing RMSR(total, g, d, t, c):\", i , _rmse, _g_rmse, _d_rmse, _t_rmse, _c_rmse)\n",
    "        print()\n",
    "#training_trend_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_matrix(row_num, col_num, user, movie, rating):\n",
    "    # row is movie infor num\n",
    "    # col is user num\n",
    "    matr = [[0 for i in range(col_num)] for j in range(row_num)]\n",
    "    for i in range(len(user)):\n",
    "        matr[movie[i]][user[i]] = float(rating[i])\n",
    "    \n",
    "    return matr\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_v, director_v, topic_v, genre_v, cast_v = sess.run([U,D,T,G,C])\n",
    "\n",
    "d_mat = generate_matrix(DIRECTOR_NUM, USER_NUM, d_user, d_movie, d_rating)\n",
    "t_mat = generate_matrix(TOPIC_NUM, USER_NUM, t_user, t_movie, t_rating)\n",
    "c_mat = generate_matrix(CAST_NUM, USER_NUM, c_user, c_movie, c_rating)\n",
    "g_mat = generate_matrix(GENRE_NUM, USER_NUM, g_user, g_movie, g_rating)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(d_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id_2_genre = pickle.load(open('../data/'+folder+'/id_2_genre.pkl', 'rb'))\n",
    "id_2_director = pickle.load(open('../data/'+folder+'/id_2_director.pkl', 'rb'))\n",
    "id_2_cast = pickle.load(open('../data/'+folder+'/id_2_cast.pkl', 'rb'))\n",
    "genre_2_id = pickle.load(open('../data/'+folder+'/genre_2_id.pkl', 'rb'))\n",
    "director_2_id = pickle.load(open('../data/'+folder+'/director_2_id.pkl', 'rb'))\n",
    "cast_2_id = pickle.load(open('../data/'+folder+'/cast_2_id.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_rating_list(info_id_list, info_matrix, user_vector, user_id, matrix):\n",
    "    result_list = []\n",
    "    for _id in info_id_list:\n",
    "        if matrix[_id][user_id] > 0:\n",
    "            result_list.append(matrix[_id][user_id]) \n",
    "        else:\n",
    "            \n",
    "            _v = info_matrix[:, int(_id)]\n",
    "            _rating = np.dot(user_vector, _v)\n",
    "            result_list.append(_rating)\n",
    "    return result_list\n",
    "\n",
    "def get_rating_list_train(id_tup_list, matrix):\n",
    "    result_list = []\n",
    "    for tup in id_tup_list:\n",
    "        user_id = tup[0]\n",
    "        info_id = tup[1]\n",
    "        result_list.append(matrix[info_id][user_id])\n",
    "\n",
    "def gen_regr_data(m_id, user_rating_tups, is_training = True):\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    user_id_train = []\n",
    "    for tup in user_rating_tups:\n",
    "        one_user_x = []\n",
    "        rating = float(tup[1])\n",
    "        y_train.append(rating)\n",
    "        \n",
    "        user_id = int(tup[0])\n",
    "        user_id_train.append(user_id)\n",
    "        feature_dict = movie_feature_dict[m_id]\n",
    "        u_v = user_v[user_id]\n",
    "        \n",
    "        d_x = get_rating_list(feature_dict['d'], director_v, u_v,user_id, d_mat)\n",
    "        g_x = get_rating_list(feature_dict['g'], genre_v, u_v, user_id, g_mat)\n",
    "        t_x = get_rating_list(feature_dict['t'], topic_v, u_v, user_id, t_mat)\n",
    "        c_x = get_rating_list(feature_dict['c'], cast_v, u_v, user_id, c_mat)\n",
    "        #print(user_id, d_x, g_x, t_x, c_x)\n",
    "        \n",
    "        \n",
    "        one_user_x = d_x + g_x + t_x + c_x\n",
    "        #one_user_x = d_x + g_x + c_x\n",
    "        x_train.append(one_user_x)\n",
    "        \n",
    "        feature_name = []\n",
    "        feature_name += [\"d_\" + id_2_director[_id] for _id in feature_dict['d']]\n",
    "        feature_name += [\"g_\" + id_2_genre[_id] for _id in feature_dict['g']]\n",
    "        feature_name += [\"t_\" + str(_id) for _id in feature_dict['t']]\n",
    "        feature_name += [\"c_\" + id_2_cast[_id] for _id in feature_dict['c']]\n",
    "        feature_name += [\"bias\"]\n",
    "        \n",
    "        \n",
    "    return x_train, y_train, feature_name, user_id_train\n",
    "        \n",
    "            \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_count_list = pickle.load(open('../data/'+folder+'/user_count_list.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name_list = ['d', 't', 'c', 'g']\n",
    "divid_list = []\n",
    "for name in name_list:\n",
    "    summ = 0\n",
    "    total = 0\n",
    "    for user in user_count_list[name]:\n",
    "        for key in user_count_list[name][user]:\n",
    "            summ += user_count_list[name][user][key]\n",
    "            total += 1\n",
    "\n",
    "    divid_list.append(summ/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store result matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_d_result, _t_result, _g_result, _c_result =  sess.run([d_result, t_result, g_result, c_result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(_d_result, open('../data/'+folder+'/ranking_matrix_director.pkl', 'wb'))\n",
    "pickle.dump(_t_result, open('../data/'+folder+'/ranking_matrix_type.pkl', 'wb'))\n",
    "pickle.dump(_g_result, open('../data/'+folder+'/ranking_matrix_genre.pkl', 'wb'))\n",
    "pickle.dump(_c_result, open('../data/'+folder+'/ranking_matrix_cast.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "import numpy as np\n",
    "\n",
    "def select_feature_index(feature, n):\n",
    "    index_list = [0 for i in range(len(feature))]\n",
    "    temp_f = [abs(feature[i]) for i in range(len(feature))]\n",
    "    for i in (range(n)):\n",
    "        min_index = temp_f.index(max(temp_f))\n",
    "        index_list[min_index] = 1\n",
    "        temp_f[min_index] = 0\n",
    "    \n",
    "    return index_list\n",
    "\n",
    "def get_predict_rating_old(user_rating, features):\n",
    "    return np.mean([user_rating[i] + features[i] for i in range(len(user_rating))])\n",
    "\n",
    "\n",
    "def get_predict_rating(user_rating, features):\n",
    "    weight_n = [float(abs(ur)) for ur in user_rating]\n",
    "    summ = sum(weight_n)\n",
    "    weight = [ur/summ for ur in weight_n]\n",
    "    #weight = [1./len(weight_n) for ur in weight_n]\n",
    "    \n",
    "    weight_total = sum([weight[i]* (user_rating[i] + features[i]) for i in range(len(weight))])\n",
    "    \n",
    "    return weight_total\n",
    "\n",
    "def get_predict_rating_select(user_rating, features, n):\n",
    "    if len(user_rating) < n:\n",
    "        return get_predict_rating_old(user_rating, features)\n",
    "    else:\n",
    "        index_list = select_feature_index(user_rating, n)\n",
    "        account_list = []\n",
    "        for i in range(len(user_rating)):\n",
    "            if index_list[i] == 1:\n",
    "                account_list.append(user_rating[i] + features[i])\n",
    "        return np.mean(account_list)\n",
    "\n",
    "def get_predict_rating_count(user_rating, features, user_id, feature_name):\n",
    "    weight = []\n",
    "    weight_sum = 0\n",
    "    offset = 0.01\n",
    "    for j in range(len(feature_name)-1):\n",
    "        name = feature_name[j]\n",
    "        dict_name = name.split('_')[0]\n",
    "        feature_n = name.split('_')[1]\n",
    "        try:\n",
    "            if dict_name == 'd':\n",
    "                feature_key = director_2_id[feature_n]\n",
    "                offset = 1/divid_list[0]\n",
    "            elif dict_name == 'c':\n",
    "                feature_key = cast_2_id[feature_n]\n",
    "                offset = 1/divid_list[2]\n",
    "            elif dict_name == 'g':\n",
    "                feature_key = genre_2_id[feature_n]\n",
    "                offset = 1/divid_list[3]\n",
    "            elif dict_name == 't':\n",
    "                feature_key = int(feature_n)\n",
    "                offset = 1/divid_list[1]\n",
    "        except:\n",
    "            feature_key = ''\n",
    "        try:\n",
    "            prop = user_count_list[dict_name][user_id][feature_key]\n",
    "        except:\n",
    "            prop = 1\n",
    "        prop = offset * prop\n",
    "        weight_sum += prop\n",
    "        weight.append(prop)      \n",
    "    \n",
    "    weight = [w/float(weight_sum) for w in weight]\n",
    "    result = 0\n",
    "    for i in range(len(user_rating)):\n",
    "        #name = feature_name[i]\n",
    "        #dict_name = name.split('_')[0]\n",
    "        #ppr = (user_rating[i] + features[i]) * weight[i]\n",
    "        result += (user_rating[i] + features[i]) * weight[i]\n",
    "    \"\"\"\n",
    "    print(user_id)\n",
    "    for i in range(len(weight)):\n",
    "        print(feature_name[i], end='')\n",
    "        print(\" %.2f %.2f %.2f\" % (user_rating[i], features[i], weight[i]))\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equal weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_predict = np.array([])\n",
    "total_rating = np.array([])\n",
    "total_predict_v = np.array([])\n",
    "total_rating_v = np.array([])\n",
    "total_predict_tr = np.array([])\n",
    "total_rating_tr = np.array([])\n",
    "#train_rmse_compare_F = []\n",
    "for m in range(0, MOVIE_NUM+1):\n",
    "    if m not in movie_rating:\n",
    "        print(\"not in:\", m)\n",
    "        continue\n",
    "    # tuples is (userid, movierating)\n",
    "    user_rating_tups = movie_rating[m]\n",
    "    x_train, y_train, feature_name, user_id_train = gen_regr_data(int(m), user_rating_tups)\n",
    "    \n",
    "    if len(y_train) < 5:\n",
    "        print('less than 5:', m)\n",
    "        continue\n",
    "    #x_train = x_train[1:2]\n",
    "    #y_train = y_train[1:2]\n",
    "    \n",
    "    #if len(y_train) > 50:\n",
    "     #   continue\n",
    "    \n",
    "    if m in movie_rating_test:     \n",
    "        user_rating_tups_test = movie_rating_test[m]\n",
    "        x_test, y_test, _, user_id_test= gen_regr_data(int(m), user_rating_tups_test)\n",
    "    else:\n",
    "        x_test = []\n",
    "        y_test = []\n",
    "        user_id_test = []\n",
    "    \n",
    "    if m in movie_rating_validation:     \n",
    "        user_rating_tups_validation = movie_rating_validation[m]\n",
    "        x_valid, y_valid, _, user_id_valid= gen_regr_data(int(m), user_rating_tups_validation)\n",
    "    else:\n",
    "        x_valid = []\n",
    "        y_valid = []\n",
    "        user_id_valid = []\n",
    "        \n",
    "    print(len(x_train), len(x_test), len(x_valid))\n",
    "    feature = [0 for i in range(len(feature_name)-1)]\n",
    "    \n",
    "    for j in range(len(feature_name)-1):\n",
    "        name = feature_name[j]\n",
    "        dict_name = name.split('_')[0]\n",
    "        feature_n = name.split('_')[1]\n",
    "\n",
    "        \n",
    "        for i in range(len(x_train)):\n",
    "            feature[j] += (y_train[i] - x_train[i][j])\n",
    "        \n",
    "        feature[j] /= len(x_train)\n",
    "    \n",
    "    #weight = [1 - abs(f) for f in feature] if len(feature) > 0 else []\n",
    "    #select_indexs = select_feature_index(feature, 100)\n",
    "    #select_indexs = [1 for i in range(len(feature))]\n",
    "    #for i in range(len(feature)):\n",
    "        #print(feature_name[i], 1/(1-feature[i]))\n",
    "        #if select_indexs[i] == 1:\n",
    "            #print(feature_name[i], feature[i], weight[i])\n",
    "    #print()\n",
    "    \n",
    "\n",
    "    predict_t = []\n",
    "    predict_v = []\n",
    "    predict_tr = []\n",
    "    \n",
    "    for i in range(len(x_train)):\n",
    "        cc = get_predict_rating_old(x_train[i], feature)\n",
    "        #print(\"%.2f %.2f\" %(cc, y_train[i]))\n",
    "        #print()\n",
    "        predict_tr.append(cc)\n",
    "    \n",
    "    for i in range(len(x_test)):\n",
    "        cc = get_predict_rating_old(x_test[i], feature)\n",
    "        #print(\"%.2f %.2f\" %(cc, y_test[i]))\n",
    "        #print()\n",
    "        predict_t.append(cc)\n",
    "    \n",
    "\n",
    "    for i in range(len(x_valid)):\n",
    "        cc = get_predict_rating_old(x_valid[i], feature)\n",
    "        predict_v.append(cc)\n",
    "        #print(\"%.2f %.2f\" %(cc, y_valid[i]))\n",
    "        #print()\n",
    "     \n",
    "    train_rmse = np.sqrt(np.mean(np.square(np.subtract(predict_tr, y_train))))\n",
    "    valid_rmse = np.sqrt(np.mean(np.square(np.subtract(predict_v, y_valid))))\n",
    "    test_rmse = np.sqrt(np.mean(np.square(np.subtract(predict_t, y_test))))\n",
    "    \n",
    "    #print(m, train_rmse, valid_rmse, test_rmse)\n",
    "    #train_rmse_compare_F.append([m, len(y_train), len(y_valid),len(y_test), train_rmse, valid_rmse, test_rmse])\n",
    "    \n",
    "    total_predict = np.concatenate((total_predict, predict_t), axis = 0)\n",
    "    total_rating = np.concatenate((total_rating, y_test), axis = 0)\n",
    "    total_predict_v = np.concatenate((total_predict_v, predict_v), axis = 0)\n",
    "    total_rating_v = np.concatenate((total_rating_v, y_valid), axis = 0)\n",
    "    total_predict_tr = np.concatenate((total_predict_tr, predict_tr), axis = 0)\n",
    "    total_rating_tr = np.concatenate((total_rating_tr, y_train), axis = 0)\n",
    "    \n",
    "    #print(feature)\n",
    "    #print(feature_name)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rmse = np.sqrt(np.mean(np.square(np.subtract(total_predict, total_rating))))  \n",
    "print(\"train rmse:\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(total_predict)):\n",
    "    if total_predict[i] < 0:\n",
    "        total_predict[i] = 0.\n",
    "    if total_predict[i] > 5:\n",
    "        total_predict[i] = 5.\n",
    "\n",
    "for i in range(len(total_predict_v)):\n",
    "    if total_predict_v[i] < 0:\n",
    "        total_predict_v[i] = 0.\n",
    "    if total_predict_v[i] > 5:\n",
    "        total_predict_v[i] = 5.\n",
    "        \n",
    "for i in range(len(total_predict_tr)):\n",
    "    if total_predict_tr[i] < 0:\n",
    "        total_predict_tr[i] = 0.\n",
    "    if total_predict_tr[i] > 5:\n",
    "        total_predict_tr[i] = 5.\n",
    "\n",
    "        \n",
    "rmse = np.sqrt(np.mean(np.square(np.subtract(total_predict_tr, total_rating_tr))))  \n",
    "print(\"train rmse:\", rmse)\n",
    "\n",
    "mae = mean_absolute_error(total_rating_tr, total_predict_tr)\n",
    "print(\"train mae:\", mae)\n",
    "\n",
    "rmse = np.sqrt(np.mean(np.square(np.subtract(total_predict_v, total_rating_v))))  \n",
    "print(\"valid rmse:\", rmse)\n",
    "\n",
    "mae = mean_absolute_error(total_rating_v, total_predict_v)\n",
    "print(\"valid mae:\", mae)\n",
    "\n",
    "rmse = np.sqrt(np.mean(np.square(np.subtract(total_predict, total_rating))))  \n",
    "print(\"test rmse:\", rmse)\n",
    "\n",
    "mae = mean_absolute_error(total_rating, total_predict)\n",
    "print(\"test mae:\", mae)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# weight with number of featured movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_predict = np.array([])\n",
    "total_rating = np.array([])\n",
    "total_predict_v = np.array([])\n",
    "total_rating_v = np.array([])\n",
    "total_predict_tr = np.array([])\n",
    "total_rating_tr = np.array([])\n",
    "train_rmse_compare_E = []\n",
    "movie_feature = {}\n",
    "feature_name_dict = {}\n",
    "dropped_movie = 0\n",
    "for m in range(0, MOVIE_NUM+1):\n",
    "    if m not in movie_rating:\n",
    "        print('not in', m)\n",
    "        dropped_movie += 1\n",
    "        continue\n",
    "    user_rating_tups = movie_rating[m]\n",
    "    x_train, y_train, feature_name, user_id_train = gen_regr_data(int(m), user_rating_tups)\n",
    "    \n",
    "    if len(y_train) < 5:\n",
    "        print('less than 5:', m)\n",
    "        dropped_movie += 1\n",
    "        continue\n",
    "    #if len(y_train) > 50:\n",
    "     #   continue\n",
    "    \n",
    "    if m in movie_rating_test:     \n",
    "        user_rating_tups_test = movie_rating_test[m]\n",
    "        x_test, y_test, _, user_id_test= gen_regr_data(int(m), user_rating_tups_test)\n",
    "    else:\n",
    "        x_test = []\n",
    "        y_test = []\n",
    "        user_id_test = []\n",
    "    \n",
    "    if m in movie_rating_validation:     \n",
    "        user_rating_tups_validation = movie_rating_validation[m]\n",
    "        x_valid, y_valid, _, user_id_valid= gen_regr_data(int(m), user_rating_tups_validation)\n",
    "    else:\n",
    "        x_valid = []\n",
    "        y_valid = []\n",
    "        user_id_valid = []\n",
    "    \n",
    "    if len(x_test) == 0:\n",
    "        continue\n",
    "        \n",
    "    \n",
    "    feature = [0 for i in range(len(feature_name)-1)]\n",
    "    \n",
    "    for j in range(len(feature_name)-1):\n",
    "        prop_sum = 0\n",
    "        name = feature_name[j]\n",
    "        dict_name = name.split('_')[0]\n",
    "        feature_n = name.split('_')[1]\n",
    "        offset = 0.01\n",
    "        try:\n",
    "            if dict_name == 'd':\n",
    "                feature_key = director_2_id[feature_n]\n",
    "                offset = 1/divid_list[0]\n",
    "            elif dict_name == 'c':\n",
    "                feature_key = cast_2_id[feature_n]\n",
    "                offset = 1/divid_list[2]\n",
    "            elif dict_name == 'g':\n",
    "                feature_key = genre_2_id[feature_n]\n",
    "                offsedivid_listt = 1/divid_list[3]\n",
    "            elif dict_name == 't':\n",
    "                feature_key = int(feature_n)\n",
    "                offset = 1/divid_list[1]\n",
    "        except:\n",
    "            feature_key = ''\n",
    "        \n",
    "        for i in range(len(x_train)):\n",
    "            try:\n",
    "                prop = user_count_list[dict_name][user_id_train[i]][feature_key]\n",
    "            except:\n",
    "                prop = 1\n",
    "            prop = offset * prop\n",
    "            #prop_sum += prop\n",
    "            #feature[j] += (y_train[i] - x_train[i][j]) * prop\n",
    "            prop_sum += 1\n",
    "            feature[j] += (y_train[i] - x_train[i][j])\n",
    "        \n",
    "        feature[j] /= prop_sum\n",
    "    feature_name_dict[m] = feature_name\n",
    "    movie_feature[m] = feature\n",
    "    #weight = [1 - abs(f) for f in feature] if len(feature) > 0 else []\n",
    "    #select_indexs = select_feature_index(feature, 100)\n",
    "    #select_indexs = [1 for i in range(len(feature))]\n",
    "    #for i in range(len(feature)):\n",
    "        #print(feature_name[i], 1/(1-feature[i]))\n",
    "        #if select_indexs[i] == 1:\n",
    "            #print(feature_name[i], feature[i], weight[i])\n",
    "    #print()\n",
    "    \n",
    "\n",
    "    predict_t = []\n",
    "    predict_v = []\n",
    "    predict_tr = []\n",
    "    \n",
    "    for i in range(len(x_train)):\n",
    "        cc = get_predict_rating_count(x_train[i], feature, user_id_train[i], feature_name)\n",
    "        #print(\"%.2f %.2f\" %(cc, y_train[i]))\n",
    "        #print()\n",
    "        predict_tr.append(cc)\n",
    "    \n",
    "    for i in range(len(x_test)):\n",
    "        cc = get_predict_rating_count(x_test[i], feature, user_id_test[i], feature_name)\n",
    "        #print(\"%.2f %.2f\" %(cc, y_test[i]))\n",
    "        #print()\n",
    "        predict_t.append(cc)\n",
    "    \n",
    "\n",
    "    for i in range(len(x_valid)):\n",
    "        cc = get_predict_rating_count(x_valid[i], feature, user_id_valid[i], feature_name )\n",
    "        predict_v.append(cc)\n",
    "        #print(\"%.2f %.2f\" %(cc, y_valid[i]))\n",
    "        #print()\n",
    "        \n",
    "        \n",
    "    #train_rmse = np.sqrt(np.mean(np.square(np.subtract(predict_tr, y_train))))\n",
    "    #valid_rmse = np.sqrt(np.mean(np.square(np.subtract(predict_v, y_valid))))\n",
    "    #test_rmse = np.sqrt(np.mean(np.square(np.subtract(predict_t, y_test))))\n",
    "    \n",
    "    #print(m, train_rmse, valid_rmse, test_rmse)\n",
    "    \n",
    "    #print(m, train_rmse, valid_rmse, test_rmse)\n",
    "    #train_rmse_compare_E.append([m, len(y_train), len(y_valid),len(y_test), train_rmse, valid_rmse, test_rmse])\n",
    "    \n",
    "        \n",
    "    total_predict = np.concatenate((total_predict, predict_t), axis = 0)\n",
    "    total_rating = np.concatenate((total_rating, y_test), axis = 0)\n",
    "    total_predict_v = np.concatenate((total_predict_v, predict_v), axis = 0)\n",
    "    total_rating_v = np.concatenate((total_rating_v, y_valid), axis = 0)\n",
    "    total_predict_tr = np.concatenate((total_predict_tr, predict_tr), axis = 0)\n",
    "    total_rating_tr = np.concatenate((total_rating_tr, y_train), axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(total_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = 3\n",
    "for i in range(len(movie_feature[index])):\n",
    "    print(feature_name_dict[index][i], movie_feature[index][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(total_predict)):\n",
    "    if total_predict[i] < 0:\n",
    "        total_predict[i] = 0.\n",
    "    if total_predict[i] > 5:\n",
    "        total_predict[i] = 5.\n",
    "\n",
    "for i in range(len(total_predict_v)):\n",
    "    if total_predict_v[i] < 0:\n",
    "        total_predict_v[i] = 0.\n",
    "    if total_predict_v[i] > 5:\n",
    "        total_predict_v[i] = 5.\n",
    "        \n",
    "for i in range(len(total_predict_tr)):\n",
    "    if total_predict_tr[i] < 0:\n",
    "        total_predict_tr[i] = 0.\n",
    "    if total_predict_tr[i] > 5:\n",
    "        total_predict_tr[i] = 5.\n",
    "\n",
    "        \n",
    "rmse = np.sqrt(np.mean(np.square(np.subtract(total_predict_tr, total_rating_tr))))  \n",
    "print( rmse)\n",
    "\n",
    "mae = mean_absolute_error(total_rating_tr, total_predict_tr)\n",
    "print(mae)\n",
    "\n",
    "rmse = np.sqrt(np.mean(np.square(np.subtract(total_predict_v, total_rating_v))))  \n",
    "print(rmse)\n",
    "\n",
    "mae = mean_absolute_error(total_rating_v, total_predict_v)\n",
    "print( mae)\n",
    "\n",
    "\n",
    "rmse = np.sqrt(np.mean(np.square(np.subtract(total_predict, total_rating))))  \n",
    "print( rmse)\n",
    "\n",
    "mae = mean_absolute_error(total_rating, total_predict)\n",
    "print(mae)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get one user's information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_id = 1\n",
    "feature_dict = ['d','t','c', 'g']\n",
    "one_user_count_list = {}\n",
    "for type_name in feature_dict:\n",
    "    one_user_count_list[type_name] = {}\n",
    "for i in range(len(feature_dict)):\n",
    "    dict_name = feature_dict[i]\n",
    "    divid_value = divid_list[i]\n",
    "    for feature_key in user_count_list[dict_name][user_id]:\n",
    "        one_user_count_list[dict_name][feature_key] = user_count_list[dict_name][user_id][feature_key]/divid_value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_movie_rating = {}\n",
    "user_movie_info = {}\n",
    "\n",
    "for m in range(0, MOVIE_NUM+1):\n",
    "    if m not in movie_rating:\n",
    "        print('not in', m)\n",
    "        dropped_movie += 1\n",
    "        continue\n",
    "    user_rating_tups = movie_rating[m]\n",
    "    x_train, y_train, feature_name, user_id_train = gen_regr_data(int(m), user_rating_tups)\n",
    "    if len(y_train) < 5:\n",
    "        print('less than 5:', m)\n",
    "        dropped_movie += 1\n",
    "        continue\n",
    "    \n",
    "    one_uesr_ratings = []\n",
    "    one_user_info = []\n",
    "    weight = []\n",
    "    x_pred, _, feature_name, user_id_pred = gen_regr_data(int(m), [(user_id, 0)])\n",
    "    if user_id in user_id_train:\n",
    "        ind = user_id_train.index(user_id)\n",
    "        real_rating = y_train[ind]\n",
    "    else:\n",
    "        real_rating = -1\n",
    "    for j in range(len(feature_name)-1):\n",
    "        prop_sum = 0\n",
    "        name = feature_name[j]\n",
    "        dict_name = name.split('_')[0]\n",
    "        feature_n = name.split('_')[1]\n",
    "        offset = 0.01\n",
    "        try:\n",
    "            if dict_name == 'd':\n",
    "                feature_key = director_2_id[feature_n]\n",
    "            elif dict_name == 'c':\n",
    "                feature_key = cast_2_id[feature_n]\n",
    "            elif dict_name == 'g':\n",
    "                feature_key = genre_2_id[feature_n]\n",
    "            elif dict_name == 't':\n",
    "                feature_key = int(feature_n)\n",
    "        except:\n",
    "            feature_key = ''\n",
    "        try:\n",
    "            weight.append(one_user_count_list[dict_name][feature_key])\n",
    "        except:\n",
    "            # if user have never seen this information\n",
    "            weight.append(0.1)\n",
    "\n",
    "    weight_normal = [w/sum(weight) for w in weight]\n",
    "    print('begin')\n",
    "    try:\n",
    "        one_pred_value = sum([weight_normal[i]*(x_pred[0][i] + movie_feature[m][i]) for i in range(len(x_pred[0]))])\n",
    "    except:\n",
    "        continue\n",
    "    feature_name_dict\n",
    "    \"\"\"\n",
    "    print(\"movie:\", m)\n",
    "    print(\"pred:\", one_pred_value)\n",
    "    print(\"real:\", real_rating)\n",
    "    \n",
    "    for i in range(len(x_pred[0])):\n",
    "        print(feature_name_dict[m][i], x_pred[0][i], movie_feature[m][i], weight_normal[i])\n",
    "    \"\"\"\n",
    "    user_movie_rating[m] = (one_pred_value, real_rating)\n",
    "    user_movie_info[m] = {}\n",
    "    user_movie_info[m]['info_name'] = feature_name_dict[m][0:-1]\n",
    "    user_movie_info[m]['info_rating'] = x_pred[0]\n",
    "    user_movie_info[m]['movie_offset'] = movie_feature[m]\n",
    "    user_movie_info[m]['weight'] = weight_normal\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for m in user_movie_rating:\n",
    "    if user_movie_rating[m][1] > 0:\n",
    "        print(m, user_movie_rating[m][1])\n",
    "        length = len(user_movie_info[m]['info_rating'])\n",
    "        for i in range((length)):\n",
    "            print(user_movie_info[m]['info_name'][i], user_movie_info[m]['info_rating'][i] + user_movie_info[m]['movie_offset'][i], user_movie_info[m]['weight'][i])\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_movie = -1\n",
    "best_rating = 0\n",
    "for m in user_movie_rating:\n",
    "    if user_movie_rating[m][1] < 0 and user_movie_rating[m][0] > best_rating:\n",
    "        best_rating = user_movie_rating[m][0]\n",
    "        best_movie = m\n",
    "\n",
    "best_movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_movie = 1817\n",
    "print('rating:',user_movie_rating[best_movie])\n",
    "length = len(user_movie_info[best_movie]['info_rating'])\n",
    "for i in range((length)):\n",
    "    part = (user_movie_info[best_movie]['info_rating'][i] + user_movie_info[best_movie]['movie_offset'][i]) * user_movie_info[best_movie]['weight'][i]\n",
    "    print(user_movie_info[best_movie]['info_name'][i], user_movie_info[best_movie]['info_rating'][i], user_movie_info[best_movie]['movie_offset'][i], user_movie_info[best_movie]['weight'][i], part)\n",
    "    #print(user_movie_info[best_movie]['info_name'][i])\n",
    "    \n",
    "    #print(part)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ws3.6",
   "language": "python",
   "name": "ws"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
