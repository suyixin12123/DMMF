{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie: 206\n",
      "user: 3550\n",
      "director: 194\n",
      "genre: 20\n",
      "topic: 50\n",
      "cast: 474\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nMOVIE_NUM = 1682\\nDIRECTOR_NUM = 1139\\nGENRE_NUM = 24\\nTOPIC_NUM = 50\\nCAST_NUM = 2894\\nUSER_NUM = 943\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Global values\n",
    "\"\"\"\n",
    "folder = 'boxoffice_10'\n",
    "\n",
    "len_dtcgum = pickle.load(open('../data/'+folder+'/len_dtcgum.pkl', 'rb'))\n",
    "\n",
    "FEATURE_LEN = 5\n",
    "\n",
    "MOVIE_NUM = len_dtcgum[5]\n",
    "DIRECTOR_NUM = len_dtcgum[0]\n",
    "GENRE_NUM = len_dtcgum[3]\n",
    "TOPIC_NUM = 50\n",
    "CAST_NUM = len_dtcgum[2]\n",
    "USER_NUM = len_dtcgum[4]\n",
    "\n",
    "print('movie:', MOVIE_NUM)\n",
    "print('user:', USER_NUM)\n",
    "print('director:', DIRECTOR_NUM)\n",
    "print('genre:', GENRE_NUM)\n",
    "print('topic:', TOPIC_NUM)\n",
    "print('cast:', CAST_NUM)\n",
    "\n",
    "\"\"\"\n",
    "MOVIE_NUM = 1682\n",
    "DIRECTOR_NUM = 1139\n",
    "GENRE_NUM = 24\n",
    "TOPIC_NUM = 50\n",
    "CAST_NUM = 2894\n",
    "USER_NUM = 943\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3550"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USER_NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movie_rating = pickle.load(open('../data/'+folder+'/movie_rating.pkl', 'rb'))\n",
    "movie_rating_test = pickle.load(open('../data/'+folder+'/movie_rating_test.pkl', 'rb'))\n",
    "movie_rating_validation = pickle.load(open('../data/'+folder+'/movie_rating_validation.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(user_name, movie_name, rating_name, last_same):\n",
    "    user = pickle.load(open('../data/'+folder+'/'+user_name+last_same+'.pkl', 'rb'))\n",
    "    movie = pickle.load(open('../data/'+folder+'/'+movie_name+last_same+'.pkl', 'rb'))\n",
    "    rating = pickle.load(open('../data/'+folder+'/'+rating_name+last_same+'.pkl', 'rb'))\n",
    "    return shuffle(user, movie, rating)\n",
    "\n",
    "#load training data\n",
    "d_user, d_movie, d_rating = load_data(\"train_user\", \"train_movie\", \"train_rating\", '_d')\n",
    "t_user, t_movie, t_rating = load_data(\"train_user\", \"train_movie\", \"train_rating\", '_t')\n",
    "g_user, g_movie, g_rating = load_data(\"train_user\", \"train_movie\", \"train_rating\", '_g')   \n",
    "c_user, c_movie, c_rating = load_data(\"train_user\", \"train_movie\", \"train_rating\", '_c')\n",
    "\n",
    "#load validation data\n",
    "d_user_v, d_movie_v, d_rating_v = load_data(\"train_user\", \"train_movie\", \"train_rating\", '_d_v')\n",
    "t_user_v, t_movie_v, t_rating_v = load_data(\"train_user\", \"train_movie\", \"train_rating\", '_t_v')\n",
    "g_user_v, g_movie_v, g_rating_v = load_data(\"train_user\", \"train_movie\", \"train_rating\", '_g_v')        \n",
    "c_user_v, c_movie_v, c_rating_v = load_data(\"train_user\", \"train_movie\", \"train_rating\", '_c_v')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "U = tf.Variable(initial_value=tf.truncated_normal([USER_NUM, FEATURE_LEN]), name='users')\n",
    "D = tf.Variable(initial_value=tf.truncated_normal([FEATURE_LEN, DIRECTOR_NUM]), name='directors')\n",
    "T = tf.Variable(initial_value=tf.truncated_normal([FEATURE_LEN, TOPIC_NUM]), name='topics')\n",
    "G = tf.Variable(initial_value=tf.truncated_normal([FEATURE_LEN, GENRE_NUM]), name='genures')\n",
    "C = tf.Variable(initial_value=tf.truncated_normal([FEATURE_LEN, CAST_NUM]), name='casts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_result = tf.matmul(U, D)\n",
    "t_result = tf.matmul(U, T)\n",
    "g_result = tf.matmul(U, G)\n",
    "c_result = tf.matmul(U, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clip_U = U.assign(tf.maximum(tf.zeros_like(U), U))\n",
    "clip_D = D.assign(tf.maximum(tf.zeros_like(D), D))\n",
    "clip_T = T.assign(tf.maximum(tf.zeros_like(T), T))\n",
    "clip_C = C.assign(tf.maximum(tf.zeros_like(C), C))\n",
    "clip_G = G.assign(tf.maximum(tf.zeros_like(G), G))\n",
    "clip = tf.group(clip_U, clip_D,clip_T,clip_C,clip_G )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MatMul_2:0' shape=(3550, 20) dtype=float32>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_result_flatten = tf.reshape(d_result, [-1])\n",
    "t_result_flatten = tf.reshape(t_result, [-1])\n",
    "g_result_flatten = tf.reshape(g_result, [-1])\n",
    "c_result_flatten = tf.reshape(c_result, [-1])\n",
    "\n",
    "D_R = tf.gather(d_result_flatten, d_user * tf.shape(d_result)[1] + d_movie, name='director_user_rate')\n",
    "T_R = tf.gather(t_result_flatten, t_user * tf.shape(t_result)[1] + t_movie, name='topic_user_rate')\n",
    "G_R = tf.gather(g_result_flatten, g_user * tf.shape(g_result)[1] + g_movie, name='genre_user_rate')\n",
    "C_R = tf.gather(c_result_flatten, c_user * tf.shape(c_result)[1] + c_movie, name='cast_user_rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_diff_op = tf.subtract(D_R, d_rating, name='d_trainig_diff')\n",
    "t_diff_op = tf.subtract(T_R, t_rating, name='t_trainig_diff')\n",
    "g_diff_op = tf.subtract(G_R, g_rating, name='g_trainig_diff')\n",
    "c_diff_op = tf.subtract(C_R, c_rating, name='c_trainig_diff')\n",
    "\n",
    "d_diff_op_squared = tf.abs(d_diff_op, name=\"d_squared_difference\")\n",
    "t_diff_op_squared = tf.abs(t_diff_op, name=\"t_squared_difference\")\n",
    "g_diff_op_squared = tf.abs(g_diff_op, name=\"g_squared_difference\")\n",
    "c_diff_op_squared = tf.abs(c_diff_op, name=\"c_squared_difference\")\n",
    "\n",
    "#\"\"\"\n",
    "d_base_cost = tf.reduce_sum(d_diff_op_squared, name=\"d_sum_squared_error\")/ tf.cast(tf.shape(d_diff_op_squared)[0], tf.float32) * 10000 \n",
    "t_base_cost = tf.reduce_sum(t_diff_op_squared, name=\"t_sum_squared_error\")/ tf.cast(tf.shape(t_diff_op_squared)[0], tf.float32) * 10000\n",
    "g_base_cost = tf.reduce_sum(g_diff_op_squared, name=\"g_sum_squared_error\")/ tf.cast(tf.shape(g_diff_op_squared)[0], tf.float32) * 10000\n",
    "c_base_cost = tf.reduce_sum(c_diff_op_squared, name=\"c_sum_squared_error\")/ tf.cast(tf.shape(c_diff_op_squared)[0], tf.float32) * 10000\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "d_base_cost = tf.reduce_sum(d_diff_op_squared, name=\"d_sum_squared_error\")/30.\n",
    "t_base_cost = tf.reduce_sum(t_diff_op_squared, name=\"t_sum_squared_error\")/30.\n",
    "g_base_cost = tf.reduce_sum(g_diff_op_squared, name=\"g_sum_squared_error\")/30.\n",
    "c_base_cost = tf.reduce_sum(c_diff_op_squared, name=\"c_sum_squared_error\")/30.\n",
    "\"\"\"\n",
    "\n",
    "base_cost = d_base_cost + t_base_cost + g_base_cost + c_base_cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = tf.constant(.001, name='lambda')\n",
    "\n",
    "\"\"\"\n",
    "u_norm = tf.reduce_sum(tf.abs(U, name='user_abs'), name='user_norm') / tf.cast(tf.shape(U)[0], tf.float32)\n",
    "d_norm = tf.reduce_sum(tf.abs(D, name='director_abs'), name='director_norm')/ tf.cast(tf.shape(D)[1], tf.float32)\n",
    "t_norm = tf.reduce_sum(tf.abs(T, name='topic_abs'), name='topic_norm')/ tf.cast(tf.shape(T)[1], tf.float32)\n",
    "g_norm = tf.reduce_sum(tf.abs(G, name='genre_abs'), name='genre_norm')/ tf.cast(tf.shape(G)[1], tf.float32)\n",
    "c_norm = tf.reduce_sum(tf.abs(C, name='cast_abs'), name='cast_norm')/ tf.cast(tf.shape(C)[1], tf.float32)\n",
    "\"\"\"\n",
    "u_norm = tf.reduce_sum(tf.abs(U, name='user_abs'), name='user_norm') \n",
    "d_norm = tf.reduce_sum(tf.abs(D, name='director_abs'), name='director_norm')\n",
    "t_norm = tf.reduce_sum(tf.abs(T, name='topic_abs'), name='topic_norm')\n",
    "g_norm = tf.reduce_sum(tf.abs(G, name='genre_abs'), name='genre_norm')\n",
    "c_norm = tf.reduce_sum(tf.abs(C, name='cast_abs'), name='cast_norm')\n",
    "\n",
    "norm_sums = u_norm + d_norm + t_norm + g_norm + c_norm\n",
    "regularizer = tf.multiply(norm_sums, lda, 'regularizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cost = tf.add(base_cost, regularizer)\n",
    "cost = base_cost + regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = tf.constant(.001, name='learning_rate')\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = tf.train.exponential_decay(lr, global_step, 10000, 0.96, staircase=True)\n",
    "#learning_rate = 0.01\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "training_step = optimizer.minimize(cost, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate Training RMSE\n",
    "\"\"\"\n",
    "g_user_test = g_user\n",
    "g_item_test = g_movie\n",
    "g_rate_test = g_rating\n",
    "g_result_flatten = g_result_flatten\n",
    "#genre rmse\n",
    "g_test = tf.gather(g_result_flatten, g_user_test * tf.shape(g_result)[1] + g_item_test, name='g_test')\n",
    "g_rmse = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(g_rate_test, g_test))))\n",
    "\n",
    "d_user_test = d_user\n",
    "d_item_test = d_movie\n",
    "d_rate_test = d_rating\n",
    "d_result_flatten = d_result_flatten\n",
    "#genre rmse\n",
    "d_test = tf.gather(d_result_flatten, d_user_test * tf.shape(d_result)[1] + d_item_test, name='d_test')\n",
    "d_rmse = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(d_rate_test, d_test))))\n",
    "\n",
    "t_user_test = t_user\n",
    "t_item_test = t_movie\n",
    "t_rate_test = t_rating\n",
    "t_result_flatten = t_result_flatten\n",
    "#genre rmse\n",
    "t_test = tf.gather(t_result_flatten, t_user_test * tf.shape(t_result)[1] + t_item_test, name='t_test')\n",
    "t_rmse = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(t_rate_test, t_test))))\n",
    "\n",
    "c_user_test = c_user\n",
    "c_item_test = c_movie\n",
    "c_rate_test = c_rating\n",
    "c_result_flatten = c_result_flatten\n",
    "#genre rmse\n",
    "c_test = tf.gather(c_result_flatten, c_user_test * tf.shape(c_result)[1] + c_item_test, name='c_test')\n",
    "c_rmse = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(c_rate_test, c_test))))\n",
    "\n",
    "\n",
    "rmse = tf.divide((g_rmse + d_rmse + t_rmse + c_rmse), 4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate Validation RMSE\n",
    "\"\"\"\n",
    "g_user_v = g_user_v\n",
    "g_item_v = g_movie_v\n",
    "g_rate_v = g_rating_v\n",
    "g_result_flatten = g_result_flatten\n",
    "#genre rmse\n",
    "g_v = tf.gather(g_result_flatten, g_user_v * tf.shape(g_result)[1] + g_item_v, name='g_v')\n",
    "g_rmse_v = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(g_rate_v, g_v))))\n",
    "\n",
    "d_user_v = d_user_v\n",
    "d_item_v = d_movie_v\n",
    "d_rate_v = d_rating_v\n",
    "d_result_flatten = d_result_flatten\n",
    "#genre rmse\n",
    "d_v = tf.gather(d_result_flatten, d_user_v * tf.shape(d_result)[1] + d_item_v, name='d_v')\n",
    "d_rmse_v = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(d_rate_v, d_v))))\n",
    "\n",
    "t_user_v = t_user_v\n",
    "t_item_v = t_movie_v\n",
    "t_rate_v = t_rating_v\n",
    "t_result_flatten = t_result_flatten\n",
    "#genre rmse\n",
    "t_v = tf.gather(t_result_flatten, t_user_v * tf.shape(t_result)[1] + t_item_v, name='t_v')\n",
    "t_rmse_v = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(t_rate_v, t_v))))\n",
    "\n",
    "c_user_v = c_user_v\n",
    "c_item_v = c_movie_v\n",
    "c_rate_v = c_rating_v\n",
    "c_result_flatten = c_result_flatten\n",
    "#genre rmse\n",
    "c_v = tf.gather(c_result_flatten, c_user_v * tf.shape(c_result)[1] + c_item_v, name='c_v')\n",
    "c_rmse_v = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(c_rate_v, c_v))))\n",
    "\n",
    "\n",
    "rmse_v = tf.divide((g_rmse_v + d_rmse_v + t_rmse_v + c_rmse_v), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(g_user_v)):\n",
    "    if g_user_v[i] < 0:\n",
    "        print(i, g_item_v[i], g_user_v[i])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 143470.0 current rmsr:  4.01684 lowest rmse:  4.01684 lowest num:  0\n",
      "4.02873 3.9755 4.05321 4.00992\n",
      "\n",
      "Traing RMSR(total, g, d, t, c): 0 3.12561 3.19357 3.15693 3.07146 3.0805\n",
      "\n",
      "20 56726.5 current rmsr:  1.68831 lowest rmse:  1.68831 lowest num:  20\n",
      "1.3654 1.76819 1.41574 2.20393\n",
      "40 31340.5 current rmsr:  1.09548 lowest rmse:  1.09548 lowest num:  40\n",
      "0.904285 1.09524 0.942372 1.44004\n",
      "60 24985.0 current rmsr:  0.930218 lowest rmse:  0.930218 lowest num:  60\n",
      "0.868904 0.904794 0.836008 1.11116\n",
      "80 23094.2 current rmsr:  0.875345 lowest rmse:  0.875345 lowest num:  80\n",
      "0.866982 0.842984 0.81151 0.979904\n",
      "100 22326.4 current rmsr:  0.852054 lowest rmse:  0.852054 lowest num:  100\n",
      "0.8689 0.817075 0.805561 0.916678\n",
      "120 21897.2 current rmsr:  0.839378 lowest rmse:  0.839378 lowest num:  120\n",
      "0.870093 0.802851 0.804033 0.880535\n",
      "140 21619.1 current rmsr:  0.83131 lowest rmse:  0.83131 lowest num:  140\n",
      "0.870133 0.79357 0.803698 0.857839\n",
      "160 21386.0 current rmsr:  0.825472 lowest rmse:  0.825472 lowest num:  160\n",
      "0.86845 0.787165 0.803626 0.842649\n",
      "180 21243.7 current rmsr:  0.821382 lowest rmse:  0.821382 lowest num:  180\n",
      "0.867496 0.782653 0.803605 0.831776\n",
      "200 21105.8 current rmsr:  0.817911 lowest rmse:  0.817911 lowest num:  200\n",
      "0.864658 0.779542 0.803817 0.823627\n",
      "\n",
      "Traing RMSR(total, g, d, t, c): 200 0.716624 0.652848 0.728563 0.710047 0.775038\n",
      "\n",
      "220 21000.4 current rmsr:  0.815164 lowest rmse:  0.815164 lowest num:  220\n",
      "0.862136 0.777333 0.803956 0.817231\n",
      "240 20925.3 current rmsr:  0.813056 lowest rmse:  0.813056 lowest num:  240\n",
      "0.860289 0.775815 0.804069 0.812051\n",
      "260 20843.7 current rmsr:  0.81125 lowest rmse:  0.81125 lowest num:  260\n",
      "0.858496 0.774611 0.804184 0.807708\n",
      "280 20768.0 current rmsr:  0.80941 lowest rmse:  0.80941 lowest num:  280\n",
      "0.855686 0.77372 0.804148 0.804086\n",
      "300 20681.8 current rmsr:  0.807568 lowest rmse:  0.807568 lowest num:  300\n",
      "0.852187 0.773044 0.803998 0.801042\n",
      "320 20633.5 current rmsr:  0.806202 lowest rmse:  0.806202 lowest num:  320\n",
      "0.849832 0.772467 0.804161 0.798347\n",
      "340 20598.4 current rmsr:  0.805099 lowest rmse:  0.805099 lowest num:  340\n",
      "0.848159 0.772014 0.804197 0.796025\n",
      "360 20524.0 current rmsr:  0.803819 lowest rmse:  0.803819 lowest num:  360\n",
      "0.845636 0.771647 0.803945 0.794047\n",
      "380 20461.4 current rmsr:  0.802599 lowest rmse:  0.802599 lowest num:  380\n",
      "0.84287 0.771312 0.803933 0.79228\n",
      "400 20412.0 current rmsr:  0.801579 lowest rmse:  0.801579 lowest num:  400\n",
      "0.84037 0.771066 0.804098 0.790781\n",
      "\n",
      "Traing RMSR(total, g, d, t, c): 400 0.697557 0.626394 0.718381 0.709822 0.73563\n",
      "\n",
      "420 20366.9 current rmsr:  0.80086 lowest rmse:  0.801579 lowest num:  400\n",
      "0.839031 0.770837 0.804089 0.789481\n",
      "440 20308.5 current rmsr:  0.800148 lowest rmse:  0.800148 lowest num:  440\n",
      "0.837553 0.770625 0.804118 0.788293\n",
      "460 20237.5 current rmsr:  0.799276 lowest rmse:  0.800148 lowest num:  440\n",
      "0.835213 0.770468 0.804098 0.787325\n",
      "480 20211.9 current rmsr:  0.798636 lowest rmse:  0.798636 lowest num:  480\n",
      "0.833781 0.77029 0.804074 0.786399\n",
      "500 20196.2 current rmsr:  0.798005 lowest rmse:  0.798636 lowest num:  480\n",
      "0.832164 0.770201 0.804027 0.78563\n",
      "520 20122.0 current rmsr:  0.797103 lowest rmse:  0.797103 lowest num:  520\n",
      "0.829165 0.770142 0.8041 0.785005\n",
      "540 20081.1 current rmsr:  0.796443 lowest rmse:  0.797103 lowest num:  520\n",
      "0.827248 0.770037 0.804084 0.784403\n",
      "560 20045.3 current rmsr:  0.796078 lowest rmse:  0.796078 lowest num:  560\n",
      "0.826131 0.770006 0.804228 0.783947\n",
      "580 20030.3 current rmsr:  0.795732 lowest rmse:  0.796078 lowest num:  560\n",
      "0.825279 0.769937 0.80423 0.783483\n",
      "600 19986.5 current rmsr:  0.79511 lowest rmse:  0.796078 lowest num:  560\n",
      "0.82319 0.769893 0.804251 0.783107\n",
      "\n",
      "Traing RMSR(total, g, d, t, c): 600 0.688334 0.606574 0.714816 0.708469 0.723477\n",
      "\n",
      "620 19957.6 current rmsr:  0.794581 lowest rmse:  0.794581 lowest num:  620\n",
      "0.821493 0.769839 0.804261 0.782731\n",
      "640 19922.2 current rmsr:  0.794233 lowest rmse:  0.794581 lowest num:  620\n",
      "0.820435 0.769785 0.8043 0.782413\n",
      "660 19879.2 current rmsr:  0.793845 lowest rmse:  0.794581 lowest num:  620\n",
      "0.819229 0.769762 0.80425 0.782138\n",
      "680 19836.9 current rmsr:  0.793426 lowest rmse:  0.793426 lowest num:  680\n",
      "0.817638 0.76979 0.804376 0.781898\n",
      "700 19795.0 current rmsr:  0.792958 lowest rmse:  0.793426 lowest num:  680\n",
      "0.816057 0.769715 0.804422 0.781639\n",
      "720 19731.4 current rmsr:  0.79229 lowest rmse:  0.79229 lowest num:  720\n",
      "0.813589 0.769721 0.804398 0.781451\n",
      "740 19703.7 current rmsr:  0.792057 lowest rmse:  0.79229 lowest num:  720\n",
      "0.812606 0.76977 0.804524 0.781329\n",
      "760 19669.2 current rmsr:  0.791544 lowest rmse:  0.79229 lowest num:  720\n",
      "0.810764 0.769749 0.804532 0.781132\n",
      "780 19634.8 current rmsr:  0.791146 lowest rmse:  0.791146 lowest num:  780\n",
      "0.809183 0.769813 0.804581 0.781005\n",
      "800 19634.1 current rmsr:  0.791369 lowest rmse:  0.791146 lowest num:  780\n",
      "0.809825 0.769925 0.804787 0.780939\n",
      "\n",
      "Traing RMSR(total, g, d, t, c): 800 0.681231 0.590489 0.710994 0.706629 0.716812\n",
      "\n",
      "820 19586.3 current rmsr:  0.790965 lowest rmse:  0.791146 lowest num:  780\n",
      "0.808057 0.770032 0.804908 0.780863\n",
      "840 19563.8 current rmsr:  0.79074 lowest rmse:  0.791146 lowest num:  780\n",
      "0.807252 0.770044 0.804943 0.780721\n",
      "860 19513.2 current rmsr:  0.79032 lowest rmse:  0.791146 lowest num:  780\n",
      "0.805725 0.770059 0.804886 0.780611\n",
      "880 19495.5 current rmsr:  0.790114 lowest rmse:  0.790114 lowest num:  880\n",
      "0.804907 0.770088 0.804943 0.780518\n",
      "900 19437.5 current rmsr:  0.789691 lowest rmse:  0.790114 lowest num:  880\n",
      "0.803047 0.770173 0.805055 0.780488\n",
      "920 19429.1 current rmsr:  0.78954 lowest rmse:  0.790114 lowest num:  880\n",
      "0.802431 0.770162 0.805151 0.780418\n",
      "940 19387.5 current rmsr:  0.789226 lowest rmse:  0.790114 lowest num:  880\n",
      "0.801333 0.770133 0.805144 0.780295\n",
      "960 19340.7 current rmsr:  0.788677 lowest rmse:  0.788677 lowest num:  960\n",
      "0.798959 0.770214 0.805264 0.780272\n",
      "980 19340.1 current rmsr:  0.78895 lowest rmse:  0.788677 lowest num:  960\n",
      "0.799791 0.770285 0.805462 0.780261\n",
      "1000 19296.3 current rmsr:  0.788508 lowest rmse:  0.788677 lowest num:  960\n",
      "0.797942 0.770367 0.805479 0.780243\n",
      "\n",
      "Traing RMSR(total, g, d, t, c): 1000 0.673952 0.574639 0.706089 0.704031 0.711048\n",
      "\n",
      "1020 19288.6 current rmsr:  0.788659 lowest rmse:  0.788677 lowest num:  960\n",
      "0.798405 0.77041 0.805612 0.780208\n",
      "1040 19247.1 current rmsr:  0.788074 lowest rmse:  0.788677 lowest num:  960\n",
      "0.796087 0.770412 0.805636 0.780159\n",
      "1060 19205.5 current rmsr:  0.788149 lowest rmse:  0.788677 lowest num:  960\n",
      "0.796274 0.770498 0.805682 0.78014\n",
      "1080 19165.2 current rmsr:  0.787699 lowest rmse:  0.788677 lowest num:  960\n",
      "0.794385 0.770549 0.805751 0.78011\n",
      "1100 19114.0 current rmsr:  0.787398 lowest rmse:  0.787398 lowest num:  1100\n",
      "0.79287 0.770695 0.80589 0.780137\n",
      "1120 19088.4 current rmsr:  0.787523 lowest rmse:  0.787398 lowest num:  1100\n",
      "0.793217 0.770767 0.805989 0.780117\n",
      "1140 19066.9 current rmsr:  0.787427 lowest rmse:  0.787398 lowest num:  1100\n",
      "0.792502 0.770888 0.806176 0.780141\n",
      "1160 19043.2 current rmsr:  0.787283 lowest rmse:  0.787398 lowest num:  1100\n",
      "0.791657 0.771008 0.80634 0.780127\n",
      "1180 19002.9 current rmsr:  0.786935 lowest rmse:  0.787398 lowest num:  1100\n",
      "0.79018 0.771086 0.806386 0.780088\n",
      "1200 18970.8 current rmsr:  0.78695 lowest rmse:  0.787398 lowest num:  1100\n",
      "0.789886 0.771244 0.806547 0.780122\n",
      "\n",
      "Traing RMSR(total, g, d, t, c): 1200 0.667329 0.563811 0.700041 0.700557 0.704908\n",
      "\n",
      "1220 18942.7 current rmsr:  0.786841 lowest rmse:  0.787398 lowest num:  1100\n",
      "0.789261 0.771319 0.806674 0.780109\n",
      "1240 18926.1 current rmsr:  0.786977 lowest rmse:  0.787398 lowest num:  1100\n",
      "0.789422 0.771404 0.80694 0.780142\n",
      "1260 18874.8 current rmsr:  0.786655 lowest rmse:  0.787398 lowest num:  1100\n",
      "0.78793 0.771462 0.80706 0.780169\n",
      "1280 18873.4 current rmsr:  0.787012 lowest rmse:  0.787398 lowest num:  1100\n",
      "0.788925 0.771595 0.807299 0.780229\n",
      "1300 18820.1 current rmsr:  0.786618 lowest rmse:  0.787398 lowest num:  1100\n",
      "0.787142 0.771633 0.807432 0.780263\n"
     ]
    }
   ],
   "source": [
    "#training_trend_f = open('../data/'+folder+'/trends.csv', 'w')\n",
    "#csv_trend = csv.writer(training_trend_f)\n",
    "#headers = ['round','train_total', 'train_g', 'train_d', 'train_t', 'train_c', 'valid_total', 'valid_g', 'valid_d', 'valid_t', 'valid_c']\n",
    "#csv_trend.writerow(headers)\n",
    "lowest_num = 0\n",
    "lowest_rmse = 9999999\n",
    "not_dec_num = 0\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "for i in range(100001):\n",
    "    _, c, r = sess.run([training_step, cost, regularizer])\n",
    "    if i%20 == 0:\n",
    "        _rmse_v, _g_rmse_v, _d_rmse_v, _t_rmse_v, _c_rmse_v  = sess.run([rmse_v, g_rmse_v, d_rmse_v, t_rmse_v, c_rmse_v])\n",
    "        sess.run(clip)\n",
    "        if lowest_rmse > _rmse_v + 0.001:\n",
    "            lowest_rmse = _rmse_v\n",
    "            lowest_num = i\n",
    "            not_dec_num = 0\n",
    "        else:\n",
    "            not_dec_num += 1\n",
    "            if not_dec_num > 10:\n",
    "                break\n",
    "            \n",
    "        print(i, c, \"current rmsr: \", _rmse_v,  \"lowest rmse: \", lowest_rmse, \"lowest num: \", lowest_num)\n",
    "        print(_g_rmse_v, _d_rmse_v, _t_rmse_v, _c_rmse_v)\n",
    "        _rmse, _g_rmse, _d_rmse, _t_rmse, _c_rmse = sess.run([rmse, g_rmse, d_rmse, t_rmse, c_rmse])\n",
    "        #w_row = [i, _rmse, _g_rmse, _d_rmse, _t_rmse, _c_rmse, _rmse_v, _g_rmse_v, _d_rmse_v, _t_rmse_v, _c_rmse_v]\n",
    "        #csv_trend.writerow(w_row)\n",
    "    if i%200 == 0:\n",
    "        print()\n",
    "        print(\"Traing RMSR(total, g, d, t, c):\", i , _rmse, _g_rmse, _d_rmse, _t_rmse, _c_rmse)\n",
    "        print()\n",
    "#training_trend_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movie_feature_dict = pickle.load(open('../data/'+folder+'/movie_feature_dict.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_matrix(row_num, col_num, user, movie, rating):\n",
    "    # row is movie infor num\n",
    "    # col is user num\n",
    "    matr = [[0 for i in range(col_num)] for j in range(row_num)]\n",
    "    for i in range(len(user)):\n",
    "        matr[movie[i]][user[i]] = float(rating[i])\n",
    "    \n",
    "    return matr\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_v, director_v, topic_v, genre_v, cast_v = sess.run([U,D,T,G,C])\n",
    "\n",
    "d_mat = generate_matrix(DIRECTOR_NUM, USER_NUM, d_user, d_movie, d_rating)\n",
    "t_mat = generate_matrix(TOPIC_NUM, USER_NUM, t_user, t_movie, t_rating)\n",
    "c_mat = generate_matrix(CAST_NUM, USER_NUM, c_user, c_movie, c_rating)\n",
    "g_mat = generate_matrix(GENRE_NUM, USER_NUM, g_user, g_movie, g_rating)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id_2_genre = pickle.load(open('../data/'+folder+'/id_2_genre.pkl', 'rb'))\n",
    "id_2_director = pickle.load(open('../data/'+folder+'/id_2_director.pkl', 'rb'))\n",
    "id_2_cast = pickle.load(open('../data/'+folder+'/id_2_cast.pkl', 'rb'))\n",
    "genre_2_id = pickle.load(open('../data/'+folder+'/genre_2_id.pkl', 'rb'))\n",
    "director_2_id = pickle.load(open('../data/'+folder+'/director_2_id.pkl', 'rb'))\n",
    "cast_2_id = pickle.load(open('../data/'+folder+'/cast_2_id.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_rating_list(info_id_list, info_matrix, user_vector, user_id, matrix):\n",
    "    result_list = []\n",
    "    for _id in info_id_list:\n",
    "        if matrix[_id][user_id] > 0:\n",
    "            result_list.append(matrix[_id][user_id]) \n",
    "        else:\n",
    "            \n",
    "            _v = info_matrix[:, int(_id)]\n",
    "            _rating = np.dot(user_vector, _v)\n",
    "            result_list.append(_rating)\n",
    "    return result_list\n",
    "\n",
    "def get_rating_list_train(id_tup_list, matrix):\n",
    "    result_list = []\n",
    "    for tup in id_tup_list:\n",
    "        user_id = tup[0]\n",
    "        info_id = tup[1]\n",
    "        result_list.append(matrix[info_id][user_id])\n",
    "\n",
    "def gen_regr_data(m_id, user_rating_tups, is_training = True):\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    user_id_train = []\n",
    "    for tup in user_rating_tups:\n",
    "        one_user_x = []\n",
    "        rating = float(tup[1])\n",
    "        y_train.append(rating)\n",
    "        \n",
    "        user_id = int(tup[0])\n",
    "        user_id_train.append(user_id)\n",
    "        feature_dict = movie_feature_dict[m_id]\n",
    "        u_v = user_v[user_id]\n",
    "        \n",
    "        d_x = get_rating_list(feature_dict['d'], director_v, u_v,user_id, d_mat)\n",
    "        g_x = get_rating_list(feature_dict['g'], genre_v, u_v, user_id, g_mat)\n",
    "        t_x = get_rating_list(feature_dict['t'], topic_v, u_v, user_id, t_mat)\n",
    "        c_x = get_rating_list(feature_dict['c'], cast_v, u_v, user_id, c_mat)\n",
    "        #print(user_id, d_x, g_x, t_x, c_x)\n",
    "        \n",
    "        \n",
    "        one_user_x = d_x + g_x + t_x + c_x\n",
    "        #one_user_x = d_x + g_x + c_x\n",
    "        x_train.append(one_user_x)\n",
    "        \n",
    "        feature_name = []\n",
    "        feature_name += [\"d_\" + id_2_director[_id] for _id in feature_dict['d']]\n",
    "        feature_name += [\"g_\" + id_2_genre[_id] for _id in feature_dict['g']]\n",
    "        feature_name += [\"t_\" + str(_id) for _id in feature_dict['t']]\n",
    "        feature_name += [\"c_\" + id_2_cast[_id] for _id in feature_dict['c']]\n",
    "        feature_name += [\"bias\"]\n",
    "        \n",
    "        \n",
    "    return x_train, y_train, feature_name, user_id_train\n",
    "        \n",
    "            \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_count_list = pickle.load(open('../data/'+folder+'/user_count_list.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name_list = ['d', 't', 'c', 'g']\n",
    "divid_list = []\n",
    "for name in name_list:\n",
    "    summ = 0\n",
    "    total = 0\n",
    "    for user in user_count_list[name]:\n",
    "        for key in user_count_list[name][user]:\n",
    "            summ += user_count_list[name][user][key]\n",
    "            total += 1\n",
    "\n",
    "    divid_list.append(summ/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store result matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_d_result, _t_result, _g_result, _c_result =  sess.run([d_result, t_result, g_result, c_result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(_d_result, open('../data/'+folder+'/ranking_matrix_director.pkl', 'wb'))\n",
    "pickle.dump(_t_result, open('../data/'+folder+'/ranking_matrix_type.pkl', 'wb'))\n",
    "pickle.dump(_g_result, open('../data/'+folder+'/ranking_matrix_genre.pkl', 'wb'))\n",
    "pickle.dump(_c_result, open('../data/'+folder+'/ranking_matrix_cast.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "import numpy as np\n",
    "\n",
    "def select_feature_index(feature, n):\n",
    "    index_list = [0 for i in range(len(feature))]\n",
    "    temp_f = [abs(feature[i]) for i in range(len(feature))]\n",
    "    for i in (range(n)):\n",
    "        min_index = temp_f.index(max(temp_f))\n",
    "        index_list[min_index] = 1\n",
    "        temp_f[min_index] = 0\n",
    "    \n",
    "    return index_list\n",
    "\n",
    "def get_predict_rating_old(user_rating, features):\n",
    "    return np.mean([user_rating[i] + features[i] for i in range(len(user_rating))])\n",
    "\n",
    "\n",
    "def get_predict_rating(user_rating, features):\n",
    "    weight_n = [float(abs(ur)) for ur in user_rating]\n",
    "    summ = sum(weight_n)\n",
    "    weight = [ur/summ for ur in weight_n]\n",
    "    #weight = [1./len(weight_n) for ur in weight_n]\n",
    "    \n",
    "    weight_total = sum([weight[i]* (user_rating[i] + features[i]) for i in range(len(weight))])\n",
    "    \n",
    "    return weight_total\n",
    "\n",
    "def get_predict_rating_select(user_rating, features, n):\n",
    "    if len(user_rating) < n:\n",
    "        return get_predict_rating_old(user_rating, features)\n",
    "    else:\n",
    "        index_list = select_feature_index(user_rating, n)\n",
    "        account_list = []\n",
    "        for i in range(len(user_rating)):\n",
    "            if index_list[i] == 1:\n",
    "                account_list.append(user_rating[i] + features[i])\n",
    "        return np.mean(account_list)\n",
    "\n",
    "def get_predict_rating_count(user_rating, features, user_id, feature_name):\n",
    "    type_prop = {}\n",
    "    weight = []\n",
    "    weight_sum = 0\n",
    "    offset = 0.01\n",
    "    for j in range(len(feature_name)-1):\n",
    "        name = feature_name[j]\n",
    "        dict_name = name.split('_')[0]\n",
    "        feature_n = name.split('_')[1]\n",
    "        try:\n",
    "            if dict_name == 'd':\n",
    "                feature_key = director_2_id[feature_n]\n",
    "                offset = 1/divid_list[0]\n",
    "            elif dict_name == 'c':\n",
    "                feature_key = cast_2_id[feature_n]\n",
    "                offset = 1/divid_list[2]\n",
    "            elif dict_name == 'g':\n",
    "                feature_key = genre_2_id[feature_n]\n",
    "                offset = 1/divid_list[3]\n",
    "            elif dict_name == 't':\n",
    "                feature_key = int(feature_n)\n",
    "                offset = 1/divid_list[1]\n",
    "        except:\n",
    "            feature_key = ''\n",
    "        try:\n",
    "            prop = user_count_list[dict_name][user_id][feature_key]\n",
    "        except:\n",
    "            prop = 1\n",
    "        prop = offset * prop\n",
    "        weight_sum += prop\n",
    "        weight.append(prop)      \n",
    "    \n",
    "    weight = [w/float(weight_sum) for w in weight]\n",
    "    result = 0\n",
    "    ppr_list = []\n",
    "    for i in range(len(user_rating)):\n",
    "        ppr = (user_rating[i] + features[i]) * weight[i]\n",
    "        ppr_list.append(ppr)\n",
    "        result += ppr\n",
    "        # add each kind information proportion to calculate average proportion of each information in movie \n",
    "    \n",
    "    for i in range(len(ppr_list)):\n",
    "        name = feature_name[i]\n",
    "        dict_name = name.split('_')[0]\n",
    "        if dict_name not in type_prop:\n",
    "            type_prop[dict_name] = [ppr_list[i]/result]\n",
    "        else:\n",
    "            type_prop[dict_name] = type_prop[dict_name] + [ppr_list[i]/result]\n",
    "    \"\"\"\n",
    "    print(user_id)\n",
    "    for i in range(len(weight)):\n",
    "        print(feature_name[i], end='')\n",
    "        print(\" %.2f %.2f %.2f\" % (user_rating[i], features[i], weight[i]))\n",
    "    \n",
    "    \"\"\"\n",
    "    type_mean = {}\n",
    "    for key in type_prop:\n",
    "        type_mean[key] = np.sum(type_prop[key])\n",
    "    \n",
    "    return result, type_mean\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equal weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1427 175 172\n",
      "759 107 88\n",
      "1106 148 163\n",
      "543 72 80\n",
      "208 28 26\n",
      "692 71 84\n",
      "519 57 61\n",
      "1002 132 151\n",
      "371 42 51\n",
      "1119 151 165\n",
      "200 24 24\n",
      "1168 134 132\n",
      "352 45 47\n",
      "378 53 59\n",
      "1000 131 89\n",
      "530 51 52\n",
      "1366 157 160\n",
      "873 104 104\n",
      "273 38 33\n",
      "539 86 58\n",
      "383 51 41\n",
      "1104 125 136\n",
      "868 122 115\n",
      "516 53 63\n",
      "787 89 99\n",
      "313 39 31\n",
      "639 72 85\n",
      "346 36 54\n",
      "480 44 69\n",
      "193 20 25\n",
      "50 7 5\n",
      "109 14 20\n",
      "238 33 25\n",
      "13 3 0\n",
      "176 23 23\n",
      "657 70 88\n",
      "412 55 50\n",
      "53 7 3\n",
      "289 36 42\n",
      "335 45 40\n",
      "1258 142 165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Suyixin/anaconda/envs/ws/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2889: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/Suyixin/anaconda/envs/ws/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135 17 16\n",
      "789 104 104\n",
      "1149 139 154\n",
      "1330 173 178\n",
      "912 102 125\n",
      "493 55 49\n",
      "97 15 12\n",
      "742 84 84\n",
      "1136 138 156\n",
      "43 6 13\n",
      "642 69 84\n",
      "164 13 17\n",
      "1242 155 186\n",
      "1045 123 114\n",
      "810 92 103\n",
      "less than 5: 56\n",
      "265 36 32\n",
      "235 35 30\n",
      "1560 207 202\n",
      "410 54 52\n",
      "46 6 7\n",
      "322 37 44\n",
      "31 7 3\n",
      "32 4 7\n",
      "250 42 28\n",
      "1177 165 150\n",
      "61 7 12\n",
      "300 37 45\n",
      "less than 5: 69\n",
      "254 33 32\n",
      "217 29 38\n",
      "374 54 51\n",
      "16 4 1\n",
      "447 53 54\n",
      "731 87 81\n",
      "743 95 90\n",
      "664 92 79\n",
      "66 7 14\n",
      "71 5 12\n",
      "335 50 41\n",
      "113 13 14\n",
      "231 26 35\n",
      "70 3 9\n",
      "9 0 1\n",
      "1009 110 118\n",
      "38 3 2\n",
      "37 4 7\n",
      "648 70 72\n",
      "748 108 84\n",
      "90 11 12\n",
      "58 8 9\n",
      "666 83 84\n",
      "186 37 28\n",
      "71 8 15\n",
      "266 43 33\n",
      "445 36 48\n",
      "1070 133 109\n",
      "440 54 56\n",
      "240 24 29\n",
      "271 29 29\n",
      "535 77 64\n",
      "58 7 11\n",
      "402 46 54\n",
      "96 14 10\n",
      "244 32 27\n",
      "94 10 11\n",
      "275 27 39\n",
      "less than 5: 108\n",
      "117 20 13\n",
      "133 16 12\n",
      "212 30 21\n",
      "103 9 10\n",
      "387 43 43\n",
      "165 19 21\n",
      "89 13 19\n",
      "195 23 25\n",
      "131 14 9\n",
      "367 61 44\n",
      "488 58 62\n",
      "105 14 17\n",
      "663 88 73\n",
      "895 105 114\n",
      "55 4 4\n",
      "50 9 8\n",
      "216 30 30\n",
      "728 106 77\n",
      "571 73 68\n",
      "1064 130 146\n",
      "93 13 13\n",
      "632 91 95\n",
      "244 27 32\n",
      "60 7 9\n",
      "86 17 11\n",
      "331 35 35\n",
      "44 6 2\n",
      "1224 168 173\n",
      "219 40 26\n",
      "26 5 3\n",
      "138 16 9\n",
      "239 25 29\n",
      "111 17 16\n",
      "212 18 32\n",
      "40 8 4\n",
      "20 3 0\n",
      "258 32 38\n",
      "102 8 9\n",
      "10 0 3\n",
      "1006 127 115\n",
      "156 27 18\n",
      "391 55 45\n",
      "45 11 3\n",
      "102 16 12\n",
      "238 23 32\n",
      "130 22 12\n",
      "343 47 34\n",
      "1509 182 188\n",
      "377 42 39\n",
      "948 117 117\n",
      "102 13 11\n",
      "621 74 69\n",
      "328 40 33\n",
      "65 5 5\n",
      "107 20 10\n",
      "18 4 0\n",
      "577 88 83\n",
      "140 18 14\n",
      "168 19 23\n",
      "537 75 71\n",
      "905 122 142\n",
      "400 45 46\n",
      "less than 5: 171\n",
      "87 12 9\n",
      "457 55 57\n",
      "479 65 63\n",
      "773 110 105\n",
      "230 39 35\n",
      "58 8 2\n",
      "174 33 31\n",
      "593 73 61\n",
      "236 27 26\n",
      "12 0 0\n",
      "98 12 23\n",
      "47 8 11\n",
      "568 77 70\n",
      "763 89 80\n",
      "not in: 186\n",
      "154 25 12\n",
      "213 22 21\n",
      "12 0 4\n",
      "230 27 24\n",
      "340 34 36\n",
      "less than 5: 192\n",
      "1010 112 114\n",
      "26 4 4\n",
      "61 5 10\n",
      "76 4 15\n",
      "71 3 10\n",
      "77 8 8\n",
      "31 3 2\n",
      "21 5 4\n",
      "517 65 51\n",
      "273 31 38\n",
      "14 3 3\n",
      "12 4 1\n",
      "26 1 6\n",
      "283 36 29\n"
     ]
    }
   ],
   "source": [
    "total_predict = np.array([])\n",
    "total_rating = np.array([])\n",
    "total_predict_v = np.array([])\n",
    "total_rating_v = np.array([])\n",
    "total_predict_tr = np.array([])\n",
    "total_rating_tr = np.array([])\n",
    "#train_rmse_compare_F = []\n",
    "for m in range(0, MOVIE_NUM+1):\n",
    "    if m not in movie_rating:\n",
    "        print(\"not in:\", m)\n",
    "        continue\n",
    "    user_rating_tups = movie_rating[m]\n",
    "    x_train, y_train, feature_name, user_id_train = gen_regr_data(int(m), user_rating_tups)\n",
    "    \n",
    "    if len(y_train) < 5:\n",
    "        print('less than 5:', m)\n",
    "        continue\n",
    "    #x_train = x_train[1:2]\n",
    "    #y_train = y_train[1:2]\n",
    "    \n",
    "    #if len(y_train) > 50:\n",
    "     #   continue\n",
    "    \n",
    "    if m in movie_rating_test:     \n",
    "        user_rating_tups_test = movie_rating_test[m]\n",
    "        x_test, y_test, _, user_id_test= gen_regr_data(int(m), user_rating_tups_test)\n",
    "    else:\n",
    "        x_test = []\n",
    "        y_test = []\n",
    "        user_id_test = []\n",
    "    \n",
    "    if m in movie_rating_validation:     \n",
    "        user_rating_tups_validation = movie_rating_validation[m]\n",
    "        x_valid, y_valid, _, user_id_valid= gen_regr_data(int(m), user_rating_tups_validation)\n",
    "    else:\n",
    "        x_valid = []\n",
    "        y_valid = []\n",
    "        user_id_valid = []\n",
    "        \n",
    "    print(len(x_train), len(x_test), len(x_valid))\n",
    "    feature = [0 for i in range(len(feature_name)-1)]\n",
    "    \n",
    "    for j in range(len(feature_name)-1):\n",
    "        name = feature_name[j]\n",
    "        dict_name = name.split('_')[0]\n",
    "        feature_n = name.split('_')[1]\n",
    "\n",
    "        \n",
    "        for i in range(len(x_train)):\n",
    "            feature[j] += (y_train[i] - x_train[i][j])\n",
    "        \n",
    "        feature[j] /= len(x_train)\n",
    "    \n",
    "    #weight = [1 - abs(f) for f in feature] if len(feature) > 0 else []\n",
    "    #select_indexs = select_feature_index(feature, 100)\n",
    "    #select_indexs = [1 for i in range(len(feature))]\n",
    "    #for i in range(len(feature)):\n",
    "        #print(feature_name[i], 1/(1-feature[i]))\n",
    "        #if select_indexs[i] == 1:\n",
    "            #print(feature_name[i], feature[i], weight[i])\n",
    "    #print()\n",
    "    \n",
    "\n",
    "    predict_t = []\n",
    "    predict_v = []\n",
    "    predict_tr = []\n",
    "    \n",
    "    for i in range(len(x_train)):\n",
    "        cc = get_predict_rating_old(x_train[i], feature)\n",
    "        #print(\"%.2f %.2f\" %(cc, y_train[i]))\n",
    "        #print()\n",
    "        predict_tr.append(cc)\n",
    "    \n",
    "    for i in range(len(x_test)):\n",
    "        cc = get_predict_rating_old(x_test[i], feature)\n",
    "        #print(\"%.2f %.2f\" %(cc, y_test[i]))\n",
    "        #print()\n",
    "        predict_t.append(cc)\n",
    "    \n",
    "\n",
    "    for i in range(len(x_valid)):\n",
    "        cc = get_predict_rating_old(x_valid[i], feature)\n",
    "        predict_v.append(cc)\n",
    "        #print(\"%.2f %.2f\" %(cc, y_valid[i]))\n",
    "        #print()\n",
    "     \n",
    "    train_rmse = np.sqrt(np.mean(np.square(np.subtract(predict_tr, y_train))))\n",
    "    valid_rmse = np.sqrt(np.mean(np.square(np.subtract(predict_v, y_valid))))\n",
    "    test_rmse = np.sqrt(np.mean(np.square(np.subtract(predict_t, y_test))))\n",
    "    \n",
    "    #print(m, train_rmse, valid_rmse, test_rmse)\n",
    "    #train_rmse_compare_F.append([m, len(y_train), len(y_valid),len(y_test), train_rmse, valid_rmse, test_rmse])\n",
    "    \n",
    "    total_predict = np.concatenate((total_predict, predict_t), axis = 0)\n",
    "    total_rating = np.concatenate((total_rating, y_test), axis = 0)\n",
    "    total_predict_v = np.concatenate((total_predict_v, predict_v), axis = 0)\n",
    "    total_rating_v = np.concatenate((total_rating_v, y_valid), axis = 0)\n",
    "    total_predict_tr = np.concatenate((total_predict_tr, predict_tr), axis = 0)\n",
    "    total_rating_tr = np.concatenate((total_rating_tr, y_train), axis = 0)\n",
    "    \n",
    "    #print(feature)\n",
    "    #print(feature_name)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse: 0.760961253976\n"
     ]
    }
   ],
   "source": [
    "rmse = np.sqrt(np.mean(np.square(np.subtract(total_predict, total_rating))))  \n",
    "print(\"train rmse:\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse: 0.29143128588\n",
      "train mae: 0.21081254878\n",
      "valid rmse: 0.753641973129\n",
      "valid mae: 0.554399160574\n",
      "test rmse: 0.760657148598\n",
      "test mae: 0.555822446024\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(total_predict)):\n",
    "    if total_predict[i] < 0:\n",
    "        total_predict[i] = 0.\n",
    "    if total_predict[i] > 5:\n",
    "        total_predict[i] = 5.\n",
    "\n",
    "for i in range(len(total_predict_v)):\n",
    "    if total_predict_v[i] < 0:\n",
    "        total_predict_v[i] = 0.\n",
    "    if total_predict_v[i] > 5:\n",
    "        total_predict_v[i] = 5.\n",
    "        \n",
    "for i in range(len(total_predict_tr)):\n",
    "    if total_predict_tr[i] < 0:\n",
    "        total_predict_tr[i] = 0.\n",
    "    if total_predict_tr[i] > 5:\n",
    "        total_predict_tr[i] = 5.\n",
    "\n",
    "        \n",
    "rmse = np.sqrt(np.mean(np.square(np.subtract(total_predict_tr, total_rating_tr))))  \n",
    "print(\"train rmse:\", rmse)\n",
    "\n",
    "mae = mean_absolute_error(total_rating_tr, total_predict_tr)\n",
    "print(\"train mae:\", mae)\n",
    "\n",
    "rmse = np.sqrt(np.mean(np.square(np.subtract(total_predict_v, total_rating_v))))  \n",
    "print(\"valid rmse:\", rmse)\n",
    "\n",
    "mae = mean_absolute_error(total_rating_v, total_predict_v)\n",
    "print(\"valid mae:\", mae)\n",
    "\n",
    "rmse = np.sqrt(np.mean(np.square(np.subtract(total_predict, total_rating))))  \n",
    "print(\"test rmse:\", rmse)\n",
    "\n",
    "mae = mean_absolute_error(total_rating, total_predict)\n",
    "print(\"test mae:\", mae)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# weight with number of featured movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not in 186\n"
     ]
    }
   ],
   "source": [
    "total_predict = np.array([])\n",
    "total_rating = np.array([])\n",
    "total_predict_v = np.array([])\n",
    "total_rating_v = np.array([])\n",
    "total_predict_tr = np.array([])\n",
    "total_rating_tr = np.array([])\n",
    "train_rmse_compare_E = []\n",
    "movie_feature = {}\n",
    "feature_name_dict = {}\n",
    "dropped_movie = 0\n",
    "info_prop = {}\n",
    "info_prop['t'] = 0\n",
    "info_prop['g'] = 0\n",
    "info_prop['c'] = 0\n",
    "info_prop['d'] = 0\n",
    "for m in range(0, MOVIE_NUM+1):\n",
    "    if m not in movie_rating:\n",
    "        print('not in', m)\n",
    "        dropped_movie += 1\n",
    "        continue\n",
    "    user_rating_tups = movie_rating[m]\n",
    "    x_train, y_train, feature_name, user_id_train = gen_regr_data(int(m), user_rating_tups)\n",
    "    \n",
    "    if len(y_train) < 0:\n",
    "        print('less than 5:', m)\n",
    "        dropped_movie += 1\n",
    "        continue\n",
    "    #if len(y_train) > 50:\n",
    "     #   continue\n",
    "    \n",
    "    if m in movie_rating_test:     \n",
    "        user_rating_tups_test = movie_rating_test[m]\n",
    "        x_test, y_test, _, user_id_test= gen_regr_data(int(m), user_rating_tups_test)\n",
    "    else:\n",
    "        x_test = []\n",
    "        y_test = []\n",
    "        user_id_test = []\n",
    "    \n",
    "    if m in movie_rating_validation:     \n",
    "        user_rating_tups_validation = movie_rating_validation[m]\n",
    "        x_valid, y_valid, _, user_id_valid= gen_regr_data(int(m), user_rating_tups_validation)\n",
    "    else:\n",
    "        x_valid = []\n",
    "        y_valid = []\n",
    "        user_id_valid = []\n",
    "    \n",
    "    if len(x_test) == 0:\n",
    "        continue\n",
    "        \n",
    "    \n",
    "    feature = [0 for i in range(len(feature_name)-1)]\n",
    "    \n",
    "    for j in range(len(feature_name)-1):\n",
    "        prop_sum = 0\n",
    "        name = feature_name[j]\n",
    "        dict_name = name.split('_')[0]\n",
    "        feature_n = name.split('_')[1]\n",
    "        offset = 0.01\n",
    "        try:\n",
    "            if dict_name == 'd':\n",
    "                feature_key = director_2_id[feature_n]\n",
    "                offset = 1/divid_list[0]\n",
    "            elif dict_name == 'c':\n",
    "                feature_key = cast_2_id[feature_n]\n",
    "                offset = 1/divid_list[2]\n",
    "            elif dict_name == 'g':\n",
    "                feature_key = genre_2_id[feature_n]\n",
    "                offsedivid_listt = 1/divid_list[3]\n",
    "            elif dict_name == 't':\n",
    "                feature_key = int(feature_n)\n",
    "                offset = 1/divid_list[1]\n",
    "        except:\n",
    "            feature_key = ''\n",
    "        \n",
    "        for i in range(len(x_train)):\n",
    "            try:\n",
    "                prop = user_count_list[dict_name][user_id_train[i]][feature_key]\n",
    "            except:\n",
    "                prop = 1\n",
    "            prop = offset * prop\n",
    "            #prop_sum += prop\n",
    "            #feature[j] += (y_train[i] - x_train[i][j]) * prop\n",
    "            prop_sum += 1\n",
    "            feature[j] += (y_train[i] - x_train[i][j])\n",
    "        \n",
    "        feature[j] /= prop_sum\n",
    "    feature_name_dict[m] = feature_name\n",
    "    movie_feature[m] = feature\n",
    "    #weight = [1 - abs(f) for f in feature] if len(feature) > 0 else []\n",
    "    #select_indexs = select_feature_index(feature, 100)\n",
    "    #select_indexs = [1 for i in range(len(feature))]\n",
    "    #for i in range(len(feature)):\n",
    "        #print(feature_name[i], 1/(1-feature[i]))\n",
    "        #if select_indexs[i] == 1:\n",
    "            #print(feature_name[i], feature[i], weight[i])\n",
    "    #print()\n",
    "    \n",
    "\n",
    "    predict_t = []\n",
    "    predict_v = []\n",
    "    predict_tr = []\n",
    "    for i in range(len(x_train)):\n",
    "        cc,info_prop_s = get_predict_rating_count(x_train[i], feature, user_id_train[i], feature_name)\n",
    "        #print(\"%.2f %.2f\" %(cc, y_train[i]))\n",
    "        #print()\n",
    "        for key in info_prop:\n",
    "            if key in info_prop_s:\n",
    "                info_prop[key] = info_prop[key] + info_prop_s[key]\n",
    "        predict_tr.append(cc)\n",
    "    \n",
    "    for i in range(len(x_test)):\n",
    "        cc,info_prop_s = get_predict_rating_count(x_test[i], feature, user_id_test[i], feature_name)\n",
    "        #print(\"%.2f %.2f\" %(cc, y_test[i]))\n",
    "        #print()\n",
    "        for key in info_prop:\n",
    "            if key in info_prop_s:\n",
    "                info_prop[key] = info_prop[key] + info_prop_s[key]\n",
    "        predict_t.append(cc)\n",
    "    \n",
    "\n",
    "    for i in range(len(x_valid)):\n",
    "        cc,info_prop_s = get_predict_rating_count(x_valid[i], feature, user_id_valid[i], feature_name )\n",
    "        for key in info_prop:\n",
    "            if key in info_prop_s:\n",
    "                info_prop[key] = info_prop[key] + info_prop_s[key]\n",
    "        predict_v.append(cc)\n",
    "        #print(\"%.2f %.2f\" %(cc, y_valid[i]))\n",
    "        #print()\n",
    "        \n",
    "        \n",
    "    #train_rmse = np.sqrt(np.mean(np.square(np.subtract(predict_tr, y_train))))\n",
    "    #valid_rmse = np.sqrt(np.mean(np.square(np.subtract(predict_v, y_valid))))\n",
    "    #test_rmse = np.sqrt(np.mean(np.square(np.subtract(predict_t, y_test))))\n",
    "    \n",
    "    #print(m, train_rmse, valid_rmse, test_rmse)\n",
    "    \n",
    "    #print(m, train_rmse, valid_rmse, test_rmse)\n",
    "    #train_rmse_compare_E.append([m, len(y_train), len(y_valid),len(y_test), train_rmse, valid_rmse, test_rmse])\n",
    "    \n",
    "        \n",
    "    total_predict = np.concatenate((total_predict, predict_t), axis = 0)\n",
    "    total_rating = np.concatenate((total_rating, y_test), axis = 0)\n",
    "    total_predict_v = np.concatenate((total_predict_v, predict_v), axis = 0)\n",
    "    total_rating_v = np.concatenate((total_rating_v, y_valid), axis = 0)\n",
    "    total_predict_tr = np.concatenate((total_predict_tr, predict_tr), axis = 0)\n",
    "    total_rating_tr = np.concatenate((total_rating_tr, y_train), axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9932"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t : 0.101275083226\n",
      "g : 0.522117231551\n",
      "c : 0.275120535756\n",
      "d : 0.101487149467\n"
     ]
    }
   ],
   "source": [
    "summ = 0\n",
    "for key in info_prop:\n",
    "    summ += info_prop[key]\n",
    "\n",
    "for key in info_prop:\n",
    "    print(key, ':', info_prop[key]/summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_Michael Bay 0.03360957642725598\n",
      "g_Action -0.4520801335041177\n",
      "g_Adventure -0.5222022198128377\n",
      "g_Sci-Fi -0.5247564581382188\n",
      "t_0 -0.07597237047091458\n",
      "c_Shia LaBeouf -0.04929876753081174\n",
      "c_Rosie Huntington-Whiteley -7.68773217788322e-17\n",
      "c_Josh Duhamel 0.0\n"
     ]
    }
   ],
   "source": [
    "index = 3\n",
    "for i in range(len(movie_feature[index])):\n",
    "    print(feature_name_dict[index][i], movie_feature[index][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.402485198126\n",
      "0.288492403305\n",
      "0.75007781213\n",
      "0.552879152063\n",
      "0.757359215551\n",
      "0.554899368944\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(total_predict)):\n",
    "    if total_predict[i] < 0:\n",
    "        total_predict[i] = 0.\n",
    "    if total_predict[i] > 5:\n",
    "        total_predict[i] = 5.\n",
    "\n",
    "for i in range(len(total_predict_v)):\n",
    "    if total_predict_v[i] < 0:\n",
    "        total_predict_v[i] = 0.\n",
    "    if total_predict_v[i] > 5:\n",
    "        total_predict_v[i] = 5.\n",
    "        \n",
    "for i in range(len(total_predict_tr)):\n",
    "    if total_predict_tr[i] < 0:\n",
    "        total_predict_tr[i] = 0.\n",
    "    if total_predict_tr[i] > 5:\n",
    "        total_predict_tr[i] = 5.\n",
    "\n",
    "        \n",
    "rmse = np.sqrt(np.mean(np.square(np.subtract(total_predict_tr, total_rating_tr))))  \n",
    "print( rmse)\n",
    "\n",
    "mae = mean_absolute_error(total_rating_tr, total_predict_tr)\n",
    "print(mae)\n",
    "\n",
    "rmse = np.sqrt(np.mean(np.square(np.subtract(total_predict_v, total_rating_v))))  \n",
    "print(rmse)\n",
    "\n",
    "mae = mean_absolute_error(total_rating_v, total_predict_v)\n",
    "print( mae)\n",
    "\n",
    "\n",
    "rmse = np.sqrt(np.mean(np.square(np.subtract(total_predict, total_rating))))  \n",
    "print( rmse)\n",
    "\n",
    "mae = mean_absolute_error(total_rating, total_predict)\n",
    "print(mae)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get one user's information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_id = 5\n",
    "feature_dict = ['d','t','c', 'g']\n",
    "one_user_count_list = {}\n",
    "for type_name in feature_dict:\n",
    "    one_user_count_list[type_name] = {}\n",
    "for i in range(len(feature_dict)):\n",
    "    dict_name = feature_dict[i]\n",
    "    divid_value = divid_list[i]\n",
    "    for feature_key in user_count_list[dict_name][user_id]:\n",
    "        one_user_count_list[dict_name][feature_key] = user_count_list[dict_name][user_id][feature_key]/divid_value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "less than 5: 56\n",
      "less than 5: 69\n",
      "less than 5: 108\n",
      "less than 5: 171\n",
      "not in 186\n",
      "less than 5: 192\n"
     ]
    }
   ],
   "source": [
    "user_movie_rating = {}\n",
    "user_movie_info = {}\n",
    "\n",
    "for m in range(0, MOVIE_NUM+1):\n",
    "    if m not in movie_rating:\n",
    "        print('not in', m)\n",
    "        dropped_movie += 1\n",
    "        continue\n",
    "    user_rating_tups = movie_rating[m]\n",
    "    x_train, y_train, feature_name, user_id_train = gen_regr_data(int(m), user_rating_tups)\n",
    "    if len(y_train) < 5:\n",
    "        print('less than 5:', m)\n",
    "        dropped_movie += 1\n",
    "        continue\n",
    "    \n",
    "    one_uesr_ratings = []\n",
    "    one_user_info = []\n",
    "    weight = []\n",
    "    x_pred, _, feature_name, user_id_pred = gen_regr_data(int(m), [(user_id, 0)])\n",
    "    if user_id in user_id_train:\n",
    "        ind = user_id_train.index(user_id)\n",
    "        real_rating = y_train[ind]\n",
    "    else:\n",
    "        real_rating = -1\n",
    "    for j in range(len(feature_name)-1):\n",
    "        prop_sum = 0\n",
    "        name = feature_name[j]\n",
    "        dict_name = name.split('_')[0]\n",
    "        feature_n = name.split('_')[1]\n",
    "        offset = 0.01\n",
    "        try:\n",
    "            if dict_name == 'd':\n",
    "                feature_key = director_2_id[feature_n]\n",
    "            elif dict_name == 'c':\n",
    "                feature_key = cast_2_id[feature_n]\n",
    "            elif dict_name == 'g':\n",
    "                feature_key = genre_2_id[feature_n]\n",
    "            elif dict_name == 't':\n",
    "                feature_key = int(feature_n)\n",
    "        except:\n",
    "            feature_key = ''\n",
    "        try:\n",
    "            weight.append(one_user_count_list[dict_name][feature_key])\n",
    "        except:\n",
    "            # if user have never seen this information\n",
    "            weight.append(0.1)\n",
    "\n",
    "    weight_normal = [w/sum(weight) for w in weight]\n",
    "    #print('begin')\n",
    "    try:\n",
    "        one_pred_value = sum([weight_normal[i]*(x_pred[0][i] + movie_feature[m][i]) for i in range(len(x_pred[0]))])\n",
    "    except:\n",
    "        continue\n",
    "    feature_name_dict\n",
    "    \"\"\"\n",
    "    print(\"movie:\", m)\n",
    "    print(\"pred:\", one_pred_value)\n",
    "    print(\"real:\", real_rating)\n",
    "    \n",
    "    for i in range(len(x_pred[0])):\n",
    "        print(feature_name_dict[m][i], x_pred[0][i], movie_feature[m][i], weight_normal[i])\n",
    "    \"\"\"\n",
    "    user_movie_rating[m] = (one_pred_value, real_rating)\n",
    "    user_movie_info[m] = {}\n",
    "    user_movie_info[m]['info_name'] = feature_name_dict[m][0:-1]\n",
    "    user_movie_info[m]['info_rating'] = x_pred[0]\n",
    "    user_movie_info[m]['movie_offset'] = movie_feature[m]\n",
    "    user_movie_info[m]['weight'] = weight_normal\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor m in user_movie_rating:\\n    if user_movie_rating[m][1] > 0:\\n        print(m, user_movie_rating[m][1])\\n        length = len(user_movie_info[m]['info_rating'])\\n        for i in range((length)):\\n            print(user_movie_info[m]['info_name'][i], user_movie_info[m]['info_rating'][i] + user_movie_info[m]['movie_offset'][i], user_movie_info[m]['weight'][i])\\n\\n\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "for m in user_movie_rating:\n",
    "    if user_movie_rating[m][1] > 0:\n",
    "        print(m, user_movie_rating[m][1])\n",
    "        length = len(user_movie_info[m]['info_rating'])\n",
    "        for i in range((length)):\n",
    "            print(user_movie_info[m]['info_name'][i], user_movie_info[m]['info_rating'][i] + user_movie_info[m]['movie_offset'][i], user_movie_info[m]['weight'][i])\n",
    "\n",
    "\"\"\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_movie = -1\n",
    "best_rating = 0\n",
    "for m in user_movie_rating:\n",
    "    if user_movie_rating[m][1] < 0 and user_movie_rating[m][0] > best_rating:\n",
    "        best_rating = user_movie_rating[m][0]\n",
    "        best_movie = m\n",
    "\n",
    "best_movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1817",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-e469be7b0d14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbest_movie\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1817\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rating:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muser_movie_rating\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_movie\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_movie_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_movie\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'info_rating'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0muser_movie_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_movie\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'info_rating'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0muser_movie_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_movie\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'movie_offset'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0muser_movie_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_movie\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1817"
     ]
    }
   ],
   "source": [
    "best_movie = 1817\n",
    "print('rating:',user_movie_rating[best_movie])\n",
    "length = len(user_movie_info[best_movie]['info_rating'])\n",
    "for i in range((length)):\n",
    "    part = (user_movie_info[best_movie]['info_rating'][i] + user_movie_info[best_movie]['movie_offset'][i]) * user_movie_info[best_movie]['weight'][i]\n",
    "    print(user_movie_info[best_movie]['info_name'][i], user_movie_info[best_movie]['info_rating'][i], user_movie_info[best_movie]['movie_offset'][i], user_movie_info[best_movie]['weight'][i], part)\n",
    "    #print(user_movie_info[best_movie]['info_name'][i])\n",
    "    \n",
    "    #print(part)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## test cold start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_predict_rating_0(user_rating, features):\n",
    "    return np.mean([user_rating[i] for i in range(len(user_rating))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_predict = np.array([])\n",
    "total_rating = np.array([])\n",
    "total_predict_v = np.array([])\n",
    "total_rating_v = np.array([])\n",
    "total_predict_tr = np.array([])\n",
    "total_rating_tr = np.array([])\n",
    "train_rmse_compare_E = []\n",
    "movie_feature = {}\n",
    "feature_name_dict = {}\n",
    "dropped_movie = 0\n",
    "total_predicted = 0\n",
    "info_prop = {}\n",
    "info_prop['t'] = 0\n",
    "info_prop['g'] = 0\n",
    "info_prop['c'] = 0\n",
    "info_prop['d'] = 0\n",
    "largest_train = 10\n",
    "_RMSE_list = []\n",
    "for largest_train in range(0, 200, 3):\n",
    "    for m in range(0, MOVIE_NUM+1):\n",
    "        have_train = False\n",
    "        x_train = []\n",
    "        y_train = []\n",
    "        if m in movie_rating:\n",
    "            user_rating_tups = movie_rating[m]\n",
    "            x_train, y_train, feature_name, user_id_train = gen_regr_data(int(m), user_rating_tups)\n",
    "            have_train = True\n",
    "\n",
    "        if len(y_train) > largest_train:\n",
    "            #print('larger:', m)\n",
    "            #dropped_movie += 1\n",
    "            continue\n",
    "        #if len(y_train) > 50:\n",
    "         #   continue\n",
    "\n",
    "        if m in movie_rating_test:     \n",
    "            user_rating_tups_test = movie_rating_test[m]\n",
    "            x_test, y_test, _, user_id_test= gen_regr_data(int(m), user_rating_tups_test)\n",
    "        else:\n",
    "            x_test = []\n",
    "            y_test = []\n",
    "            user_id_test = []\n",
    "        \"\"\"\n",
    "        if m in movie_rating_validation:     \n",
    "            user_rating_tups_validation = movie_rating_validation[m]\n",
    "            x_valid, y_valid, _, user_id_valid= gen_regr_data(int(m), user_rating_tups_validation)\n",
    "        else:\n",
    "            x_valid = []\n",
    "            y_valid = []\n",
    "            user_id_valid = []\n",
    "        \"\"\"\n",
    "        if len(x_test) == 0:\n",
    "            continue\n",
    "\n",
    "\n",
    "        feature = [1.0/(len(feature_name)-1) for i in range(len(feature_name)-1)]\n",
    "\n",
    "        for j in range(len(feature_name)-1):\n",
    "            prop_sum = 0\n",
    "            name = feature_name[j]\n",
    "            dict_name = name.split('_')[0]\n",
    "            feature_n = name.split('_')[1]\n",
    "            offset = 0.01\n",
    "            try:\n",
    "                if dict_name == 'd':\n",
    "                    feature_key = director_2_id[feature_n]\n",
    "                    offset = 1/divid_list[0]\n",
    "                elif dict_name == 'c':\n",
    "                    feature_key = cast_2_id[feature_n]\n",
    "                    offset = 1/divid_list[2]\n",
    "                elif dict_name == 'g':\n",
    "                    feature_key = genre_2_id[feature_n]\n",
    "                    offsedivid_listt = 1/divid_list[3]\n",
    "                elif dict_name == 't':\n",
    "                    feature_key = int(feature_n)\n",
    "                    offset = 1/divid_list[1]\n",
    "            except:\n",
    "                feature_key = ''\n",
    "            #print(len(x_train))\n",
    "            for i in range(len(x_train)):\n",
    "                try:\n",
    "                    prop = user_count_list[dict_name][user_id_train[i]][feature_key]\n",
    "                except:\n",
    "                    prop = 1\n",
    "                prop = offset * prop\n",
    "                #prop_sum += prop\n",
    "                #feature[j] += (y_train[i] - x_train[i][j]) * prop\n",
    "                prop_sum += 1\n",
    "                feature[j] += (y_train[i] - x_train[i][j])\n",
    "\n",
    "            if len(x_train) > 0:\n",
    "                feature[j] /= prop_sum\n",
    "\n",
    "        feature_name_dict[m] = feature_name\n",
    "        movie_feature[m] = feature\n",
    "        #weight = [1 - abs(f) for f in feature] if len(feature) > 0 else []\n",
    "        #select_indexs = select_feature_index(feature, 100)\n",
    "        #select_indexs = [1 for i in range(len(feature))]\n",
    "        #for i in range(len(feature)):\n",
    "            #print(feature_name[i], 1/(1-feature[i]))\n",
    "            #if select_indexs[i] == 1:\n",
    "                #print(feature_name[i], feature[i], weight[i])\n",
    "        #print()\n",
    "\n",
    "\n",
    "        predict_t = []\n",
    "        predict_v = []\n",
    "        predict_tr = []\n",
    "\n",
    "        \"\"\"\n",
    "        for i in range(len(x_train)):\n",
    "            cc,info_prop_s = get_predict_rating_count(x_train[i], feature, user_id_train[i], feature_name)\n",
    "            #print(\"%.2f %.2f\" %(cc, y_train[i]))\n",
    "            #print()\n",
    "            for key in info_prop:\n",
    "                if key in info_prop_s:\n",
    "                    info_prop[key] = info_prop[key] + info_prop_s[key]\n",
    "            predict_tr.append(cc)\n",
    "\n",
    "        for i in range(len(x_valid)):\n",
    "            cc,info_prop_s = get_predict_rating_count(x_valid[i], feature, user_id_valid[i], feature_name )\n",
    "            for key in info_prop:\n",
    "                if key in info_prop_s:\n",
    "                    info_prop[key] = info_prop[key] + info_prop_s[key]\n",
    "            predict_v.append(cc)\n",
    "        \"\"\"\n",
    "        for i in range(len(x_test)):\n",
    "            if have_train:\n",
    "                cc,info_prop_s = get_predict_rating_count(x_test[i], feature, user_id_test[i], feature_name)\n",
    "            else:\n",
    "                cc = get_predict_rating_0(x_test[i], feature)\n",
    "            #print(\"%.2f %.2f\" %(cc, y_test[i]))\n",
    "            #print()\n",
    "            total_predicted += 1\n",
    "            predict_t.append(cc)\n",
    "\n",
    "        #train_rmse = np.sqrt(np.mean(np.square(np.subtract(predict_tr, y_train))))\n",
    "        #valid_rmse = np.sqrt(np.mean(np.square(np.subtract(predict_v, y_valid))))\n",
    "        #test_rmse = np.sqrt(np.mean(np.square(np.subtract(predict_t, y_test))))\n",
    "\n",
    "        #print(m, train_rmse, valid_rmse, test_rmse)\n",
    "\n",
    "        #print(m, train_rmse, valid_rmse, test_rmse)\n",
    "        #train_rmse_compare_E.append([m, len(y_train), len(y_valid),len(y_test), train_rmse, valid_rmse, test_rmse])\n",
    "\n",
    "\n",
    "        total_predict = np.concatenate((total_predict, predict_t), axis = 0)\n",
    "        total_rating = np.concatenate((total_rating, y_test), axis = 0)\n",
    "        total_predict_v = np.concatenate((total_predict_v, predict_v), axis = 0)\n",
    "        total_rating_v = np.concatenate((total_rating_v, y_valid), axis = 0)\n",
    "        total_predict_tr = np.concatenate((total_predict_tr, predict_tr), axis = 0)\n",
    "        total_rating_tr = np.concatenate((total_rating_tr, y_train), axis = 0)\n",
    "    rmse = np.sqrt(np.mean(np.square(np.subtract(total_predict, total_rating)))) \n",
    "    _RMSE_list.append((largest_train, rmse))\n",
    "    print(largest_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(np.mean(np.square(np.subtract(total_predict, total_rating))))  \n",
    "print( rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(total_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_RMSE_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('MMF_RMSE.csv','w') as f:\n",
    "    f_csv = csv.writer(f)\n",
    "    headers = ['num, RMSE']\n",
    "    f_csv.writerow(headers)\n",
    "    for p in _RMSE_list:\n",
    "        row = [p[0], p[1]]\n",
    "        f_csv.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ws3.6",
   "language": "python",
   "name": "ws"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
