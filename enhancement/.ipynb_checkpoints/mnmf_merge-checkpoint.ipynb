{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie: 206\n",
      "user: 3550\n",
      "director: 194\n",
      "genre: 20\n",
      "topic: 50\n",
      "cast: 474\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nMOVIE_NUM = 1682\\nDIRECTOR_NUM = 1139\\nGENRE_NUM = 24\\nTOPIC_NUM = 50\\nCAST_NUM = 2894\\nUSER_NUM = 943\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Global values\n",
    "\"\"\"\n",
    "folder = 'boxoffice_10'\n",
    "\n",
    "len_dtcgum = pickle.load(open('../data/'+folder+'/len_dtcgum.pkl', 'rb'))\n",
    "\n",
    "FEATURE_LEN = 5\n",
    "\n",
    "MOVIE_NUM = len_dtcgum[5]\n",
    "DIRECTOR_NUM = len_dtcgum[0]\n",
    "GENRE_NUM = len_dtcgum[3]\n",
    "TOPIC_NUM = 50\n",
    "CAST_NUM = len_dtcgum[2]\n",
    "USER_NUM = len_dtcgum[4]\n",
    "\n",
    "print('movie:', MOVIE_NUM)\n",
    "print('user:', USER_NUM)\n",
    "print('director:', DIRECTOR_NUM)\n",
    "print('genre:', GENRE_NUM)\n",
    "print('topic:', TOPIC_NUM)\n",
    "print('cast:', CAST_NUM)\n",
    "\n",
    "\"\"\"\n",
    "MOVIE_NUM = 1682\n",
    "DIRECTOR_NUM = 1139\n",
    "GENRE_NUM = 24\n",
    "TOPIC_NUM = 50\n",
    "CAST_NUM = 2894\n",
    "USER_NUM = 943\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movie_rating = pickle.load(open('../data/'+folder+'/movie_rating.pkl', 'rb'))\n",
    "movie_rating_test = pickle.load(open('../data/'+folder+'/movie_rating_test.pkl', 'rb'))\n",
    "movie_rating_validation = pickle.load(open('../data/'+folder+'/movie_rating_validation.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "def load_data(rating_file):\n",
    "    user_list = []\n",
    "    movie_list = []\n",
    "    rating_list = []\n",
    "    for movie_id in rating_file:\n",
    "        for tup in rating_file[movie_id]:\n",
    "            user_id = tup[0]\n",
    "            rating = tup[1]\n",
    "            user_list.append(user_id)\n",
    "            movie_list.append(movie_id)\n",
    "            rating_list.append(rating)\n",
    "    \n",
    "    return user_list, movie_list, rating_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load training data\n",
    "total_train_user, total_train_movie, total_train_rating = load_data(movie_rating)\n",
    "total_valid_user, total_valid_movie, total_valid_rating = load_data(movie_rating_validation)\n",
    "total_test_user, total_test_movie, total_test_rating = load_data(movie_rating_test)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load validation and test data\n",
    "total_train_user_v = []\n",
    "total_train_movie_v = []\n",
    "total_train_rating_v = []\n",
    "for movie_id in movie_rating:\n",
    "    for tup in movie_rating[movie_id]:\n",
    "        user_id = tup[0]\n",
    "        rating = tup[1]\n",
    "        total_train_user.append(user_id)\n",
    "        total_train_movie.append(movie_id)\n",
    "        total_train_rating.append(rating)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(user_name, movie_name, rating_name, last_same):\n",
    "    user = pickle.load(open('../data/'+folder+'/'+user_name+last_same+'.pkl', 'rb'))\n",
    "    movie = pickle.load(open('../data/'+folder+'/'+movie_name+last_same+'.pkl', 'rb'))\n",
    "    rating = pickle.load(open('../data/'+folder+'/'+rating_name+last_same+'.pkl', 'rb'))\n",
    "    return shuffle(user, movie, rating)\n",
    "\n",
    "#load training data\n",
    "d_user, d_movie, d_rating = load_data(\"train_user\", \"train_movie\", \"train_rating\", '_d')\n",
    "t_user, t_movie, t_rating = load_data(\"train_user\", \"train_movie\", \"train_rating\", '_t')\n",
    "g_user, g_movie, g_rating = load_data(\"train_user\", \"train_movie\", \"train_rating\", '_g')   \n",
    "c_user, c_movie, c_rating = load_data(\"train_user\", \"train_movie\", \"train_rating\", '_c')\n",
    "\n",
    "#load validation data\n",
    "d_user_v, d_movie_v, d_rating_v = load_data(\"train_user\", \"train_movie\", \"train_rating\", '_d_v')\n",
    "t_user_v, t_movie_v, t_rating_v = load_data(\"train_user\", \"train_movie\", \"train_rating\", '_t_v')\n",
    "g_user_v, g_movie_v, g_rating_v = load_data(\"train_user\", \"train_movie\", \"train_rating\", '_g_v')        \n",
    "c_user_v, c_movie_v, c_rating_v = load_data(\"train_user\", \"train_movie\", \"train_rating\", '_c_v')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nminvalue = 0\\nmaxvalue = 10\\nU = tf.clip_by_value(U, minvalue, maxvalue)\\nD = tf.clip_by_value(D, minvalue, maxvalue)\\nT = tf.clip_by_value(T, minvalue, maxvalue)\\nG = tf.clip_by_value(G, minvalue, maxvalue)\\nC = tf.clip_by_value(C, minvalue, maxvalue)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U = tf.Variable(initial_value=tf.truncated_normal([USER_NUM, FEATURE_LEN]), name='users')\n",
    "D = tf.Variable(initial_value=tf.truncated_normal([FEATURE_LEN, DIRECTOR_NUM]), name='directors')\n",
    "T = tf.Variable(initial_value=tf.truncated_normal([FEATURE_LEN, TOPIC_NUM]), name='topics')\n",
    "G = tf.Variable(initial_value=tf.truncated_normal([FEATURE_LEN, GENRE_NUM]), name='genures')\n",
    "C = tf.Variable(initial_value=tf.truncated_normal([FEATURE_LEN, CAST_NUM]), name='casts')\n",
    "\n",
    "\"\"\"\n",
    "minvalue = 0\n",
    "maxvalue = 10\n",
    "U = tf.clip_by_value(U, minvalue, maxvalue)\n",
    "D = tf.clip_by_value(D, minvalue, maxvalue)\n",
    "T = tf.clip_by_value(T, minvalue, maxvalue)\n",
    "G = tf.clip_by_value(G, minvalue, maxvalue)\n",
    "C = tf.clip_by_value(C, minvalue, maxvalue)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate the in the film matrix\n",
    "\n",
    "### if the information is in the movie, the value will be 1, otherwise will be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_result = tf.matmul(U, D)\n",
    "t_result = tf.matmul(U, T)\n",
    "g_result = tf.matmul(U, G)\n",
    "c_result = tf.matmul(U, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MatMul:0' shape=(3550, 194) dtype=float32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_var = tf.Variable(initial_value=tf.random_normal([USER_NUM, DIRECTOR_NUM], mean=1,stddev=0.5), name='d_var')\n",
    "t_var = tf.Variable(initial_value=tf.random_normal([USER_NUM, TOPIC_NUM], mean=1, stddev=0.5), name='t_var')\n",
    "g_var = tf.Variable(initial_value=tf.random_normal([USER_NUM, GENRE_NUM], mean=1,stddev=0.5), name='g_var')\n",
    "c_var = tf.Variable(initial_value=tf.random_normal([USER_NUM, CAST_NUM], mean=1,stddev=0.5), name='c_var')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_result = d_result + d_var\n",
    "t_result = t_result + t_var\n",
    "g_result = g_result + g_var\n",
    "c_result = c_result + c_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movie_feature_dict = pickle.load(open('../data/'+folder+'/movie_feature_dict.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#build is_in_matrix, if feature is in the movie, the value in matrix will be 1, otherwise will be 0\n",
    "d_isin = np.zeros([MOVIE_NUM+1, DIRECTOR_NUM])\n",
    "t_isin = np.zeros([MOVIE_NUM+1, TOPIC_NUM])\n",
    "c_isin = np.zeros([MOVIE_NUM+1, CAST_NUM])\n",
    "g_isin = np.zeros([MOVIE_NUM+1, GENRE_NUM])\n",
    "\n",
    "for key in movie_feature_dict:\n",
    "    for f_id in movie_feature_dict[key]['d']:\n",
    "        d_isin[key,f_id] = 1\n",
    "    \n",
    "    for f_id in movie_feature_dict[key]['t']:\n",
    "        t_isin[key,f_id] = 1\n",
    "    \n",
    "    for f_id in movie_feature_dict[key]['c']:\n",
    "        c_isin[key,f_id] = 1\n",
    "    \n",
    "    for f_id in movie_feature_dict[key]['g']:\n",
    "        g_isin[key,f_id] = 1\n",
    "    \n",
    "d_isin = tf.constant(d_isin, tf.float32)\n",
    "t_isin = tf.constant(t_isin, tf.float32)\n",
    "c_isin = tf.constant(c_isin, tf.float32)\n",
    "g_isin = tf.constant(g_isin, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "md_var = tf.Variable(initial_value=tf.truncated_normal([MOVIE_NUM+1, DIRECTOR_NUM]), name='md_var')\n",
    "mt_var = tf.Variable(initial_value=tf.truncated_normal([MOVIE_NUM+1, TOPIC_NUM]), name='mt_var')\n",
    "mg_var = tf.Variable(initial_value=tf.truncated_normal([MOVIE_NUM+1, GENRE_NUM]), name='mg_var')\n",
    "mc_var = tf.Variable(initial_value=tf.truncated_normal([MOVIE_NUM+1, CAST_NUM]), name='mc_var')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#md_var = tf.Variable(initial_value=tf.random_normal([MOVIE_NUM+1, DIRECTOR_NUM], mean=1,stddev=0.05), name='md_var')\n",
    "#mt_var = tf.Variable(initial_value=tf.random_normal([MOVIE_NUM+1, TOPIC_NUM], mean=1,stddev=0.05), name='mt_var')\n",
    "#mg_var = tf.Variable(initial_value=tf.random_normal([MOVIE_NUM+1, GENRE_NUM], mean=1,stddev=0.05), name='mg_var')\n",
    "#mc_var = tf.Variable(initial_value=tf.random_normal([MOVIE_NUM+1, CAST_NUM], mean=1,stddev=0.05), name='mc_var')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d_isin = d_isin * md_var\n",
    "#t_isin = t_isin * mt_var\n",
    "#g_isin = g_isin * mg_var\n",
    "#c_isin = c_isin * mc_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count how many feature in each movie\n",
    "movie_feature_num = np.zeros([MOVIE_NUM+1,1])\n",
    "for key in movie_feature_dict:\n",
    "    total_num = 0\n",
    "    for f_key in movie_feature_dict[key]:\n",
    "        total_num += len(movie_feature_dict[key][f_key])\n",
    "    movie_feature_num[key] = 1/total_num\n",
    "feature_num_tensor = tf.tile(movie_feature_num, [1, USER_NUM])\n",
    "feature_num_tensor = tf.cast(feature_num_tensor, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Cast:0' shape=(207, 3550) dtype=float32>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_num_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_nm = tf.matmul(d_result, tf.transpose(d_isin))\n",
    "t_nm = tf.matmul(t_result, tf.transpose(t_isin))\n",
    "c_nm = tf.matmul(c_result, tf.transpose(c_isin))\n",
    "g_nm = tf.matmul(g_result, tf.transpose(g_isin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MatMul:0' shape=(3550, 194) dtype=float32>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Total_Rate = (d_nm + t_nm + c_nm + g_nm)*tf.transpose(feature_num_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clip_U = U.assign(tf.maximum(tf.zeros_like(U), U))\n",
    "clip_D = D.assign(tf.maximum(tf.zeros_like(D), D))\n",
    "clip_T = T.assign(tf.maximum(tf.zeros_like(T), T))\n",
    "clip_C = C.assign(tf.maximum(tf.zeros_like(C), C))\n",
    "clip_G = G.assign(tf.maximum(tf.zeros_like(G), G))\n",
    "clip = tf.group(clip_U, clip_D,clip_T,clip_C,clip_G)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_result_flatten = tf.reshape(Total_Rate, [-1])\n",
    "TR_R = tf.gather(total_result_flatten, total_train_user * tf.shape(Total_Rate)[1] + total_train_movie, name='total_user_rate')\n",
    "TR_R_test = tf.gather(total_result_flatten, total_test_user * tf.shape(Total_Rate)[1] + total_test_movie, name='total_user_rate_test')\n",
    "TR_R_valid = tf.gather(total_result_flatten, total_valid_user * tf.shape(Total_Rate)[1] + total_valid_movie, name='total_user_rate_valid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "d_result_flatten = tf.reshape(d_result, [-1])\n",
    "t_result_flatten = tf.reshape(t_result, [-1])\n",
    "g_result_flatten = tf.reshape(g_result, [-1])\n",
    "c_result_flatten = tf.reshape(c_result, [-1])\n",
    "\n",
    "D_R = tf.gather(d_result_flatten, d_user * tf.shape(d_result)[1] + d_movie, name='director_user_rate')\n",
    "T_R = tf.gather(t_result_flatten, t_user * tf.shape(t_result)[1] + t_movie, name='topic_user_rate')\n",
    "G_R = tf.gather(g_result_flatten, g_user * tf.shape(g_result)[1] + g_movie, name='genre_user_rate')\n",
    "C_R = tf.gather(c_result_flatten, c_user * tf.shape(c_result)[1] + c_movie, name='cast_user_rate')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_diff_op = tf.subtract(TR_R, total_train_rating, name='total_trainig_diff')\n",
    "#total_diff_op_squared = tf.abs(total_diff_op, name=\"total_squared_difference\")\n",
    "#total_cost = tf.reduce_sum(total_diff_op_squared, name=\"total_sum_squared_error\")/10000 \n",
    "\n",
    "total_cost = tf.nn.l2_loss(total_diff_op)\n",
    "\n",
    "\n",
    "d_diff_op = tf.subtract(D_R, d_rating, name='d_trainig_diff')\n",
    "t_diff_op = tf.subtract(T_R, t_rating, name='t_trainig_diff')\n",
    "g_diff_op = tf.subtract(G_R, g_rating, name='g_trainig_diff')\n",
    "c_diff_op = tf.subtract(C_R, c_rating, name='c_trainig_diff')\n",
    "\n",
    "d_diff_op_squared = tf.abs(d_diff_op, name=\"d_squared_difference\")\n",
    "t_diff_op_squared = tf.abs(t_diff_op, name=\"t_squared_difference\")\n",
    "g_diff_op_squared = tf.abs(g_diff_op, name=\"g_squared_difference\")\n",
    "c_diff_op_squared = tf.abs(c_diff_op, name=\"c_squared_difference\")\n",
    "\n",
    "d_base_cost = tf.reduce_sum(d_diff_op_squared, name=\"d_sum_squared_error\")/ tf.cast(tf.shape(d_diff_op_squared)[0], tf.float32) * 10000 \n",
    "t_base_cost = tf.reduce_sum(t_diff_op_squared, name=\"t_sum_squared_error\")/ tf.cast(tf.shape(t_diff_op_squared)[0], tf.float32) * 10000\n",
    "g_base_cost = tf.reduce_sum(g_diff_op_squared, name=\"g_sum_squared_error\")/ tf.cast(tf.shape(g_diff_op_squared)[0], tf.float32) * 10000\n",
    "c_base_cost = tf.reduce_sum(c_diff_op_squared, name=\"c_sum_squared_error\")/ tf.cast(tf.shape(c_diff_op_squared)[0], tf.float32) * 10000\n",
    "\n",
    "#base_cost = d_base_cost + t_base_cost + g_base_cost + c_base_cost + total_cost/10\n",
    "base_cost = total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = tf.constant(.001, name='lambda')\n",
    "\n",
    "\"\"\"\n",
    "u_norm = tf.reduce_sum(tf.abs(U, name='user_abs'), name='user_norm') / tf.cast(tf.shape(U)[0], tf.float32)\n",
    "d_norm = tf.reduce_sum(tf.abs(D, name='director_abs'), name='director_norm')/ tf.cast(tf.shape(D)[1], tf.float32)\n",
    "t_norm = tf.reduce_sum(tf.abs(T, name='topic_abs'), name='topic_norm')/ tf.cast(tf.shape(T)[1], tf.float32)\n",
    "g_norm = tf.reduce_sum(tf.abs(G, name='genre_abs'), name='genre_norm')/ tf.cast(tf.shape(G)[1], tf.float32)\n",
    "c_norm = tf.reduce_sum(tf.abs(C, name='cast_abs'), name='cast_norm')/ tf.cast(tf.shape(C)[1], tf.float32)\n",
    "\"\"\"\n",
    "u_norm = tf.reduce_sum(tf.abs(U, name='user_abs'), name='user_norm') \n",
    "d_norm = tf.reduce_sum(tf.abs(D, name='director_abs'), name='director_norm')\n",
    "t_norm = tf.reduce_sum(tf.abs(T, name='topic_abs'), name='topic_norm')\n",
    "g_norm = tf.reduce_sum(tf.abs(G, name='genre_abs'), name='genre_norm')\n",
    "c_norm = tf.reduce_sum(tf.abs(C, name='cast_abs'), name='cast_norm')\n",
    "\n",
    "norm_sums = u_norm + d_norm + t_norm + g_norm + c_norm\n",
    "regularizer = tf.multiply(norm_sums, lda, 'regularizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cost = tf.add(base_cost, regularizer)\n",
    "cost = base_cost + regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = tf.constant(.0001, name='learning_rate')\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = tf.train.exponential_decay(lr, global_step, 10000, 0.96, staircase=True)\n",
    "#learning_rate = 0.01\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "training_step = optimizer.minimize(cost, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(TR_R, total_train_rating))))\n",
    "rmse_test = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(TR_R_test, total_test_rating))))\n",
    "rmse_valid = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(TR_R_valid, total_valid_rating))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.0748e+06 15.4919 107479.75 3.67781 3.68724 3.6649\n",
      "1 734268.0 7.75559 73426.7625 3.03985 3.05784 3.03383\n",
      "2 562924.0 7.91042 56292.425 2.66165 2.67931 2.65975\n",
      "3 456221.0 8.08229 45622.103125 2.39615 2.41419 2.39721\n",
      "4 386394.0 8.2523 38639.384375 2.20516 2.22412 2.20845\n",
      "5 338155.0 8.41124 33815.5 2.06293 2.08322 2.06806\n",
      "6 302698.0 8.55579 30269.753125 1.95177 1.97372 1.95858\n",
      "7 275040.0 8.68585 27503.990625 1.86047 1.88433 1.86892\n",
      "8 252414.0 8.80271 25241.390625 1.7823 1.80821 1.79243\n",
      "9 233281.0 8.908 23328.0640625 1.71342 1.74144 1.72527\n",
      "10 216744.0 9.00328 21674.44375 1.65158 1.68172 1.66516\n",
      "11 202246.0 9.08994 20224.5578125 1.59538 1.62761 1.61075\n",
      "12 189424.0 9.16928 18942.4125 1.54398 1.57825 1.56113\n",
      "13 178017.0 9.2422 17801.709375 1.49677 1.53296 1.51569\n",
      "14 167816.0 9.30952 16781.625 1.45326 1.49123 1.47393\n",
      "15 158661.0 9.37191 15866.121875 1.41306 1.45269 1.43545\n",
      "16 150416.0 9.42993 15041.6 1.37585 1.41699 1.3999\n",
      "17 142961.0 9.48401 14296.09375 1.34132 1.38385 1.36696\n",
      "18 136200.0 9.53454 13619.975 1.30922 1.353 1.33637\n",
      "19 130056.0 9.58191 13005.5703125 1.27935 1.32426 1.30795\n",
      "20 124457.0 9.6264 12445.6765625 1.25151 1.29743 1.2815\n",
      "21 119336.0 9.66824 11933.634375 1.22549 1.27233 1.2568\n",
      "22 114643.0 9.70765 11464.25625 1.20115 1.24879 1.2337\n",
      "23 110328.0 9.74481 11032.7953125 1.17833 1.22669 1.21204\n",
      "24 106352.0 9.77991 10635.1640625 1.1569 1.2059 1.19172\n",
      "25 102682.0 9.81313 10268.2164062 1.13677 1.18632 1.17262\n",
      "26 99288.8 9.84463 9928.884375 1.11783 1.16786 1.15466\n",
      "27 96144.0 9.87452 9614.40390625 1.09998 1.15043 1.13773\n",
      "28 93224.9 9.90293 9322.49375 1.08316 1.13396 1.12178\n",
      "29 90511.2 9.92995 9051.1234375 1.06727 1.11839 1.10672\n",
      "30 87984.7 9.95569 8798.4703125 1.05227 1.10364 1.09249\n",
      "31 85629.0 9.98022 8562.9 1.03809 1.08967 1.07903\n",
      "32 83429.5 10.0037 8342.95390625 1.02467 1.07642 1.0663\n",
      "33 81373.5 10.0261 8137.353125 1.01197 1.06385 1.05423\n",
      "34 79450.3 10.0475 7945.03203125 0.999938 1.05192 1.0428\n",
      "35 77648.3 10.068 7764.8328125 0.988531 1.04059 1.03196\n",
      "36 75958.2 10.0877 7595.8171875 0.977715 1.02981 1.02167\n",
      "37 74371.3 10.1066 7437.128125 0.967448 1.01956 1.01189\n",
      "38 72879.5 10.1247 7287.9453125 0.957696 1.00981 1.0026\n",
      "39 71475.8 10.1422 7147.5765625 0.948428 1.00051 0.993764\n",
      "40 70154.1 10.1589 7015.40625 0.939618 0.991655 0.985357\n",
      "41 68908.5 10.175 6890.8484375 0.931239 0.983213 0.977354\n",
      "42 67733.5 10.1906 6773.3546875 0.923266 0.975163 0.969733\n",
      "43 66624.7 10.2056 6662.47265625 0.915677 0.967483 0.962473\n",
      "44 65577.4 10.22 6557.7359375 0.908452 0.960156 0.955554\n",
      "45 64587.2 10.234 6458.71640625 0.901567 0.953161 0.948955\n",
      "46 63650.4 10.2475 6365.0359375 0.895005 0.946481 0.942658\n",
      "47 62763.5 10.2605 6276.35078125 0.888748 0.940101 0.936648\n",
      "48 61923.2 10.2732 6192.31523437 0.882777 0.934001 0.930908\n",
      "49 61126.5 10.2854 6112.6546875 0.877081 0.928169 0.925427\n",
      "50 60370.9 10.2972 6037.08554687 0.871644 0.922589 0.92019\n",
      "51 59653.7 10.3087 5965.371875 0.86645 0.917252 0.915186\n",
      "52 58972.7 10.3198 5897.27148438 0.861491 0.912143 0.9104\n",
      "53 58325.3 10.3305 5832.53085937 0.856749 0.90725 0.905821\n",
      "54 57710.7 10.341 5771.0671875 0.852223 0.902571 0.901449\n",
      "55 57126.1 10.3512 5712.60976563 0.847895 0.898093 0.897265\n",
      "56 56569.7 10.3611 5656.965625 0.843756 0.893802 0.893259\n",
      "57 56039.6 10.3708 5603.9640625 0.839793 0.889688 0.889422\n",
      "58 55534.4 10.3801 5553.4390625 0.836 0.885741 0.885748\n",
      "59 55052.7 10.3892 5505.2703125 0.832366 0.881956 0.882225\n",
      "60 54593.3 10.3981 5459.33203125 0.828887 0.878329 0.878848\n",
      "61 54155.0 10.4067 5415.49804688 0.825551 0.874847 0.875608\n",
      "62 53736.1 10.4151 5373.61484375 0.822352 0.871504 0.872498\n",
      "63 53335.9 10.4233 5333.58515625 0.819284 0.868291 0.869512\n",
      "64 52953.2 10.4313 5295.3171875 0.816339 0.865203 0.866643\n",
      "65 52587.2 10.4391 5258.71835937 0.813514 0.862238 0.863886\n",
      "66 52237.0 10.4467 5223.69882812 0.8108 0.859387 0.86124\n",
      "67 51901.6 10.4541 5190.1609375 0.808193 0.856643 0.858694\n",
      "68 51580.3 10.4614 5158.0296875 0.805687 0.854004 0.856245\n",
      "69 51272.4 10.4685 5127.24296875 0.80328 0.851463 0.853891\n",
      "70 50977.2 10.4754 5097.725 0.800964 0.849017 0.851624\n",
      "71 50694.0 10.4822 5069.4046875 0.798737 0.84666 0.849441\n",
      "72 50422.2 10.4888 5042.221875 0.796591 0.844388 0.847339\n",
      "73 50161.4 10.4953 5016.13867188 0.794529 0.842199 0.845312\n",
      "74 49910.8 10.5016 4991.0828125 0.792541 0.840089 0.843358\n",
      "75 49670.0 10.5078 4967.003125 0.790628 0.838055 0.841474\n",
      "76 49438.5 10.5139 4943.8484375 0.788783 0.836094 0.839656\n",
      "77 49215.7 10.5198 4921.5703125 0.787004 0.834201 0.837903\n",
      "78 49001.2 10.5256 4900.12421875 0.785287 0.832374 0.83621\n",
      "79 48794.8 10.5313 4879.4796875 0.78363 0.83061 0.834577\n",
      "80 48595.9 10.5369 4859.59492188 0.782033 0.828907 0.832999\n",
      "81 48404.4 10.5424 4840.44179688 0.78049 0.827261 0.831475\n",
      "82 48219.7 10.5477 4821.97109375 0.779 0.825671 0.830002\n",
      "83 48041.7 10.553 4804.17226563 0.777561 0.824135 0.828579\n",
      "84 47869.9 10.5582 4786.99375 0.776169 0.82265 0.827203\n",
      "85 47704.2 10.5633 4770.41953125 0.774825 0.821213 0.825873\n",
      "86 47544.2 10.5682 4754.415625 0.773523 0.819824 0.824586\n",
      "87 47389.5 10.5731 4738.95078125 0.772264 0.81848 0.82334\n",
      "88 47240.2 10.5779 4724.021875 0.771047 0.817178 0.822134\n",
      "89 47095.8 10.5826 4709.58007812 0.769868 0.815919 0.820966\n",
      "90 46956.2 10.5873 4695.61953125 0.768725 0.814698 0.819835\n",
      "91 46821.1 10.5918 4682.10820312 0.767619 0.813516 0.818739\n",
      "92 46690.4 10.5963 4669.040625 0.766547 0.81237 0.817676\n",
      "93 46564.0 10.6007 4656.4 0.765508 0.81126 0.816646\n",
      "94 46441.7 10.6051 4644.16796875 0.764503 0.810184 0.815647\n",
      "95 46323.2 10.6093 4632.31953125 0.763527 0.80914 0.814677\n",
      "96 46208.4 10.6135 4620.8375 0.76258 0.808128 0.813736\n",
      "97 46097.1 10.6177 4609.7125 0.761661 0.807145 0.812823\n",
      "98 45989.3 10.6217 4598.92890625 0.76077 0.806192 0.811937\n",
      "99 45884.7 10.6257 4588.47304688 0.759904 0.805267 0.811077\n",
      "100 45783.3 10.6297 4578.3265625 0.759063 0.804368 0.810242\n",
      "101 45684.7 10.6336 4568.47265625 0.758246 0.803495 0.809431\n",
      "102 45589.1 10.6374 4558.90859375 0.757452 0.802647 0.808642\n",
      "103 45496.2 10.6412 4549.62148438 0.756681 0.801825 0.807876\n",
      "104 45406.0 10.6449 4540.60078125 0.75593 0.801025 0.807131\n",
      "105 45318.3 10.6486 4531.828125 0.7552 0.800248 0.806407\n",
      "106 45233.1 10.6522 4523.30625 0.754488 0.799492 0.805702\n",
      "107 45150.1 10.6558 4515.00859375 0.753797 0.798757 0.805016\n",
      "108 45069.4 10.6593 4506.9421875 0.753123 0.798041 0.804348\n",
      "109 44990.9 10.6628 4499.09453125 0.752467 0.797346 0.803699\n",
      "110 44914.6 10.6663 4491.46328125 0.751828 0.796668 0.803066\n",
      "111 44840.2 10.6696 4484.02421875 0.751205 0.796009 0.802451\n",
      "112 44767.7 10.673 4476.77421875 0.750598 0.795367 0.80185\n",
      "113 44697.2 10.6763 4469.71640625 0.750006 0.794741 0.801265\n",
      "114 44628.4 10.6796 4462.83867187 0.74943 0.794131 0.800695\n",
      "115 44561.4 10.6828 4456.13828125 0.748868 0.793536 0.80014\n",
      "116 44496.1 10.686 4449.60664063 0.748317 0.792957 0.799597\n",
      "117 44432.3 10.6891 4443.2296875 0.74778 0.792392 0.799069\n",
      "118 44370.1 10.6922 4437.00625 0.747257 0.791841 0.798553\n",
      "119 44309.4 10.6953 4430.94140625 0.746747 0.791304 0.79805\n",
      "120 44250.4 10.6983 4425.03515625 0.746248 0.790781 0.79756\n",
      "121 44192.6 10.7014 4419.26132812 0.745761 0.790271 0.797082\n",
      "122 44136.3 10.7043 4413.63164062 0.745285 0.789774 0.796615\n",
      "123 44081.2 10.7073 4408.125 0.74482 0.789288 0.796159\n",
      "124 44027.5 10.7102 4402.74726563 0.744367 0.788813 0.795713\n",
      "125 43974.9 10.7131 4397.49140625 0.743922 0.78835 0.795277\n",
      "126 43923.5 10.7159 4392.35273438 0.743486 0.787897 0.794851\n",
      "127 43873.2 10.7188 4387.32265625 0.743062 0.787454 0.794435\n",
      "128 43824.1 10.7215 4382.40546875 0.742645 0.787022 0.794029\n",
      "129 43776.1 10.7243 4377.60664063 0.742238 0.786599 0.793632\n",
      "130 43729.1 10.727 4372.90585937 0.74184 0.786185 0.793244\n",
      "131 43683.0 10.7298 4368.3015625 0.741448 0.785781 0.792864\n",
      "132 43637.9 10.7324 4363.78984375 0.741067 0.785385 0.792491\n",
      "133 43593.8 10.7351 4359.378125 0.740691 0.784998 0.792127\n",
      "134 43550.5 10.7377 4355.04570312 0.740322 0.784619 0.79177\n",
      "135 43508.0 10.7403 4350.80195313 0.739962 0.784247 0.791421\n",
      "136 43466.4 10.7429 4346.64375 0.739608 0.783884 0.791078\n",
      "137 43425.7 10.7454 4342.57148438 0.739262 0.783528 0.790743\n",
      "138 43385.7 10.748 4338.57382812 0.738922 0.783179 0.790414\n",
      "139 43346.6 10.7505 4334.65546875 0.738589 0.782838 0.790092\n",
      "140 43308.1 10.753 4330.80898438 0.73826 0.782503 0.789776\n",
      "141 43270.4 10.7554 4327.0375 0.737939 0.782175 0.789467\n",
      "142 43233.3 10.7579 4323.328125 0.737623 0.781853 0.789163\n",
      "143 43196.9 10.7603 4319.69023437 0.737311 0.781537 0.788865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144 43161.2 10.7627 4316.1203125 0.737007 0.781228 0.788573\n",
      "145 43126.1 10.765 4312.609375 0.736707 0.780924 0.788286\n",
      "146 43091.6 10.7674 4309.1640625 0.736413 0.780626 0.788004\n",
      "147 43057.8 10.7697 4305.775 0.736123 0.780334 0.787727\n",
      "148 43024.5 10.772 4302.453125 0.735839 0.780046 0.787456\n",
      "149 42991.8 10.7743 4299.1828125 0.73556 0.779765 0.787189\n",
      "150 42959.6 10.7766 4295.96484375 0.735284 0.779488 0.786927\n",
      "151 42928.1 10.7789 4292.80703125 0.735014 0.779216 0.78667\n",
      "152 42896.9 10.7811 4289.690625 0.734747 0.778949 0.786417\n",
      "153 42866.4 10.7833 4286.63515625 0.734487 0.778687 0.786169\n",
      "154 42836.3 10.7856 4283.62773437 0.734228 0.778429 0.785925\n",
      "155 42806.6 10.7877 4280.6625 0.733974 0.778176 0.785685\n",
      "156 42777.6 10.7899 4277.75625 0.733724 0.777927 0.785449\n",
      "157 42748.8 10.7921 4274.88164062 0.733477 0.777683 0.785217\n",
      "158 42720.6 10.7942 4272.05859375 0.733235 0.777442 0.784989\n",
      "159 42692.8 10.7963 4269.27890625 0.732996 0.777206 0.784765\n",
      "160 42665.4 10.7984 4266.53984375 0.732761 0.776973 0.784544\n",
      "161 42638.4 10.8005 4263.8421875 0.73253 0.776745 0.784327\n",
      "162 42611.9 10.8026 4261.1859375 0.732302 0.77652 0.784113\n",
      "163 42585.7 10.8047 4258.56601562 0.732077 0.776299 0.783903\n",
      "164 42559.9 10.8067 4255.99257813 0.731855 0.77608 0.783696\n",
      "165 42534.5 10.8087 4253.4484375 0.731636 0.775866 0.783492\n",
      "166 42509.4 10.8108 4250.9421875 0.731421 0.775655 0.783292\n",
      "167 42484.7 10.8127 4248.47421875 0.731208 0.775448 0.783095\n",
      "168 42460.4 10.8147 4246.03710938 0.731 0.775243 0.782899\n",
      "169 42436.4 10.8167 4243.6390625 0.730791 0.775042 0.782708\n",
      "170 42412.7 10.8187 4241.26953125 0.730589 0.774843 0.78252\n",
      "171 42389.4 10.8206 4238.9359375 0.730388 0.774649 0.782334\n",
      "172 42366.4 10.8226 4236.6375 0.73019 0.774456 0.782151\n",
      "173 42343.7 10.8245 4234.36796875 0.729993 0.774267 0.78197\n",
      "174 42321.3 10.8264 4232.13046875 0.729801 0.77408 0.781792\n",
      "175 42299.1 10.8283 4229.91328125 0.72961 0.773897 0.781617\n",
      "176 42277.3 10.8302 4227.73359375 0.729421 0.773716 0.781444\n",
      "177 42255.8 10.832 4225.5796875 0.729236 0.773538 0.781273\n",
      "178 42234.5 10.8339 4223.4546875 0.729052 0.773362 0.781105\n",
      "179 42213.6 10.8358 4221.35625 0.728871 0.773189 0.780939\n",
      "180 42192.9 10.8376 4219.2875 0.728692 0.773018 0.780776\n",
      "181 42172.4 10.8394 4217.24179687 0.728516 0.772849 0.780615\n",
      "182 42152.3 10.8412 4215.23046875 0.728342 0.772683 0.780456\n",
      "183 42132.3 10.843 4213.23476563 0.728169 0.772519 0.7803\n",
      "184 42112.6 10.8448 4211.25859375 0.727999 0.772358 0.780145\n",
      "185 42093.2 10.8466 4209.31640625 0.72783 0.772198 0.779993\n",
      "186 42073.9 10.8484 4207.39296875 0.727665 0.772041 0.779842\n",
      "187 42055.0 10.8502 4205.49765625 0.727501 0.771885 0.779693\n",
      "188 42036.2 10.8519 4203.61601562 0.727338 0.771732 0.779546\n",
      "189 42017.7 10.8536 4201.76679688 0.727178 0.771582 0.779401\n",
      "190 41999.3 10.8554 4199.93007813 0.727019 0.771432 0.779258\n",
      "191 41981.2 10.8571 4198.11640625 0.726862 0.771285 0.779118\n",
      "192 41963.3 10.8588 4196.33125 0.726707 0.77114 0.778978\n",
      "193 41945.6 10.8605 4194.5625 0.726555 0.770996 0.778841\n",
      "194 41928.1 10.8622 4192.80859375 0.726403 0.770854 0.778705\n",
      "195 41910.8 10.8639 4191.0828125 0.726252 0.770714 0.778571\n",
      "196 41893.7 10.8656 4189.3671875 0.726104 0.770576 0.778439\n",
      "197 41876.8 10.8672 4187.68125 0.725958 0.770439 0.778308\n",
      "198 41860.1 10.8689 4186.00625 0.725813 0.770305 0.778179\n",
      "199 41843.5 10.8705 4184.35078125 0.72567 0.770171 0.778051\n",
      "200 41827.2 10.8722 4182.71875 0.725528 0.77004 0.777925\n",
      "201 41810.9 10.8738 4181.09492188 0.725387 0.76991 0.7778\n",
      "202 41795.0 10.8754 4179.4953125 0.725248 0.769782 0.777677\n",
      "203 41779.1 10.877 4177.9078125 0.725111 0.769654 0.777556\n",
      "204 41763.5 10.8786 4176.3453125 0.724974 0.769529 0.777436\n",
      "205 41747.9 10.8802 4174.79414063 0.724839 0.769405 0.777317\n",
      "206 41732.6 10.8818 4173.26015625 0.724707 0.769283 0.777199\n",
      "207 41717.4 10.8834 4171.7390625 0.724575 0.769162 0.777083\n",
      "208 41702.3 10.8849 4170.23046875 0.724444 0.769042 0.776968\n",
      "209 41687.4 10.8865 4168.74375 0.724315 0.768923 0.776854\n",
      "210 41672.7 10.8881 4167.271875 0.724187 0.768806 0.776742\n",
      "211 41658.2 10.8896 4165.81757813 0.724061 0.768691 0.776631\n",
      "212 41643.8 10.8911 4164.38125 0.723936 0.768576 0.776522\n",
      "213 41629.5 10.8927 4162.9515625 0.723812 0.768463 0.776414\n",
      "214 41615.5 10.8942 4161.5453125 0.723689 0.768352 0.776307\n",
      "215 41601.5 10.8957 4160.15078125 0.723568 0.768241 0.776201\n",
      "216 41587.6 10.8972 4158.7625 0.723448 0.768132 0.776097\n",
      "217 41573.9 10.8987 4157.39335937 0.723328 0.768024 0.775993\n",
      "218 41560.4 10.9002 4156.03515625 0.72321 0.767916 0.77589\n",
      "219 41546.9 10.9017 4154.6921875 0.723093 0.76781 0.775789\n",
      "220 41533.6 10.9032 4153.359375 0.722978 0.767706 0.775689\n",
      "221 41520.5 10.9047 4152.046875 0.722863 0.767602 0.77559\n",
      "222 41507.4 10.9061 4150.74375 0.72275 0.767499 0.775492\n",
      "223 41494.5 10.9076 4149.4546875 0.722637 0.767398 0.775395\n",
      "224 41481.8 10.9091 4148.1796875 0.722526 0.767298 0.775299\n",
      "225 41469.2 10.9105 4146.915625 0.722416 0.767199 0.775204\n",
      "226 41456.5 10.9119 4145.65117187 0.722307 0.7671 0.77511\n",
      "227 41444.1 10.9134 4144.41328125 0.722197 0.767003 0.775017\n",
      "228 41431.8 10.9148 4143.184375 0.722091 0.766907 0.774925\n",
      "229 41419.6 10.9162 4141.959375 0.721985 0.766811 0.774834\n",
      "230 41407.5 10.9176 4140.753125 0.72188 0.766717 0.774744\n",
      "231 41395.6 10.919 4139.55898438 0.721774 0.766624 0.774655\n",
      "232 41383.7 10.9204 4138.36875 0.721671 0.766532 0.774566\n",
      "233 41371.9 10.9218 4137.19492187 0.721569 0.766441 0.774478\n",
      "234 41360.2 10.9232 4136.01875 0.721468 0.76635 0.774392\n",
      "235 41348.7 10.9246 4134.86953125 0.721366 0.76626 0.774306\n",
      "236 41337.2 10.926 4133.72304688 0.721266 0.766172 0.774221\n",
      "237 41325.9 10.9274 4132.58671875 0.721167 0.766084 0.774137\n",
      "238 41314.6 10.9287 4131.4578125 0.721068 0.765997 0.774053\n",
      "239 41303.4 10.9301 4130.34140625 0.720972 0.76591 0.77397\n",
      "240 41292.3 10.9314 4129.234375 0.720875 0.765825 0.773888\n",
      "241 41281.3 10.9328 4128.134375 0.720779 0.76574 0.773807\n",
      "242 41270.4 10.9341 4127.04296875 0.720683 0.765656 0.773726\n",
      "243 41259.6 10.9355 4125.96210938 0.720588 0.765573 0.773646\n",
      "244 41248.9 10.9368 4124.89140625 0.720495 0.765491 0.773567\n",
      "245 41238.3 10.9381 4123.83125 0.720402 0.765409 0.773489\n",
      "246 41227.7 10.9394 4122.77421875 0.720311 0.765328 0.773412\n",
      "247 41217.3 10.9407 4121.72851562 0.720219 0.765248 0.773335\n",
      "248 41206.9 10.9421 4120.68984375 0.720129 0.765169 0.773259\n",
      "249 41196.6 10.9434 4119.659375 0.720039 0.76509 0.773183\n",
      "250 41186.4 10.9446 4118.64257812 0.719949 0.765011 0.773108\n",
      "251 41176.3 10.9459 4117.62578125 0.719861 0.764934 0.773034\n",
      "252 41166.3 10.9472 4116.6265625 0.719773 0.764858 0.77296\n",
      "253 41156.2 10.9485 4115.625 0.719687 0.764782 0.772888\n",
      "254 41146.4 10.9498 4114.6375 0.7196 0.764706 0.772816\n",
      "255 41136.6 10.9511 4113.65976562 0.719514 0.764632 0.772744\n",
      "256 41126.8 10.9523 4112.68359375 0.719428 0.764558 0.772673\n",
      "257 41117.1 10.9536 4111.71445312 0.719344 0.764484 0.772602\n",
      "258 41107.7 10.9548 4110.76679688 0.719261 0.764412 0.772533\n",
      "259 41098.2 10.9561 4109.82265625 0.719178 0.764341 0.772464\n",
      "260 41088.8 10.9573 4108.878125 0.719095 0.76427 0.772396\n",
      "261 41079.5 10.9586 4107.95351563 0.719013 0.7642 0.772329\n",
      "262 41070.3 10.9598 4107.02617187 0.718933 0.76413 0.772262\n",
      "263 41061.1 10.9611 4106.10546875 0.718853 0.764062 0.772195\n",
      "264 41052.0 10.9623 4105.20039063 0.718773 0.763993 0.772129\n",
      "265 41043.0 10.9635 4104.29921875 0.718694 0.763926 0.772064\n",
      "266 41034.1 10.9647 4103.40585937 0.718616 0.763859 0.772\n",
      "267 41025.2 10.9659 4102.5171875 0.718539 0.763794 0.771936\n",
      "268 41016.4 10.9671 4101.6359375 0.718462 0.763728 0.771873\n",
      "269 41007.6 10.9684 4100.76171875 0.718384 0.763663 0.77181\n",
      "270 40998.9 10.9696 4099.89140625 0.718308 0.763598 0.771748\n",
      "271 40990.4 10.9707 4099.0359375 0.718232 0.763534 0.771686\n",
      "272 40981.8 10.9719 4098.1765625 0.718158 0.763471 0.771625\n",
      "273 40973.2 10.9731 4097.3234375 0.718084 0.763408 0.771564\n",
      "274 40964.8 10.9743 4096.4828125 0.71801 0.763345 0.771504\n",
      "275 40956.4 10.9755 4095.64101562 0.717935 0.763283 0.771444\n",
      "276 40948.1 10.9766 4094.81054688 0.717863 0.763222 0.771384\n",
      "277 40939.8 10.9778 4093.98203125 0.717791 0.763161 0.771325\n",
      "278 40931.7 10.979 4093.16796875 0.717719 0.7631 0.771267\n",
      "279 40923.5 10.9801 4092.35195312 0.717648 0.76304 0.771209\n",
      "280 40915.4 10.9813 4091.54140625 0.717576 0.76298 0.771151\n",
      "281 40907.4 10.9824 4090.73515625 0.717505 0.762921 0.771094\n",
      "282 40899.4 10.9836 4089.9375 0.717435 0.762863 0.771037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283 40891.4 10.9847 4089.14492187 0.717366 0.762804 0.770982\n",
      "284 40883.6 10.9859 4088.3609375 0.717298 0.762747 0.770926\n",
      "285 40875.8 10.987 4087.5828125 0.71723 0.76269 0.770871\n",
      "286 40868.1 10.9881 4086.80703125 0.717161 0.762633 0.770816\n",
      "287 40860.4 10.9893 4086.040625 0.717094 0.762576 0.770761\n",
      "288 40852.7 10.9904 4085.271875 0.717026 0.76252 0.770708\n",
      "289 40845.1 10.9915 4084.5109375 0.71696 0.762465 0.770654\n",
      "290 40837.6 10.9926 4083.759375 0.716894 0.76241 0.7706\n",
      "291 40830.1 10.9937 4083.00703125 0.716828 0.762355 0.770548\n",
      "292 40822.6 10.9948 4082.25859375 0.716763 0.762302 0.770496\n",
      "293 40815.2 10.9959 4081.52109375 0.716698 0.762248 0.770445\n",
      "294 40807.9 10.997 4080.78828125 0.716634 0.762196 0.770393\n",
      "295 40800.6 10.9981 4080.06171875 0.716569 0.762143 0.770342\n",
      "296 40793.4 10.9992 4079.33984375 0.716506 0.762091 0.770292\n",
      "297 40786.2 11.0003 4078.6203125 0.716442 0.762039 0.770242\n",
      "298 40779.0 11.0014 4077.903125 0.716379 0.761988 0.770192\n",
      "299 40771.9 11.0025 4077.19375 0.716317 0.761937 0.770143\n",
      "300 40765.0 11.0035 4076.4953125 0.716256 0.761886 0.770094\n",
      "301 40757.9 11.0046 4075.79375 0.716193 0.761837 0.770045\n",
      "302 40751.0 11.0057 4075.096875 0.716133 0.761787 0.769997\n",
      "303 40744.1 11.0067 4074.409375 0.716073 0.761738 0.769949\n",
      "304 40737.2 11.0078 4073.72226563 0.716012 0.761689 0.769902\n",
      "305 40730.4 11.0088 4073.0375 0.715953 0.76164 0.769854\n",
      "306 40723.7 11.0099 4072.36835938 0.715892 0.761592 0.769807\n",
      "307 40716.9 11.011 4071.69453125 0.715834 0.761544 0.76976\n",
      "308 40710.2 11.012 4071.02226562 0.715775 0.761497 0.769714\n",
      "309 40703.6 11.013 4070.359375 0.715717 0.761449 0.769668\n",
      "310 40697.1 11.0141 4069.70546875 0.715659 0.761403 0.769623\n",
      "311 40690.5 11.0151 4069.05078125 0.715602 0.761356 0.769578\n",
      "312 40684.0 11.0162 4068.40273437 0.715544 0.76131 0.769533\n",
      "313 40677.6 11.0172 4067.75820312 0.715487 0.761264 0.769489\n",
      "314 40671.2 11.0182 4067.11640625 0.715431 0.761219 0.769444\n",
      "315 40664.8 11.0192 4066.4828125 0.715374 0.761173 0.769401\n",
      "316 40658.5 11.0203 4065.85 0.71532 0.761129 0.769358\n",
      "317 40652.3 11.0213 4065.22578125 0.715265 0.761085 0.769315\n",
      "318 40646.1 11.0223 4064.60507813 0.71521 0.761041 0.769272\n",
      "319 40639.8 11.0233 4063.9796875 0.715156 0.760997 0.76923\n",
      "320 40633.7 11.0243 4063.36875 0.715103 0.760954 0.769188\n",
      "321 40627.6 11.0253 4062.75859375 0.715049 0.760911 0.769146\n",
      "322 40621.5 11.0263 4062.14921875 0.714995 0.760868 0.769105\n",
      "323 40615.5 11.0273 4061.55 0.714942 0.760825 0.769064\n",
      "324 40609.5 11.0283 4060.94609375 0.714889 0.760783 0.769023\n",
      "325 40603.5 11.0293 4060.35078125 0.714837 0.760742 0.768983\n",
      "326 40597.6 11.0303 4059.76171875 0.714784 0.7607 0.768942\n",
      "327 40591.7 11.0313 4059.17109375 0.714732 0.760659 0.768902\n",
      "328 40585.9 11.0323 4058.590625 0.714681 0.760619 0.768863\n",
      "329 40580.1 11.0332 4058.009375 0.71463 0.760578 0.768824\n",
      "330 40574.3 11.0342 4057.43242187 0.71458 0.760538 0.768784\n",
      "331 40568.6 11.0352 4056.8578125 0.714529 0.760498 0.768746\n",
      "332 40562.9 11.0362 4056.29023438 0.714479 0.760459 0.768707\n",
      "333 40557.2 11.0371 4055.72421875 0.714429 0.760419 0.768669\n",
      "334 40551.7 11.0381 4055.16523438 0.71438 0.76038 0.768631\n",
      "335 40546.0 11.039 4054.6 0.71433 0.760341 0.768593\n",
      "336 40540.5 11.04 4054.04609375 0.714281 0.760303 0.768556\n",
      "337 40534.9 11.041 4053.49414062 0.714233 0.760264 0.768519\n",
      "338 40529.4 11.0419 4052.94375 0.714183 0.760226 0.768482\n",
      "339 40523.9 11.0429 4052.39296875 0.714135 0.760188 0.768445\n",
      "340 40518.5 11.0438 4051.8484375 0.714088 0.760151 0.768409\n",
      "341 40513.1 11.0448 4051.31171875 0.71404 0.760113 0.768372\n",
      "342 40507.7 11.0457 4050.77148438 0.713992 0.760076 0.768336\n",
      "343 40502.4 11.0466 4050.23789063 0.713946 0.76004 0.768301\n",
      "344 40497.0 11.0476 4049.7046875 0.713899 0.760003 0.768265\n",
      "345 40491.8 11.0485 4049.17578125 0.713853 0.759966 0.76823\n",
      "346 40486.5 11.0494 4048.65078125 0.713806 0.75993 0.768195\n",
      "347 40481.2 11.0504 4048.1234375 0.71376 0.759894 0.76816\n",
      "348 40476.1 11.0513 4047.60546875 0.713714 0.759858 0.768125\n",
      "349 40470.8 11.0522 4047.084375 0.713668 0.759823 0.768091\n",
      "350 40465.7 11.0531 4046.57070312 0.713623 0.759788 0.768057\n",
      "351 40460.6 11.054 4046.0578125 0.713578 0.759753 0.768023\n",
      "352 40455.5 11.055 4045.553125 0.713533 0.759718 0.767989\n",
      "353 40450.5 11.0559 4045.04570312 0.713488 0.759683 0.767956\n",
      "354 40445.4 11.0568 4044.54296875 0.713443 0.759649 0.767922\n",
      "355 40440.4 11.0577 4044.04492188 0.713399 0.759615 0.767889\n",
      "356 40435.4 11.0586 4043.540625 0.713355 0.759581 0.767856\n",
      "357 40430.4 11.0595 4043.04453125 0.713312 0.759547 0.767824\n",
      "358 40425.5 11.0604 4042.55 0.713268 0.759514 0.767791\n",
      "359 40420.6 11.0613 4042.0625 0.713225 0.75948 0.767759\n",
      "360 40415.7 11.0621 4041.57265625 0.713182 0.759448 0.767727\n",
      "361 40410.9 11.063 4041.09140625 0.713139 0.759414 0.767695\n",
      "362 40406.1 11.0639 4040.60859375 0.713096 0.759382 0.767663\n",
      "363 40401.3 11.0648 4040.12578125 0.713054 0.759349 0.767632\n",
      "364 40396.5 11.0657 4039.64570313 0.713011 0.759317 0.7676\n",
      "365 40391.7 11.0666 4039.171875 0.71297 0.759285 0.767569\n",
      "366 40387.1 11.0674 4038.70625 0.712928 0.759253 0.767538\n",
      "367 40382.3 11.0683 4038.23164063 0.712886 0.759221 0.767508\n",
      "368 40377.6 11.0692 4037.76328125 0.712845 0.75919 0.767477\n",
      "369 40373.0 11.07 4037.2953125 0.712804 0.759159 0.767447\n",
      "370 40368.3 11.0709 4036.8296875 0.712762 0.759127 0.767416\n",
      "371 40363.8 11.0718 4036.37539062 0.712722 0.759097 0.767387\n",
      "372 40359.1 11.0726 4035.91328125 0.712682 0.759066 0.767357\n",
      "373 40354.6 11.0735 4035.45898438 0.712641 0.759035 0.767327\n",
      "374 40350.0 11.0743 4035.0046875 0.712601 0.759005 0.767298\n",
      "375 40345.5 11.0752 4034.55429688 0.712561 0.758975 0.767268\n",
      "376 40341.1 11.076 4034.10546875 0.712522 0.758945 0.76724\n",
      "377 40336.6 11.0769 4033.65742188 0.712482 0.758915 0.767211\n",
      "378 40332.1 11.0777 4033.2140625 0.712442 0.758885 0.767182\n",
      "379 40327.7 11.0786 4032.771875 0.712403 0.758856 0.767153\n",
      "380 40323.3 11.0794 4032.3328125 0.712364 0.758827 0.767125\n",
      "381 40319.0 11.0803 4031.89570313 0.712326 0.758797 0.767096\n",
      "382 40314.6 11.0811 4031.459375 0.712288 0.758768 0.767068\n",
      "383 40310.3 11.0819 4031.028125 0.712249 0.758739 0.76704\n",
      "384 40306.0 11.0828 4030.59609375 0.712212 0.758711 0.767012\n",
      "385 40301.7 11.0836 4030.16523438 0.712174 0.758682 0.766985\n",
      "386 40297.4 11.0844 4029.73984375 0.712135 0.758653 0.766957\n",
      "387 40293.1 11.0852 4029.31328125 0.712097 0.758625 0.76693\n",
      "388 40288.9 11.0861 4028.89296875 0.712061 0.758597 0.766902\n",
      "389 40284.7 11.0869 4028.4703125 0.712024 0.758569 0.766875\n",
      "390 40280.5 11.0877 4028.0484375 0.711987 0.758542 0.766849\n",
      "391 40276.3 11.0885 4027.6328125 0.711951 0.758514 0.766822\n",
      "392 40272.2 11.0893 4027.21953125 0.711914 0.758487 0.766795\n",
      "393 40268.1 11.0901 4026.80703125 0.711877 0.758459 0.766769\n",
      "394 40263.9 11.0909 4026.39453125 0.711841 0.758432 0.766742\n",
      "395 40259.9 11.0917 4025.98671875 0.711804 0.758405 0.766716\n",
      "396 40255.8 11.0925 4025.58125 0.711768 0.758378 0.76669\n",
      "397 40251.8 11.0933 4025.17734375 0.711733 0.758352 0.766664\n",
      "398 40247.7 11.0941 4024.7734375 0.711697 0.758325 0.766638\n",
      "399 40243.7 11.0949 4024.37460938 0.711662 0.758299 0.766613\n",
      "400 40239.8 11.0957 4023.97578125 0.711626 0.758273 0.766587\n",
      "401 40235.8 11.0965 4023.575 0.711591 0.758247 0.766562\n",
      "402 40231.8 11.0973 4023.184375 0.711557 0.758221 0.766537\n",
      "403 40227.9 11.0981 4022.78828125 0.711522 0.758195 0.766512\n",
      "404 40224.0 11.0989 4022.3984375 0.711488 0.758169 0.766487\n",
      "405 40220.1 11.0997 4022.00703125 0.711453 0.758144 0.766462\n",
      "406 40216.1 11.1004 4021.61484375 0.711419 0.758119 0.766438\n",
      "407 40212.3 11.1012 4021.2328125 0.711384 0.758094 0.766413\n",
      "408 40208.5 11.102 4020.84609375 0.71135 0.758068 0.766389\n",
      "409 40204.6 11.1028 4020.4625 0.711316 0.758044 0.766365\n",
      "410 40200.8 11.1035 4020.08007812 0.711282 0.758019 0.766341\n",
      "411 40197.0 11.1043 4019.70117188 0.711248 0.757994 0.766317\n",
      "412 40193.2 11.1051 4019.31953125 0.711215 0.75797 0.766293\n",
      "413 40189.5 11.1058 4018.9484375 0.711182 0.757945 0.766269\n",
      "414 40185.7 11.1066 4018.57148438 0.711149 0.757921 0.766246\n",
      "415 40182.0 11.1074 4018.203125 0.711116 0.757898 0.766223\n",
      "416 40178.3 11.1081 4017.83046875 0.711083 0.757873 0.766199\n",
      "417 40174.6 11.1089 4017.4640625 0.711051 0.75785 0.766176\n",
      "418 40170.9 11.1096 4017.09179688 0.711018 0.757826 0.766153\n",
      "419 40167.3 11.1104 4016.72734375 0.710986 0.757802 0.76613\n",
      "420 40163.6 11.1111 4016.36289063 0.710954 0.757779 0.766108\n",
      "421 40160.0 11.1119 4015.996875 0.710922 0.757756 0.766085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422 40156.4 11.1126 4015.63867188 0.71089 0.757733 0.766063\n",
      "423 40152.8 11.1134 4015.2765625 0.710857 0.75771 0.76604\n",
      "424 40149.2 11.1141 4014.921875 0.710826 0.757687 0.766018\n",
      "425 40145.6 11.1148 4014.5640625 0.710795 0.757664 0.765996\n",
      "426 40142.1 11.1156 4014.209375 0.710763 0.757641 0.765974\n",
      "427 40138.6 11.1163 4013.85507813 0.710731 0.757619 0.765952\n",
      "428 40135.0 11.1171 4013.503125 0.7107 0.757596 0.76593\n",
      "429 40131.5 11.1178 4013.1546875 0.71067 0.757574 0.765909\n",
      "430 40128.1 11.1185 4012.80546875 0.710639 0.757552 0.765887\n",
      "431 40124.6 11.1193 4012.46054688 0.710608 0.75753 0.765866\n",
      "432 40121.1 11.12 4012.11367187 0.710578 0.757508 0.765845\n",
      "433 40117.7 11.1207 4011.76992187 0.710547 0.757486 0.765824\n",
      "434 40114.2 11.1214 4011.42226562 0.710517 0.757465 0.765803\n",
      "435 40110.9 11.1222 4011.08515625 0.710486 0.757443 0.765782\n",
      "436 40107.5 11.1229 4010.74765625 0.710456 0.757421 0.765761\n",
      "437 40104.1 11.1236 4010.41015625 0.710426 0.7574 0.765741\n",
      "438 40100.7 11.1243 4010.07304687 0.710396 0.757379 0.76572\n",
      "439 40097.4 11.125 4009.740625 0.710367 0.757358 0.7657\n",
      "440 40094.1 11.1257 4009.40507812 0.710337 0.757337 0.76568\n",
      "441 40090.7 11.1264 4009.06875 0.710308 0.757316 0.765659\n",
      "442 40087.4 11.1272 4008.74453125 0.710278 0.757295 0.765639\n",
      "443 40084.1 11.1279 4008.40859375 0.710249 0.757274 0.765619\n",
      "444 40080.8 11.1286 4008.08398438 0.71022 0.757254 0.765599\n",
      "445 40077.6 11.1293 4007.7625 0.710191 0.757233 0.76558\n",
      "446 40074.3 11.13 4007.43125 0.710163 0.757213 0.76556\n",
      "447 40071.0 11.1307 4007.10390625 0.710134 0.757193 0.76554\n",
      "448 40067.9 11.1314 4006.78671875 0.710104 0.757172 0.765521\n",
      "449 40064.7 11.1321 4006.4671875 0.710077 0.757152 0.765502\n",
      "450 40061.4 11.1328 4006.14101562 0.710049 0.757133 0.765482\n",
      "451 40058.2 11.1335 4005.82460937 0.710019 0.757113 0.765463\n",
      "452 40055.0 11.1341 4005.50390625 0.709992 0.757093 0.765444\n",
      "453 40051.9 11.1348 4005.19453125 0.709964 0.757073 0.765425\n",
      "454 40048.8 11.1355 4004.88125 0.709936 0.757054 0.765406\n",
      "455 40045.6 11.1362 4004.5640625 0.709908 0.757034 0.765388\n",
      "456 40042.5 11.1369 4004.25390625 0.709881 0.757015 0.765369\n",
      "457 40039.4 11.1376 4003.94414062 0.709853 0.756996 0.76535\n",
      "458 40036.3 11.1382 4003.63359375 0.709826 0.756977 0.765332\n",
      "459 40033.2 11.1389 4003.32421875 0.709798 0.756958 0.765313\n",
      "460 40030.2 11.1396 4003.01640625 0.70977 0.756939 0.765295\n",
      "461 40027.1 11.1403 4002.70703125 0.709744 0.75692 0.765277\n",
      "462 40024.1 11.141 4002.409375 0.709717 0.756901 0.765259\n",
      "463 40021.0 11.1416 4002.103125 0.709691 0.756883 0.765241\n",
      "464 40018.0 11.1423 4001.80117187 0.709664 0.756865 0.765223\n",
      "465 40015.0 11.143 4001.50195312 0.709638 0.756846 0.765205\n",
      "466 40012.1 11.1436 4001.20585937 0.70961 0.756828 0.765187\n",
      "467 40009.0 11.1443 4000.9046875 0.709585 0.75681 0.76517\n",
      "468 40006.1 11.145 4000.60664063 0.709558 0.756792 0.765152\n",
      "469 40003.1 11.1456 4000.309375 0.709532 0.756774 0.765134\n",
      "470 40000.2 11.1463 4000.01835937 0.709507 0.756757 0.765117\n",
      "471 39997.3 11.147 3999.72539062 0.70948 0.756739 0.7651\n",
      "472 39994.3 11.1476 3999.43046875 0.709454 0.756721 0.765082\n",
      "473 39991.4 11.1483 3999.1390625 0.709428 0.756703 0.765065\n",
      "474 39988.5 11.1489 3998.85039062 0.709402 0.756686 0.765048\n",
      "475 39985.6 11.1496 3998.56289062 0.709377 0.756668 0.765032\n",
      "476 39982.8 11.1502 3998.27578125 0.709351 0.756651 0.765014\n",
      "477 39979.9 11.1509 3997.990625 0.709326 0.756634 0.764998\n",
      "478 39977.0 11.1515 3997.7046875 0.709301 0.756617 0.764981\n",
      "479 39974.2 11.1522 3997.42070312 0.709276 0.7566 0.764964\n",
      "480 39971.3 11.1528 3997.134375 0.709251 0.756583 0.764948\n",
      "481 39968.5 11.1535 3996.8546875 0.709225 0.756567 0.764932\n",
      "482 39965.8 11.1541 3996.584375 0.709201 0.75655 0.764916\n",
      "483 39963.0 11.1548 3996.30390625 0.709176 0.756534 0.7649\n",
      "484 39960.4 11.1554 3996.03515625 0.709152 0.756517 0.764884\n",
      "485 39957.6 11.1561 3995.75820312 0.709127 0.756501 0.764868\n",
      "486 39954.9 11.1567 3995.4890625 0.709103 0.756485 0.764853\n",
      "487 39952.1 11.1573 3995.21210938 0.70908 0.756468 0.764837\n",
      "488 39949.4 11.158 3994.94375 0.709056 0.756452 0.764821\n",
      "489 39946.7 11.1586 3994.67304688 0.709032 0.756436 0.764806\n",
      "490 39944.1 11.1592 3994.4078125 0.709008 0.756421 0.76479\n",
      "491 39941.3 11.1599 3994.13359375 0.708984 0.756405 0.764775\n",
      "492 39938.7 11.1605 3993.86875 0.708961 0.756389 0.76476\n",
      "493 39936.0 11.1611 3993.60195312 0.708937 0.756373 0.764745\n",
      "494 39933.4 11.1617 3993.33671875 0.708914 0.756358 0.76473\n",
      "495 39930.7 11.1624 3993.07109375 0.70889 0.756342 0.764715\n",
      "496 39928.1 11.163 3992.80859375 0.708867 0.756327 0.7647\n",
      "497 39925.5 11.1636 3992.54765625 0.708843 0.756311 0.764685\n",
      "498 39922.9 11.1642 3992.2875 0.70882 0.756296 0.76467\n",
      "499 39920.3 11.1649 3992.02617187 0.708797 0.756281 0.764655\n",
      "500 39917.7 11.1655 3991.76640625 0.708774 0.756266 0.764641\n",
      "501 39915.1 11.1661 3991.50976562 0.708751 0.756251 0.764626\n",
      "502 39912.5 11.1667 3991.24921875 0.708728 0.756235 0.764612\n",
      "503 39910.0 11.1673 3990.99960938 0.708705 0.756221 0.764597\n",
      "504 39907.4 11.1679 3990.740625 0.708682 0.756206 0.764583\n",
      "505 39904.9 11.1686 3990.4890625 0.70866 0.756191 0.764569\n",
      "506 39902.4 11.1692 3990.23515625 0.708637 0.756176 0.764554\n",
      "507 39899.8 11.1698 3989.978125 0.708615 0.756161 0.76454\n",
      "508 39897.2 11.1704 3989.72421875 0.708593 0.756147 0.764526\n",
      "509 39894.8 11.171 3989.475 0.70857 0.756132 0.764512\n",
      "510 39892.3 11.1716 3989.22578125 0.708547 0.756118 0.764498\n",
      "511 39889.8 11.1722 3988.97539062 0.708526 0.756104 0.764484\n",
      "512 39887.3 11.1728 3988.72890625 0.708503 0.756089 0.76447\n",
      "513 39884.8 11.1734 3988.4828125 0.708482 0.756075 0.764457\n",
      "514 39882.4 11.174 3988.23515625 0.70846 0.756061 0.764443\n",
      "515 39879.9 11.1746 3987.9890625 0.708438 0.756047 0.76443\n",
      "516 39877.5 11.1752 3987.74570313 0.708417 0.756033 0.764416\n",
      "517 39875.0 11.1758 3987.50039062 0.708395 0.756019 0.764403\n",
      "518 39872.6 11.1764 3987.2578125 0.708373 0.756005 0.764389\n",
      "519 39870.2 11.177 3987.01679688 0.708351 0.755991 0.764376\n",
      "520 39867.7 11.1776 3986.77109375 0.708331 0.755977 0.764362\n",
      "521 39865.3 11.1782 3986.5296875 0.708309 0.755964 0.764349\n",
      "522 39862.9 11.1788 3986.29140625 0.708288 0.75595 0.764336\n",
      "523 39860.5 11.1794 3986.05039063 0.708265 0.755936 0.764323\n",
      "524 39858.1 11.1799 3985.81367188 0.708244 0.755923 0.76431\n",
      "525 39855.7 11.1805 3985.57460937 0.708224 0.755909 0.764297\n",
      "526 39853.4 11.1811 3985.33828125 0.708202 0.755896 0.764284\n",
      "527 39851.0 11.1817 3985.09804687 0.708181 0.755883 0.764271\n",
      "528 39848.6 11.1823 3984.86328125 0.708161 0.755869 0.764258\n",
      "529 39846.3 11.1829 3984.63125 0.708141 0.755856 0.764246\n",
      "530 39844.0 11.1834 3984.3984375 0.708119 0.755843 0.764233\n",
      "531 39841.6 11.184 3984.1609375 0.708099 0.75583 0.764221\n",
      "532 39839.3 11.1846 3983.92695313 0.708078 0.755817 0.764208\n",
      "533 39837.0 11.1852 3983.6984375 0.708057 0.755804 0.764196\n",
      "534 39834.7 11.1857 3983.46640625 0.708037 0.755791 0.764183\n",
      "535 39832.4 11.1863 3983.2375 0.708017 0.755779 0.764171\n",
      "536 39830.1 11.1869 3983.00625 0.707996 0.755765 0.764158\n",
      "537 39827.8 11.1874 3982.78007812 0.707976 0.755753 0.764146\n",
      "538 39825.5 11.188 3982.55390625 0.707956 0.75574 0.764133\n",
      "539 39823.3 11.1886 3982.32578125 0.707935 0.755728 0.764121\n",
      "540 39821.0 11.1892 3982.09921875 0.707915 0.755715 0.76411\n",
      "541 39818.8 11.1897 3981.87617188 0.707895 0.755703 0.764097\n",
      "542 39816.5 11.1903 3981.65 0.707874 0.75569 0.764085\n",
      "543 39814.2 11.1909 3981.42265625 0.707855 0.755678 0.764074\n",
      "544 39812.0 11.1914 3981.20117188 0.707834 0.755665 0.764062\n",
      "545 39809.8 11.192 3980.98046875 0.707815 0.755654 0.76405\n",
      "546 39807.6 11.1925 3980.75976562 0.707795 0.755641 0.764039\n",
      "547 39805.4 11.1931 3980.54140625 0.707775 0.755629 0.764027\n",
      "548 39803.2 11.1937 3980.31953125 0.707756 0.755617 0.764016\n",
      "549 39801.0 11.1942 3980.09960938 0.707737 0.755605 0.764004\n",
      "550 39798.8 11.1948 3979.88320312 0.707717 0.755593 0.763993\n",
      "551 39796.6 11.1953 3979.66289062 0.707698 0.755581 0.763981\n",
      "552 39794.5 11.1959 3979.4453125 0.707679 0.75557 0.76397\n",
      "553 39792.3 11.1964 3979.23203125 0.70766 0.755558 0.763959\n",
      "554 39790.2 11.197 3979.015625 0.707641 0.755546 0.763948\n",
      "555 39788.0 11.1975 3978.8 0.707622 0.755535 0.763937\n",
      "556 39785.9 11.1981 3978.58515625 0.707602 0.755524 0.763926\n",
      "557 39783.8 11.1986 3978.37695312 0.707584 0.755512 0.763915\n",
      "558 39781.6 11.1992 3978.16171875 0.707564 0.755501 0.763904\n",
      "559 39779.5 11.1997 3977.94960937 0.707546 0.75549 0.763893\n",
      "560 39777.4 11.2003 3977.73710937 0.707527 0.755478 0.763882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "561 39775.3 11.2008 3977.5265625 0.707508 0.755467 0.763871\n",
      "562 39773.2 11.2014 3977.31875 0.707489 0.755456 0.76386\n",
      "563 39771.1 11.2019 3977.109375 0.707471 0.755445 0.763848\n",
      "564 39769.0 11.2024 3976.90078125 0.707452 0.755434 0.763838\n",
      "565 39766.9 11.203 3976.6890625 0.707434 0.755422 0.763826\n",
      "566 39764.9 11.2035 3976.48671875 0.707415 0.755412 0.763816\n",
      "567 39762.8 11.2041 3976.27773437 0.707397 0.755401 0.763805\n",
      "568 39760.7 11.2046 3976.07070312 0.707379 0.75539 0.763795\n",
      "569 39758.7 11.2051 3975.86953125 0.707361 0.755379 0.763784\n",
      "570 39756.6 11.2057 3975.6609375 0.707342 0.755368 0.763773\n",
      "571 39754.5 11.2062 3975.453125 0.707324 0.755358 0.763763\n",
      "572 39752.5 11.2067 3975.2515625 0.707305 0.755347 0.763752\n",
      "573 39750.5 11.2073 3975.05078125 0.707288 0.755336 0.763742\n",
      "574 39748.5 11.2078 3974.85078125 0.707269 0.755326 0.763731\n",
      "575 39746.4 11.2083 3974.64492187 0.707252 0.755315 0.763721\n",
      "576 39744.4 11.2088 3974.44257813 0.707233 0.755305 0.763711\n",
      "577 39742.4 11.2094 3974.24296875 0.707216 0.755294 0.763701\n",
      "578 39740.4 11.2099 3974.04335937 0.707198 0.755284 0.763691\n",
      "579 39738.5 11.2104 3973.846875 0.707181 0.755273 0.763681\n",
      "580 39736.5 11.211 3973.64960938 0.707163 0.755263 0.763671\n",
      "581 39734.5 11.2115 3973.45078125 0.707145 0.755253 0.763661\n",
      "582 39732.5 11.212 3973.25429687 0.707128 0.755243 0.763651\n",
      "583 39730.6 11.2125 3973.05625 0.70711 0.755233 0.763641\n",
      "584 39728.6 11.213 3972.85859375 0.707093 0.755223 0.763631\n",
      "585 39726.6 11.2136 3972.6625 0.707075 0.755213 0.763621\n",
      "586 39724.7 11.2141 3972.47421875 0.707058 0.755203 0.763611\n",
      "587 39722.8 11.2146 3972.278125 0.707041 0.755193 0.763602\n",
      "588 39720.9 11.2151 3972.0859375 0.707024 0.755183 0.763592\n",
      "589 39718.9 11.2156 3971.89296875 0.707007 0.755174 0.763583\n",
      "590 39717.0 11.2162 3971.70234375 0.70699 0.755164 0.763573\n",
      "591 39715.1 11.2167 3971.51015625 0.706973 0.755154 0.763564\n",
      "592 39713.2 11.2172 3971.3203125 0.706956 0.755145 0.763555\n",
      "593 39711.3 11.2177 3971.13046875 0.706939 0.755135 0.763547\n",
      "594 39709.4 11.2182 3970.940625 0.706922 0.755126 0.763538\n",
      "595 39707.5 11.2187 3970.7515625 0.706905 0.755116 0.763529\n",
      "596 39705.6 11.2192 3970.56210938 0.706888 0.755107 0.76352\n",
      "597 39703.7 11.2197 3970.37265625 0.706872 0.755097 0.763511\n",
      "598 39701.9 11.2203 3970.1859375 0.706855 0.755088 0.763502\n",
      "599 39700.0 11.2208 3970.0 0.706839 0.755078 0.763493\n",
      "600 39698.2 11.2213 3969.81601563 0.706822 0.755069 0.763485\n",
      "601 39696.3 11.2218 3969.62578125 0.706806 0.75506 0.763476\n",
      "602 39694.4 11.2223 3969.44179687 0.706788 0.75505 0.763467\n",
      "603 39692.5 11.2228 3969.2546875 0.706772 0.755042 0.763458\n",
      "604 39690.7 11.2233 3969.06835938 0.706756 0.755032 0.76345\n",
      "605 39688.8 11.2238 3968.88242188 0.70674 0.755023 0.763441\n",
      "606 39687.0 11.2243 3968.703125 0.706722 0.755014 0.763432\n",
      "607 39685.2 11.2248 3968.52148438 0.706707 0.755005 0.763423\n",
      "608 39683.4 11.2253 3968.3359375 0.70669 0.754996 0.763414\n",
      "609 39681.6 11.2258 3968.15664062 0.706674 0.754987 0.763406\n",
      "610 39679.7 11.2263 3967.9734375 0.706658 0.754978 0.763397\n",
      "611 39677.9 11.2268 3967.79296875 0.706642 0.754968 0.763389\n",
      "612 39676.1 11.2273 3967.6125 0.706626 0.75496 0.76338\n",
      "613 39674.4 11.2278 3967.43671875 0.70661 0.754951 0.763372\n",
      "614 39672.6 11.2283 3967.25546875 0.706594 0.754942 0.763363\n",
      "615 39670.8 11.2288 3967.07851562 0.706578 0.754933 0.763355\n",
      "616 39669.0 11.2293 3966.90078125 0.706563 0.754925 0.763346\n",
      "617 39667.2 11.2298 3966.7234375 0.706547 0.754916 0.763338\n",
      "618 39665.4 11.2302 3966.54453125 0.706531 0.754907 0.76333\n",
      "619 39663.7 11.2307 3966.36914062 0.706515 0.754898 0.763321\n",
      "620 39661.9 11.2312 3966.18945312 0.706498 0.75489 0.763313\n",
      "621 39660.1 11.2317 3966.01484375 0.706483 0.754881 0.763305\n",
      "622 39658.4 11.2322 3965.83710938 0.706467 0.754872 0.763296\n",
      "623 39656.6 11.2327 3965.66171875 0.706451 0.754864 0.763288\n",
      "624 39654.9 11.2332 3965.490625 0.706436 0.754856 0.76328\n",
      "625 39653.1 11.2337 3965.31445312 0.706421 0.754847 0.763272\n",
      "626 39651.4 11.2341 3965.140625 0.706406 0.754839 0.763264\n",
      "627 39649.7 11.2346 3964.9671875 0.706391 0.754831 0.763256\n",
      "628 39647.9 11.2351 3964.79492188 0.706374 0.754822 0.763247\n",
      "629 39646.2 11.2356 3964.6203125 0.706359 0.754814 0.76324\n",
      "630 39644.5 11.2361 3964.44648438 0.706344 0.754805 0.763232\n",
      "631 39642.7 11.2366 3964.2734375 0.706328 0.754797 0.763223\n",
      "632 39641.0 11.237 3964.1015625 0.706313 0.754789 0.763215\n",
      "633 39639.3 11.2375 3963.93242187 0.706298 0.754781 0.763208\n",
      "634 39637.6 11.238 3963.759375 0.706283 0.754773 0.7632\n",
      "635 39635.9 11.2385 3963.59257812 0.706268 0.754765 0.763192\n",
      "636 39634.2 11.2389 3963.42265625 0.706253 0.754757 0.763184\n",
      "637 39632.5 11.2394 3963.25078125 0.706237 0.754749 0.763176\n",
      "638 39630.8 11.2399 3963.08242187 0.706222 0.754741 0.763168\n",
      "639 39629.1 11.2404 3962.91210938 0.706208 0.754732 0.763161\n",
      "640 39627.4 11.2409 3962.74375 0.706193 0.754725 0.763152\n",
      "641 39625.8 11.2413 3962.57617188 0.706177 0.754717 0.763145\n",
      "642 39624.1 11.2418 3962.41015625 0.706162 0.754709 0.763137\n",
      "643 39622.4 11.2423 3962.2421875 0.706147 0.754701 0.763129\n",
      "644 39620.8 11.2427 3962.07890625 0.706133 0.754694 0.763122\n",
      "645 39619.1 11.2432 3961.91328125 0.706117 0.754686 0.763114\n",
      "646 39617.5 11.2437 3961.74960938 0.706103 0.754678 0.763106\n",
      "647 39615.8 11.2441 3961.5796875 0.706089 0.75467 0.763099\n",
      "648 39614.1 11.2446 3961.41328125 0.706074 0.754663 0.763092\n",
      "649 39612.5 11.2451 3961.24609375 0.706058 0.754655 0.763084\n",
      "650 39610.9 11.2455 3961.08515625 0.706044 0.754647 0.763076\n",
      "651 39609.2 11.246 3960.91875 0.706029 0.75464 0.763068\n",
      "652 39607.6 11.2465 3960.7578125 0.706015 0.754632 0.763061\n",
      "653 39605.9 11.2469 3960.59492188 0.706 0.754625 0.763054\n",
      "654 39604.3 11.2474 3960.4328125 0.705986 0.754617 0.763047\n",
      "655 39602.7 11.2479 3960.27148438 0.705971 0.75461 0.763039\n",
      "656 39601.1 11.2483 3960.10976562 0.705957 0.754602 0.763032\n",
      "657 39599.5 11.2488 3959.946875 0.705942 0.754595 0.763024\n",
      "658 39597.8 11.2492 3959.78242188 0.705927 0.754587 0.763017\n",
      "659 39596.2 11.2497 3959.62265625 0.705913 0.75458 0.76301\n",
      "660 39594.6 11.2502 3959.46328125 0.705899 0.754573 0.763002\n",
      "661 39593.0 11.2506 3959.30390625 0.705884 0.754566 0.762995\n",
      "662 39591.4 11.2511 3959.14453125 0.70587 0.754558 0.762988\n",
      "663 39589.9 11.2515 3958.98710937 0.705856 0.754551 0.762981\n",
      "664 39588.3 11.252 3958.82578125 0.705842 0.754544 0.762974\n",
      "665 39586.6 11.2524 3958.66484375 0.705828 0.754537 0.762967\n",
      "666 39585.1 11.2529 3958.51171875 0.705814 0.754529 0.76296\n",
      "667 39583.5 11.2533 3958.3546875 0.7058 0.754522 0.762953\n",
      "668 39582.0 11.2538 3958.19609375 0.705785 0.754515 0.762945\n",
      "669 39580.4 11.2543 3958.04375 0.705772 0.754508 0.762938\n",
      "670 39578.8 11.2547 3957.88125 0.705758 0.754501 0.762932\n",
      "671 39577.3 11.2551 3957.72890625 0.705744 0.754493 0.762925\n",
      "672 39575.7 11.2556 3957.57265625 0.705731 0.754487 0.762918\n",
      "673 39574.2 11.256 3957.41679687 0.705717 0.754479 0.762911\n",
      "674 39572.6 11.2565 3957.25859375 0.705703 0.754472 0.762904\n",
      "675 39571.0 11.2569 3957.1046875 0.70569 0.754465 0.762897\n",
      "676 39569.5 11.2574 3956.9515625 0.705676 0.754459 0.76289\n",
      "677 39568.0 11.2578 3956.7953125 0.705662 0.754452 0.762883\n",
      "678 39566.4 11.2583 3956.64179688 0.705648 0.754445 0.762876\n",
      "679 39564.9 11.2587 3956.49023438 0.705634 0.754438 0.76287\n",
      "680 39563.4 11.2592 3956.3390625 0.70562 0.754431 0.762863\n",
      "681 39561.9 11.2596 3956.1859375 0.705608 0.754424 0.762856\n",
      "682 39560.4 11.2601 3956.03554687 0.705594 0.754417 0.76285\n",
      "683 39558.8 11.2605 3955.884375 0.70558 0.754411 0.762843\n",
      "684 39557.3 11.261 3955.73046875 0.705566 0.754404 0.762836\n",
      "685 39555.8 11.2614 3955.58046875 0.705552 0.754397 0.76283\n",
      "686 39554.3 11.2618 3955.4296875 0.705539 0.754391 0.762823\n",
      "687 39552.8 11.2623 3955.27890625 0.705526 0.754384 0.762817\n",
      "688 39551.2 11.2627 3955.12421875 0.705513 0.754377 0.76281\n",
      "689 39549.7 11.2631 3954.97460938 0.705499 0.754371 0.762804\n",
      "690 39548.2 11.2636 3954.82265625 0.705486 0.754364 0.762797\n",
      "691 39546.7 11.264 3954.67421875 0.705472 0.754358 0.762791\n",
      "692 39545.3 11.2645 3954.52578125 0.70546 0.754351 0.762784\n",
      "693 39543.8 11.2649 3954.37734375 0.705446 0.754345 0.762778\n",
      "694 39542.3 11.2653 3954.2296875 0.705433 0.754339 0.762771\n",
      "695 39540.8 11.2658 3954.0828125 0.70542 0.754332 0.762765\n",
      "696 39539.3 11.2662 3953.93398437 0.705406 0.754326 0.762759\n",
      "697 39537.9 11.2666 3953.78515625 0.705394 0.754319 0.762753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "698 39536.4 11.2671 3953.63867188 0.705381 0.754313 0.762746\n",
      "699 39534.9 11.2675 3953.49140625 0.705367 0.754307 0.76274\n",
      "700 39533.4 11.2679 3953.34453125 0.705354 0.754301 0.762734\n",
      "701 39532.0 11.2684 3953.2 0.705341 0.754294 0.762728\n",
      "702 39530.5 11.2688 3953.05234375 0.705328 0.754288 0.762721\n",
      "703 39529.0 11.2692 3952.90078125 0.705315 0.754282 0.762715\n",
      "704 39527.6 11.2696 3952.75585938 0.705302 0.754276 0.762709\n",
      "705 39526.1 11.2701 3952.6109375 0.705289 0.75427 0.762703\n",
      "706 39524.7 11.2705 3952.46875 0.705276 0.754263 0.762697\n",
      "707 39523.2 11.2709 3952.32148438 0.705263 0.754258 0.762691\n",
      "708 39521.8 11.2714 3952.1796875 0.705251 0.754251 0.762685\n",
      "709 39520.3 11.2718 3952.03476563 0.705237 0.754245 0.762679\n",
      "710 39518.9 11.2722 3951.8890625 0.705224 0.75424 0.762673\n",
      "711 39517.5 11.2726 3951.74765625 0.705212 0.754233 0.762667\n",
      "712 39516.1 11.2731 3951.60546875 0.705199 0.754227 0.762661\n",
      "713 39514.6 11.2735 3951.46132812 0.705186 0.754222 0.762655\n",
      "714 39513.2 11.2739 3951.32109375 0.705173 0.754215 0.762649\n",
      "715 39511.8 11.2743 3951.17578125 0.70516 0.75421 0.762643\n",
      "716 39510.3 11.2748 3951.03203125 0.705148 0.754204 0.762637\n",
      "717 39508.9 11.2752 3950.89023438 0.705135 0.754198 0.762631\n",
      "718 39507.5 11.2756 3950.75 0.705123 0.754192 0.762625\n",
      "719 39506.1 11.276 3950.60898438 0.70511 0.754186 0.76262\n",
      "720 39504.7 11.2764 3950.46601563 0.705097 0.75418 0.762614\n",
      "721 39503.2 11.2768 3950.32265625 0.705084 0.754175 0.762608\n",
      "722 39501.8 11.2773 3950.184375 0.705072 0.754169 0.762603\n",
      "723 39500.4 11.2777 3950.04296875 0.705059 0.754163 0.762597\n",
      "724 39499.0 11.2781 3949.903125 0.705047 0.754157 0.762591\n",
      "725 39497.6 11.2785 3949.76289063 0.705035 0.754151 0.762586\n",
      "726 39496.2 11.2789 3949.625 0.705022 0.754146 0.76258\n",
      "727 39494.9 11.2793 3949.48515625 0.70501 0.75414 0.762575\n",
      "728 39493.5 11.2798 3949.346875 0.704997 0.754134 0.762569\n",
      "729 39492.1 11.2802 3949.209375 0.704985 0.754129 0.762563\n",
      "730 39490.7 11.2806 3949.071875 0.704972 0.754123 0.762558\n",
      "731 39489.3 11.281 3948.93359375 0.70496 0.754117 0.762552\n",
      "732 39487.9 11.2814 3948.790625 0.704946 0.754111 0.762547\n",
      "733 39486.5 11.2818 3948.65390625 0.704935 0.754106 0.762541\n",
      "734 39485.2 11.2822 3948.5171875 0.704923 0.7541 0.762536\n",
      "735 39483.8 11.2827 3948.38046875 0.704911 0.754094 0.76253\n",
      "736 39482.4 11.2831 3948.24375 0.704898 0.754089 0.762525\n",
      "737 39481.1 11.2835 3948.1078125 0.704887 0.754084 0.762519\n",
      "738 39479.7 11.2839 3947.96796875 0.704875 0.754078 0.762514\n",
      "739 39478.3 11.2843 3947.8328125 0.704862 0.754072 0.762509\n",
      "740 39477.0 11.2847 3947.696875 0.70485 0.754067 0.762503\n",
      "741 39475.6 11.2851 3947.56445312 0.704838 0.754061 0.762498\n",
      "742 39474.3 11.2855 3947.42929688 0.704826 0.754056 0.762493\n",
      "743 39472.9 11.2859 3947.29414063 0.704814 0.754051 0.762487\n",
      "744 39471.5 11.2863 3947.1546875 0.704802 0.754045 0.762482\n",
      "745 39470.2 11.2867 3947.01875 0.70479 0.75404 0.762477\n",
      "746 39468.9 11.2871 3946.88828125 0.704778 0.754035 0.762472\n",
      "747 39467.6 11.2875 3946.75625 0.704766 0.754029 0.762466\n",
      "748 39466.2 11.2879 3946.62109375 0.704754 0.754024 0.762461\n",
      "749 39464.9 11.2883 3946.48515625 0.704742 0.754019 0.762456\n",
      "750 39463.5 11.2887 3946.3546875 0.704731 0.754014 0.762451\n",
      "751 39462.2 11.2892 3946.22265625 0.704718 0.754008 0.762446\n",
      "752 39460.9 11.2896 3946.08671875 0.704706 0.754003 0.762441\n",
      "753 39459.5 11.29 3945.95234375 0.704695 0.753998 0.762436\n",
      "754 39458.2 11.2904 3945.8203125 0.704682 0.753993 0.762431\n",
      "755 39456.9 11.2908 3945.69296875 0.704671 0.753988 0.762425\n",
      "756 39455.6 11.2912 3945.559375 0.704659 0.753983 0.76242\n",
      "757 39454.3 11.2916 3945.428125 0.704648 0.753978 0.762416\n",
      "758 39453.0 11.292 3945.29609375 0.704636 0.753973 0.762411\n",
      "759 39451.7 11.2923 3945.16640625 0.704624 0.753968 0.762405\n",
      "760 39450.4 11.2927 3945.0359375 0.704612 0.753963 0.762401\n",
      "761 39449.0 11.2931 3944.90351563 0.704601 0.753958 0.762396\n",
      "762 39447.8 11.2935 3944.77734375 0.704589 0.753953 0.762391\n",
      "763 39446.4 11.2939 3944.64453125 0.704578 0.753948 0.762386\n",
      "764 39445.1 11.2943 3944.5109375 0.704567 0.753943 0.762381\n",
      "765 39443.8 11.2947 3944.384375 0.704555 0.753938 0.762376\n",
      "766 39442.6 11.2951 3944.26015625 0.704543 0.753933 0.762372\n",
      "767 39441.3 11.2955 3944.13203125 0.704531 0.753929 0.762367\n",
      "768 39440.0 11.2959 3944.00039062 0.704519 0.753924 0.762362\n",
      "769 39438.7 11.2963 3943.8734375 0.704508 0.753919 0.762357\n",
      "770 39437.4 11.2967 3943.74414062 0.704497 0.753914 0.762353\n",
      "771 39436.2 11.2971 3943.61640625 0.704486 0.75391 0.762348\n",
      "772 39434.9 11.2975 3943.49023438 0.704474 0.753905 0.762343\n",
      "773 39433.6 11.2979 3943.35859375 0.704463 0.7539 0.762339\n",
      "774 39432.3 11.2983 3943.23125 0.704451 0.753895 0.762334\n",
      "775 39431.1 11.2987 3943.11015625 0.704439 0.753891 0.76233\n",
      "776 39429.8 11.2991 3942.97929687 0.704428 0.753886 0.762325\n",
      "777 39428.5 11.2994 3942.853125 0.704417 0.753882 0.762321\n",
      "778 39427.3 11.2998 3942.72773438 0.704406 0.753877 0.762316\n",
      "779 39426.0 11.3002 3942.60039062 0.704395 0.753872 0.762312\n",
      "780 39424.7 11.3006 3942.4734375 0.704383 0.753868 0.762307\n",
      "781 39423.5 11.301 3942.34765625 0.704372 0.753864 0.762303\n",
      "782 39422.2 11.3014 3942.22304687 0.704361 0.753859 0.762298\n",
      "783 39420.9 11.3018 3942.0921875 0.704349 0.753854 0.762294\n",
      "784 39419.7 11.3022 3941.96953125 0.704339 0.75385 0.762289\n",
      "785 39418.5 11.3025 3941.8453125 0.704327 0.753845 0.762285\n",
      "786 39417.2 11.3029 3941.721875 0.704317 0.753841 0.762281\n",
      "787 39416.0 11.3033 3941.59765625 0.704306 0.753836 0.762277\n",
      "788 39414.7 11.3037 3941.47421875 0.704294 0.753832 0.762272\n",
      "789 39413.5 11.3041 3941.3515625 0.704283 0.753827 0.762268\n",
      "790 39412.3 11.3045 3941.22617188 0.704272 0.753823 0.762264\n",
      "791 39411.0 11.3049 3941.10078125 0.704261 0.753818 0.76226\n",
      "792 39409.8 11.3052 3940.98046875 0.70425 0.753814 0.762255\n",
      "793 39408.6 11.3056 3940.86015625 0.704239 0.753809 0.762251\n",
      "794 39407.4 11.306 3940.7390625 0.704228 0.753805 0.762247\n",
      "795 39406.1 11.3064 3940.6140625 0.704217 0.753801 0.762242\n",
      "796 39404.9 11.3068 3940.49296875 0.704206 0.753796 0.762238\n",
      "797 39403.7 11.3072 3940.36953125 0.704195 0.753792 0.762234\n",
      "798 39402.5 11.3075 3940.24570313 0.704185 0.753788 0.76223\n",
      "799 39401.2 11.3079 3940.12460938 0.704174 0.753784 0.762226\n",
      "800 39400.0 11.3083 3940.00390625 0.704163 0.753779 0.762222\n",
      "801 39398.8 11.3087 3939.88203125 0.704152 0.753775 0.762218\n",
      "802 39397.6 11.3091 3939.76328125 0.704141 0.753771 0.762214\n",
      "803 39396.4 11.3094 3939.640625 0.70413 0.753767 0.76221\n",
      "804 39395.2 11.3098 3939.51875 0.704119 0.753763 0.762206\n",
      "805 39394.0 11.3102 3939.39804688 0.704108 0.753759 0.762202\n",
      "806 39392.8 11.3106 3939.27890625 0.704098 0.753755 0.762197\n",
      "807 39391.5 11.3109 3939.153125 0.704088 0.75375 0.762194\n",
      "808 39390.3 11.3113 3939.03476563 0.704077 0.753746 0.76219\n",
      "809 39389.1 11.3117 3938.9140625 0.704066 0.753742 0.762185\n",
      "810 39388.0 11.3121 3938.796875 0.704055 0.753738 0.762181\n",
      "811 39386.8 11.3124 3938.6765625 0.704044 0.753734 0.762177\n",
      "812 39385.6 11.3128 3938.5578125 0.704033 0.75373 0.762174\n",
      "813 39384.4 11.3132 3938.43828125 0.704022 0.753726 0.76217\n",
      "814 39383.2 11.3136 3938.32070312 0.704012 0.753722 0.762166\n",
      "815 39382.0 11.3139 3938.2015625 0.704001 0.753718 0.762162\n",
      "816 39380.8 11.3143 3938.0828125 0.703991 0.753714 0.762158\n",
      "817 39379.6 11.3147 3937.9640625 0.703979 0.75371 0.762154\n",
      "818 39378.4 11.3151 3937.84140625 0.703969 0.753706 0.76215\n",
      "819 39377.2 11.3154 3937.72421875 0.703958 0.753703 0.762146\n",
      "820 39376.0 11.3158 3937.6046875 0.703947 0.753699 0.762142\n",
      "821 39374.9 11.3162 3937.4859375 0.703937 0.753695 0.762138\n",
      "822 39373.7 11.3166 3937.37265625 0.703926 0.753691 0.762134\n",
      "823 39372.5 11.3169 3937.253125 0.703916 0.753687 0.76213\n",
      "824 39371.4 11.3173 3937.1359375 0.703905 0.753683 0.762126\n",
      "825 39370.2 11.3177 3937.01835937 0.703895 0.75368 0.762122\n",
      "826 39369.0 11.318 3936.9015625 0.703885 0.753676 0.762118\n",
      "827 39367.9 11.3184 3936.7859375 0.703874 0.753672 0.762114\n",
      "828 39366.6 11.3188 3936.66484375 0.703863 0.753668 0.76211\n",
      "829 39365.5 11.3191 3936.54921875 0.703853 0.753664 0.762107\n",
      "830 39364.3 11.3195 3936.43164062 0.703843 0.75366 0.762103\n",
      "831 39363.2 11.3199 3936.3171875 0.703832 0.753657 0.762099\n",
      "832 39362.0 11.3202 3936.20117188 0.703822 0.753653 0.762095\n",
      "833 39360.9 11.3206 3936.08671875 0.703812 0.753649 0.762092\n",
      "834 39359.7 11.321 3935.96992188 0.703801 0.753646 0.762088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "835 39358.5 11.3213 3935.85429687 0.703791 0.753642 0.762084\n",
      "836 39357.4 11.3217 3935.74023438 0.703781 0.753638 0.762081\n",
      "837 39356.2 11.3221 3935.62265625 0.70377 0.753634 0.762077\n",
      "838 39355.1 11.3224 3935.5078125 0.70376 0.753631 0.762074\n",
      "839 39353.9 11.3228 3935.39414063 0.70375 0.753627 0.76207\n",
      "840 39352.8 11.3231 3935.28046875 0.703739 0.753623 0.762066\n",
      "841 39351.6 11.3235 3935.1640625 0.703729 0.75362 0.762063\n",
      "842 39350.5 11.3239 3935.04921875 0.703719 0.753616 0.762059\n",
      "843 39349.4 11.3242 3934.93671875 0.703709 0.753612 0.762056\n",
      "844 39348.2 11.3246 3934.81875 0.703699 0.753609 0.762052\n",
      "845 39347.1 11.325 3934.70625 0.703688 0.753605 0.762048\n",
      "846 39345.9 11.3253 3934.58671875 0.703678 0.753602 0.762045\n",
      "847 39344.7 11.3257 3934.47421875 0.703668 0.753598 0.762041\n",
      "848 39343.6 11.326 3934.36015625 0.703658 0.753595 0.762038\n",
      "849 39342.5 11.3264 3934.25078125 0.703648 0.753591 0.762034\n",
      "850 39341.4 11.3268 3934.1390625 0.703637 0.753588 0.762031\n",
      "851 39340.2 11.3271 3934.025 0.703627 0.753584 0.762027\n",
      "852 39339.1 11.3275 3933.91132813 0.703618 0.753581 0.762024\n",
      "853 39338.0 11.3278 3933.79765625 0.703608 0.753577 0.76202\n",
      "854 39336.9 11.3282 3933.68710938 0.703597 0.753574 0.762017\n",
      "855 39335.7 11.3286 3933.57070312 0.703587 0.75357 0.762013\n",
      "856 39334.6 11.3289 3933.45546875 0.703578 0.753567 0.76201\n",
      "857 39333.4 11.3293 3933.34140625 0.703568 0.753564 0.762007\n",
      "858 39332.3 11.3296 3933.23242188 0.703558 0.75356 0.762003\n",
      "859 39331.2 11.33 3933.12109375 0.703548 0.753557 0.762\n",
      "860 39330.1 11.3303 3933.01054687 0.703537 0.753553 0.761997\n",
      "861 39329.0 11.3307 3932.896875 0.703527 0.75355 0.761993\n",
      "862 39327.8 11.3311 3932.78359375 0.703518 0.753546 0.761989\n",
      "863 39326.7 11.3314 3932.6703125 0.703507 0.753543 0.761986\n",
      "864 39325.6 11.3318 3932.5609375 0.703497 0.75354 0.761983\n",
      "865 39324.5 11.3321 3932.45117188 0.703487 0.753536 0.761979\n",
      "866 39323.4 11.3325 3932.33789062 0.703478 0.753533 0.761976\n",
      "867 39322.3 11.3328 3932.22851562 0.703468 0.75353 0.761973\n",
      "868 39321.2 11.3332 3932.11601562 0.703458 0.753527 0.761969\n",
      "869 39320.1 11.3335 3932.00703125 0.703448 0.753523 0.761966\n",
      "870 39319.0 11.3339 3931.89765625 0.703438 0.75352 0.761963\n",
      "871 39317.9 11.3342 3931.7859375 0.703429 0.753517 0.761959\n",
      "872 39316.7 11.3346 3931.6734375 0.703419 0.753513 0.761956\n",
      "873 39315.6 11.3349 3931.56484375 0.703408 0.75351 0.761953\n",
      "874 39314.5 11.3353 3931.45195312 0.703398 0.753507 0.761949\n",
      "875 39313.4 11.3356 3931.3421875 0.703388 0.753504 0.761946\n",
      "876 39312.3 11.336 3931.23398438 0.703379 0.753501 0.761943\n",
      "877 39311.2 11.3363 3931.1234375 0.703369 0.753498 0.76194\n",
      "878 39310.1 11.3367 3931.01328125 0.70336 0.753495 0.761937\n",
      "879 39309.0 11.337 3930.90234375 0.70335 0.753492 0.761933\n",
      "880 39307.9 11.3374 3930.79296875 0.70334 0.753489 0.76193\n",
      "881 39306.9 11.3377 3930.68984375 0.703331 0.753485 0.761927\n",
      "882 39305.8 11.3381 3930.5796875 0.703321 0.753482 0.761924\n",
      "883 39304.7 11.3384 3930.47109375 0.703311 0.753479 0.761921\n",
      "884 39303.6 11.3388 3930.36171875 0.703301 0.753476 0.761918\n",
      "885 39302.6 11.3391 3930.25546875 0.703292 0.753473 0.761915\n",
      "886 39301.5 11.3395 3930.1484375 0.703282 0.75347 0.761912\n",
      "887 39300.4 11.3398 3930.03828125 0.703272 0.753467 0.761909\n",
      "888 39299.3 11.3402 3929.93046875 0.703262 0.753464 0.761906\n",
      "889 39298.2 11.3405 3929.81914062 0.703252 0.753462 0.761903\n",
      "890 39297.1 11.3408 3929.71484375 0.703243 0.753459 0.7619\n",
      "891 39296.1 11.3412 3929.6078125 0.703233 0.753456 0.761898\n",
      "892 39295.0 11.3415 3929.50234375 0.703224 0.753453 0.761895\n",
      "893 39293.9 11.3419 3929.39453125 0.703214 0.75345 0.761892\n",
      "894 39292.8 11.3422 3929.28398437 0.703205 0.753447 0.761889\n",
      "895 39291.8 11.3426 3929.178125 0.703195 0.753444 0.761886\n",
      "896 39290.7 11.3429 3929.07382813 0.703185 0.753442 0.761883\n",
      "897 39289.6 11.3432 3928.96367187 0.703175 0.753439 0.76188\n",
      "898 39288.6 11.3436 3928.86132812 0.703166 0.753436 0.761877\n",
      "899 39287.5 11.3439 3928.75390625 0.703156 0.753433 0.761875\n",
      "900 39286.5 11.3443 3928.646875 0.703147 0.75343 0.761871\n",
      "901 39285.4 11.3446 3928.5390625 0.703138 0.753428 0.761869\n",
      "902 39284.3 11.345 3928.43203125 0.703128 0.753425 0.761866\n",
      "903 39283.2 11.3453 3928.325 0.703118 0.753422 0.761863\n",
      "904 39282.2 11.3456 3928.21953125 0.703108 0.753419 0.76186\n",
      "905 39281.2 11.346 3928.1171875 0.703099 0.753417 0.761857\n",
      "906 39280.1 11.3463 3928.01015625 0.70309 0.753414 0.761854\n",
      "907 39279.0 11.3467 3927.90390625 0.703081 0.753411 0.761851\n",
      "908 39278.0 11.347 3927.80078125 0.703071 0.753408 0.761848\n",
      "909 39276.9 11.3474 3927.6921875 0.703062 0.753405 0.761846\n",
      "910 39275.9 11.3477 3927.5859375 0.703052 0.753403 0.761843\n",
      "911 39274.8 11.348 3927.48398438 0.703043 0.7534 0.76184\n",
      "912 39273.8 11.3484 3927.37695312 0.703034 0.753397 0.761837\n",
      "913 39272.7 11.3487 3927.2734375 0.703024 0.753395 0.761834\n",
      "914 39271.7 11.349 3927.16953125 0.703015 0.753392 0.761831\n",
      "915 39270.6 11.3494 3927.06171875 0.703005 0.753389 0.761828\n",
      "916 39269.6 11.3497 3926.95585937 0.702996 0.753386 0.761826\n",
      "917 39268.5 11.3501 3926.85390625 0.702987 0.753384 0.761823\n",
      "918 39267.5 11.3504 3926.74765625 0.702977 0.753381 0.76182\n",
      "919 39266.5 11.3507 3926.64921875 0.702967 0.753378 0.761817\n",
      "920 39265.4 11.3511 3926.54414063 0.702958 0.753376 0.761815\n",
      "921 39264.4 11.3514 3926.44179687 0.702949 0.753373 0.761812\n",
      "922 39263.4 11.3517 3926.3375 0.70294 0.75337 0.761809\n",
      "923 39262.3 11.3521 3926.23203125 0.702931 0.753368 0.761807\n",
      "924 39261.3 11.3524 3926.12578125 0.702922 0.753365 0.761804\n",
      "925 39260.2 11.3527 3926.0234375 0.702913 0.753363 0.761801\n",
      "926 39259.1 11.3531 3925.9140625 0.702904 0.75336 0.761799\n",
      "927 39258.1 11.3534 3925.81171875 0.702894 0.753358 0.761796\n",
      "928 39257.1 11.3537 3925.709375 0.702885 0.753355 0.761793\n",
      "929 39256.1 11.3541 3925.60625 0.702876 0.753353 0.761791\n",
      "930 39255.0 11.3544 3925.50390625 0.702867 0.75335 0.761788\n",
      "931 39254.0 11.3547 3925.39921875 0.702857 0.753347 0.761785\n",
      "932 39253.0 11.3551 3925.296875 0.702848 0.753345 0.761783\n",
      "933 39252.0 11.3554 3925.1953125 0.702839 0.753342 0.76178\n",
      "934 39250.9 11.3557 3925.090625 0.702829 0.75334 0.761778\n",
      "935 39249.9 11.3561 3924.99023438 0.70282 0.753337 0.761775\n",
      "936 39248.9 11.3564 3924.88554687 0.702811 0.753335 0.761773\n",
      "937 39247.8 11.3567 3924.78164062 0.702802 0.753332 0.76177\n",
      "938 39246.8 11.357 3924.67929688 0.702793 0.75333 0.761768\n",
      "939 39245.8 11.3574 3924.57695312 0.702784 0.753328 0.761765\n",
      "940 39244.8 11.3577 3924.47539062 0.702774 0.753325 0.761763\n",
      "941 39243.8 11.358 3924.37578125 0.702765 0.753323 0.76176\n",
      "942 39242.8 11.3584 3924.27734375 0.702756 0.75332 0.761758\n",
      "943 39241.7 11.3587 3924.17304688 0.702746 0.753318 0.761755\n",
      "944 39240.7 11.359 3924.07265625 0.702737 0.753315 0.761753\n",
      "945 39239.7 11.3593 3923.9703125 0.702728 0.753313 0.76175\n",
      "946 39238.7 11.3597 3923.865625 0.702719 0.753311 0.761748\n",
      "947 39237.7 11.36 3923.765625 0.70271 0.753309 0.761745\n",
      "948 39236.6 11.3603 3923.6640625 0.702701 0.753306 0.761743\n",
      "949 39235.7 11.3606 3923.56640625 0.702692 0.753304 0.761741\n",
      "950 39234.6 11.361 3923.46367187 0.702683 0.753302 0.761738\n",
      "951 39233.6 11.3613 3923.36171875 0.702674 0.753299 0.761736\n",
      "952 39232.6 11.3616 3923.26015625 0.702665 0.753297 0.761734\n",
      "953 39231.6 11.362 3923.15703125 0.702656 0.753295 0.761731\n",
      "954 39230.6 11.3623 3923.05820313 0.702647 0.753293 0.761729\n",
      "955 39229.6 11.3626 3922.95546875 0.702637 0.75329 0.761727\n",
      "956 39228.5 11.3629 3922.85390625 0.702628 0.753288 0.761725\n",
      "957 39227.6 11.3632 3922.75625 0.702619 0.753286 0.761722\n",
      "958 39226.5 11.3636 3922.65234375 0.70261 0.753284 0.76172\n",
      "959 39225.5 11.3639 3922.55390625 0.702601 0.753282 0.761718\n",
      "960 39224.5 11.3642 3922.45273438 0.702592 0.753279 0.761715\n",
      "961 39223.6 11.3645 3922.35546875 0.702583 0.753277 0.761713\n",
      "962 39222.5 11.3649 3922.25195312 0.702574 0.753275 0.761711\n",
      "963 39221.5 11.3652 3922.15273437 0.702565 0.753273 0.761709\n",
      "964 39220.6 11.3655 3922.05703125 0.702557 0.75327 0.761707\n",
      "965 39219.6 11.3658 3921.95507812 0.702548 0.753268 0.761705\n",
      "966 39218.5 11.3662 3921.85390625 0.702539 0.753266 0.761703\n",
      "967 39217.6 11.3665 3921.75859375 0.70253 0.753264 0.7617\n",
      "968 39216.6 11.3668 3921.65820312 0.702521 0.753262 0.761698\n",
      "969 39215.6 11.3671 3921.56015625 0.702512 0.75326 0.761696\n",
      "970 39214.6 11.3674 3921.4578125 0.702503 0.753258 0.761694\n",
      "971 39213.6 11.3678 3921.36171875 0.702495 0.753256 0.761691\n",
      "972 39212.6 11.3681 3921.26171875 0.702486 0.753254 0.76169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "973 39211.6 11.3684 3921.16445312 0.702478 0.753252 0.761687\n",
      "974 39210.7 11.3687 3921.06640625 0.702469 0.753249 0.761685\n",
      "975 39209.6 11.369 3920.9640625 0.70246 0.753248 0.761683\n",
      "976 39208.7 11.3694 3920.8703125 0.702452 0.753246 0.761681\n",
      "977 39207.7 11.3697 3920.76953125 0.702443 0.753244 0.761679\n",
      "978 39206.7 11.37 3920.67226562 0.702434 0.753241 0.761677\n",
      "979 39205.7 11.3703 3920.57421875 0.702425 0.75324 0.761675\n",
      "980 39204.8 11.3707 3920.4765625 0.702416 0.753238 0.761672\n",
      "981 39203.8 11.371 3920.37578125 0.702407 0.753236 0.76167\n",
      "982 39202.8 11.3713 3920.2796875 0.702399 0.753234 0.761668\n",
      "983 39201.8 11.3716 3920.1796875 0.702389 0.753232 0.761667\n",
      "984 39200.8 11.3719 3920.0828125 0.702381 0.75323 0.761664\n",
      "985 39199.8 11.3722 3919.98203125 0.702372 0.753228 0.761662\n",
      "986 39198.9 11.3726 3919.88710937 0.702364 0.753226 0.76166\n",
      "987 39197.9 11.3729 3919.78828125 0.702355 0.753224 0.761659\n",
      "988 39196.9 11.3732 3919.69296875 0.702346 0.753222 0.761656\n",
      "989 39196.0 11.3735 3919.5953125 0.702338 0.75322 0.761655\n",
      "990 39195.0 11.3738 3919.49726563 0.702329 0.753219 0.761653\n",
      "991 39194.0 11.3741 3919.40039062 0.70232 0.753217 0.761651\n",
      "992 39193.1 11.3745 3919.30507812 0.702311 0.753215 0.761649\n",
      "993 39192.1 11.3748 3919.2078125 0.702302 0.753213 0.761647\n",
      "994 39191.1 11.3751 3919.10859375 0.702294 0.753211 0.761645\n",
      "995 39190.1 11.3754 3919.01210937 0.702285 0.753209 0.761643\n",
      "996 39189.1 11.3757 3918.91484375 0.702276 0.753207 0.761641\n",
      "997 39188.2 11.376 3918.81953125 0.702268 0.753205 0.761639\n",
      "998 39187.2 11.3763 3918.71953125 0.702258 0.753203 0.761637\n",
      "999 39186.2 11.3767 3918.62421875 0.70225 0.753201 0.761636\n",
      "1000 39185.3 11.377 3918.53125 0.702241 0.7532 0.761634\n",
      "1001 39184.4 11.3773 3918.43632812 0.702232 0.753198 0.761632\n",
      "1002 39183.4 11.3776 3918.33828125 0.702224 0.753196 0.76163\n",
      "1003 39182.4 11.3779 3918.24140625 0.702215 0.753195 0.761628\n",
      "1004 39181.4 11.3782 3918.1421875 0.702206 0.753193 0.761626\n",
      "1005 39180.5 11.3785 3918.04609375 0.702197 0.753191 0.761624\n",
      "1006 39179.5 11.3789 3917.9515625 0.702189 0.75319 0.761623\n",
      "1007 39178.6 11.3792 3917.85625 0.70218 0.753188 0.761621\n",
      "1008 39177.6 11.3795 3917.759375 0.702172 0.753186 0.761619\n",
      "1009 39176.6 11.3798 3917.6640625 0.702164 0.753184 0.761617\n",
      "1010 39175.7 11.3801 3917.5671875 0.702156 0.753182 0.761615\n",
      "1011 39174.8 11.3804 3917.47578125 0.702147 0.753181 0.761613\n",
      "1012 39173.8 11.3807 3917.37773437 0.702138 0.753179 0.761612\n",
      "1013 39172.8 11.381 3917.284375 0.70213 0.753178 0.76161\n",
      "1014 39171.9 11.3813 3917.1875 0.702121 0.753176 0.761607\n",
      "1015 39170.9 11.3816 3917.09375 0.702112 0.753175 0.761606\n",
      "1016 39169.9 11.382 3916.99375 0.702104 0.753173 0.761604\n",
      "1017 39169.0 11.3823 3916.89960938 0.702095 0.753171 0.761602\n",
      "1018 39168.0 11.3826 3916.803125 0.702087 0.753169 0.7616\n",
      "1019 39167.1 11.3829 3916.71015625 0.702078 0.753168 0.761598\n",
      "1020 39166.2 11.3832 3916.61601562 0.702071 0.753166 0.761596\n",
      "1021 39165.2 11.3835 3916.52109375 0.702062 0.753165 0.761595\n",
      "1022 39164.3 11.3838 3916.4265625 0.702053 0.753163 0.761593\n",
      "1023 39163.3 11.3841 3916.3296875 0.702045 0.753162 0.761591\n",
      "1024 39162.4 11.3844 3916.2375 0.702036 0.75316 0.761589\n",
      "1025 39161.5 11.3847 3916.14570313 0.702028 0.753159 0.761587\n",
      "1026 39160.5 11.385 3916.05078125 0.702019 0.753157 0.761586\n",
      "1027 39159.5 11.3854 3915.95234375 0.702011 0.753156 0.761584\n",
      "1028 39158.6 11.3857 3915.86171875 0.702002 0.753154 0.761582\n",
      "1029 39157.6 11.386 3915.76484375 0.701993 0.753153 0.761581\n",
      "1030 39156.7 11.3863 3915.67265625 0.701985 0.753151 0.761579\n",
      "1031 39155.8 11.3866 3915.5796875 0.701977 0.75315 0.761577\n",
      "1032 39154.8 11.3869 3915.48203125 0.701969 0.753148 0.761576\n",
      "1033 39153.9 11.3872 3915.390625 0.701961 0.753147 0.761574\n",
      "1034 39152.9 11.3875 3915.29296875 0.701952 0.753145 0.761572\n",
      "1035 39152.0 11.3878 3915.2015625 0.701944 0.753144 0.761571\n",
      "1036 39151.1 11.3881 3915.11015625 0.701935 0.753143 0.761569\n",
      "1037 39150.2 11.3884 3915.01640625 0.701927 0.753141 0.761567\n",
      "1038 39149.2 11.3887 3914.921875 0.701919 0.75314 0.761566\n",
      "1039 39148.3 11.389 3914.83203125 0.70191 0.753138 0.761564\n",
      "1040 39147.4 11.3893 3914.7421875 0.701902 0.753137 0.761563\n",
      "1041 39146.5 11.3896 3914.6453125 0.701894 0.753136 0.761561\n",
      "1042 39145.5 11.3899 3914.55078125 0.701885 0.753134 0.761559\n",
      "1043 39144.6 11.3902 3914.45625 0.701877 0.753133 0.761558\n",
      "1044 39143.6 11.3905 3914.3625 0.701868 0.753132 0.761556\n",
      "1045 39142.7 11.3908 3914.271875 0.70186 0.75313 0.761554\n",
      "1046 39141.8 11.3911 3914.17773438 0.701851 0.753129 0.761553\n",
      "1047 39140.8 11.3915 3914.08242187 0.701843 0.753128 0.761551\n",
      "1048 39139.9 11.3918 3913.99257812 0.701834 0.753126 0.76155\n",
      "1049 39139.0 11.3921 3913.896875 0.701827 0.753125 0.761548\n",
      "1050 39138.1 11.3924 3913.80546875 0.701818 0.753124 0.761547\n",
      "1051 39137.1 11.3927 3913.71015625 0.70181 0.753122 0.761545\n",
      "1052 39136.2 11.393 3913.6203125 0.701801 0.753121 0.761544\n",
      "1053 39135.3 11.3933 3913.52890625 0.701793 0.75312 0.761543\n",
      "1054 39134.4 11.3936 3913.43828125 0.701785 0.753119 0.761541\n",
      "1055 39133.4 11.3939 3913.34492188 0.701777 0.753117 0.76154\n",
      "1056 39132.5 11.3942 3913.253125 0.701768 0.753115 0.761538\n",
      "1057 39131.7 11.3945 3913.16523438 0.70176 0.753115 0.761537\n",
      "1058 39130.7 11.3948 3913.06796875 0.701752 0.753114 0.761535\n",
      "1059 39129.7 11.3951 3912.97382813 0.701744 0.753112 0.761534\n",
      "1060 39128.9 11.3954 3912.88515625 0.701735 0.753111 0.761532\n",
      "1061 39127.9 11.3957 3912.7875 0.701727 0.75311 0.761531\n",
      "1062 39127.0 11.396 3912.69804687 0.701719 0.753109 0.76153\n",
      "1063 39126.1 11.3963 3912.60507813 0.701711 0.753107 0.761528\n",
      "1064 39125.1 11.3966 3912.51484375 0.701703 0.753106 0.761527\n",
      "1065 39124.3 11.3969 3912.4265625 0.701694 0.753105 0.761525\n",
      "1066 39123.3 11.3972 3912.3328125 0.701686 0.753103 0.761524\n",
      "1067 39122.4 11.3975 3912.24375 0.701677 0.753102 0.761522\n",
      "1068 39121.5 11.3977 3912.15195313 0.70167 0.753101 0.761521\n",
      "1069 39120.6 11.3981 3912.06015625 0.701661 0.7531 0.761519\n",
      "1070 39119.7 11.3983 3911.96640625 0.701654 0.753098 0.761518\n",
      "1071 39118.7 11.3986 3911.8734375 0.701645 0.753098 0.761516\n",
      "1072 39117.8 11.3989 3911.78164062 0.701637 0.753096 0.761515\n",
      "1073 39117.0 11.3992 3911.69570312 0.701629 0.753095 0.761513\n",
      "1074 39116.0 11.3995 3911.60390625 0.701621 0.753094 0.761512\n",
      "1075 39115.1 11.3998 3911.50976562 0.701612 0.753093 0.76151\n",
      "1076 39114.2 11.4001 3911.41953125 0.701604 0.753092 0.761509\n",
      "1077 39113.3 11.4004 3911.33046875 0.701596 0.753091 0.761507\n",
      "1078 39112.4 11.4007 3911.23828125 0.701588 0.753089 0.761506\n",
      "1079 39111.5 11.401 3911.14765625 0.70158 0.753088 0.761504\n",
      "1080 39110.6 11.4013 3911.06015625 0.701571 0.753087 0.761503\n",
      "1081 39109.7 11.4016 3910.9671875 0.701563 0.753086 0.761502\n",
      "1082 39108.8 11.4019 3910.87890625 0.701555 0.753085 0.761501\n",
      "1083 39107.8 11.4022 3910.7828125 0.701547 0.753083 0.761499\n",
      "1084 39106.9 11.4025 3910.6921875 0.701539 0.753083 0.761498\n",
      "1085 39106.0 11.4028 3910.6046875 0.701531 0.753082 0.761496\n",
      "1086 39105.1 11.4031 3910.5140625 0.701522 0.753081 0.761495\n",
      "1087 39104.2 11.4034 3910.425 0.701514 0.753079 0.761494\n",
      "1088 39103.3 11.4037 3910.33203125 0.701506 0.753078 0.761493\n",
      "1089 39102.4 11.4039 3910.2390625 0.701498 0.753077 0.761491\n",
      "1090 39101.5 11.4042 3910.1484375 0.70149 0.753076 0.76149\n",
      "1091 39100.6 11.4045 3910.05625 0.701482 0.753076 0.761489\n",
      "1092 39099.7 11.4048 3909.96914062 0.701474 0.753074 0.761488\n",
      "1093 39098.8 11.4051 3909.88320312 0.701465 0.753073 0.761487\n",
      "1094 39097.9 11.4054 3909.79101562 0.701457 0.753073 0.761485\n",
      "1095 39097.0 11.4057 3909.70078125 0.701449 0.753072 0.761484\n",
      "1096 39096.1 11.406 3909.6109375 0.701441 0.75307 0.761483\n",
      "1097 39095.2 11.4063 3909.5203125 0.701433 0.753069 0.761482\n",
      "1098 39094.3 11.4066 3909.43046875 0.701425 0.753069 0.76148\n",
      "1099 39093.4 11.4069 3909.3421875 0.701417 0.753068 0.761479\n",
      "1100 39092.5 11.4072 3909.25 0.701409 0.753066 0.761478\n",
      "1101 39091.6 11.4075 3909.1640625 0.701402 0.753066 0.761477\n",
      "1102 39090.7 11.4078 3909.0734375 0.701393 0.753065 0.761476\n",
      "1103 39089.8 11.408 3908.98320312 0.701385 0.753064 0.761475\n",
      "1104 39088.9 11.4083 3908.89335937 0.701377 0.753063 0.761473\n",
      "1105 39088.1 11.4086 3908.80507812 0.701369 0.753062 0.761472\n",
      "1106 39087.2 11.4089 3908.715625 0.701361 0.753061 0.761472\n",
      "1107 39086.3 11.4092 3908.62851563 0.701352 0.75306 0.76147\n",
      "1108 39085.4 11.4095 3908.5421875 0.701345 0.75306 0.76147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1109 39084.5 11.4098 3908.44804687 0.701337 0.753059 0.761468\n",
      "1110 39083.6 11.4101 3908.36328125 0.701329 0.753058 0.761467\n",
      "1111 39082.7 11.4104 3908.27109375 0.701321 0.753057 0.761466\n",
      "1112 39081.8 11.4107 3908.17929688 0.701312 0.753056 0.761465\n",
      "1113 39080.9 11.411 3908.09375 0.701304 0.753055 0.761464\n",
      "1114 39080.0 11.4112 3908.00429687 0.701297 0.753054 0.761463\n",
      "1115 39079.1 11.4115 3907.91054687 0.701289 0.753053 0.761462\n",
      "1116 39078.2 11.4118 3907.821875 0.701281 0.753053 0.761461\n",
      "1117 39077.3 11.4121 3907.73476562 0.701273 0.753052 0.76146\n",
      "1118 39076.5 11.4124 3907.64765625 0.701265 0.753051 0.761459\n",
      "1119 39075.6 11.4127 3907.55546875 0.701257 0.75305 0.761458\n",
      "1120 39074.7 11.413 3907.46875 0.701249 0.753049 0.761457\n",
      "1121 39073.8 11.4133 3907.37890625 0.701241 0.753048 0.761456\n",
      "1122 39072.9 11.4135 3907.28828125 0.701233 0.753048 0.761455\n",
      "1123 39072.1 11.4138 3907.20507812 0.701225 0.753047 0.761454\n",
      "1124 39071.2 11.4141 3907.11796875 0.701217 0.753046 0.761454\n",
      "1125 39070.3 11.4144 3907.02695313 0.701209 0.753045 0.761453\n",
      "1126 39069.4 11.4147 3906.9375 0.701202 0.753045 0.761452\n",
      "1127 39068.5 11.415 3906.84765625 0.701194 0.753044 0.761451\n",
      "1128 39067.6 11.4153 3906.76054687 0.701186 0.753044 0.76145\n",
      "1129 39066.7 11.4155 3906.67265625 0.701178 0.753043 0.761449\n",
      "1130 39065.8 11.4158 3906.58320313 0.701169 0.753042 0.761448\n",
      "1131 39064.9 11.4161 3906.49375 0.701162 0.753041 0.761447\n",
      "1132 39064.0 11.4164 3906.4046875 0.701154 0.75304 0.761446\n",
      "1133 39063.2 11.4167 3906.32109375 0.701146 0.75304 0.761446\n",
      "1134 39062.3 11.417 3906.23242188 0.701138 0.753039 0.761445\n",
      "1135 39061.4 11.4173 3906.14140625 0.70113 0.753038 0.761444\n",
      "1136 39060.5 11.4175 3906.05390625 0.701122 0.753038 0.761443\n",
      "1137 39059.7 11.4178 3905.965625 0.701114 0.753037 0.761443\n",
      "1138 39058.8 11.4181 3905.87773437 0.701106 0.753037 0.761441\n",
      "1139 39057.9 11.4184 3905.78828125 0.701098 0.753036 0.76144\n",
      "1140 39057.0 11.4187 3905.6953125 0.701091 0.753035 0.76144\n",
      "1141 39056.1 11.419 3905.61015625 0.701082 0.753035 0.761439\n",
      "1142 39055.2 11.4192 3905.5234375 0.701074 0.753034 0.761438\n",
      "1143 39054.4 11.4195 3905.43710938 0.701066 0.753033 0.761437\n",
      "1144 39053.5 11.4198 3905.3515625 0.701059 0.753033 0.761436\n",
      "1145 39052.6 11.4201 3905.26210937 0.701051 0.753033 0.761435\n",
      "1146 39051.8 11.4204 3905.178125 0.701043 0.753032 0.761435\n",
      "1147 39050.9 11.4206 3905.08984375 0.701036 0.753031 0.761434\n",
      "1148 39050.0 11.4209 3905.00078125 0.701028 0.753031 0.761433\n",
      "1149 39049.1 11.4212 3904.9140625 0.70102 0.75303 0.761432\n",
      "1150 39048.2 11.4215 3904.825 0.701013 0.75303 0.761432\n",
      "1151 39047.4 11.4218 3904.73671875 0.701004 0.753029 0.761431\n",
      "1152 39046.5 11.4221 3904.64921875 0.700996 0.753029 0.76143\n",
      "1153 39045.6 11.4223 3904.5609375 0.700989 0.753028 0.76143\n",
      "1154 39044.7 11.4226 3904.47265625 0.700981 0.753028 0.761429\n",
      "1155 39043.9 11.4229 3904.3875 0.700973 0.753027 0.761428\n",
      "1156 39043.0 11.4232 3904.2984375 0.700965 0.753026 0.761428\n",
      "1157 39042.1 11.4235 3904.2125 0.700957 0.753026 0.761427\n",
      "1158 39041.3 11.4237 3904.128125 0.700949 0.753025 0.761427\n",
      "1159 39040.4 11.424 3904.040625 0.700942 0.753025 0.761426\n",
      "1160 39039.5 11.4243 3903.95117188 0.700934 0.753024 0.761425\n",
      "1161 39038.7 11.4246 3903.865625 0.700926 0.753024 0.761424\n",
      "1162 39037.8 11.4249 3903.77578125 0.700918 0.753023 0.761424\n",
      "1163 39036.9 11.4251 3903.690625 0.70091 0.753023 0.761423\n",
      "1164 39036.0 11.4254 3903.603125 0.700903 0.753022 0.761423\n",
      "1165 39035.2 11.4257 3903.51640625 0.700895 0.753022 0.761422\n",
      "1166 39034.3 11.426 3903.42734375 0.700887 0.753021 0.761421\n",
      "1167 39033.4 11.4263 3903.33984375 0.700879 0.753021 0.761421\n",
      "1168 39032.5 11.4265 3903.25429687 0.700871 0.753021 0.761421\n",
      "1169 39031.7 11.4268 3903.1671875 0.700863 0.75302 0.76142\n",
      "1170 39030.8 11.4271 3903.08125 0.700855 0.75302 0.761419\n",
      "1171 39029.9 11.4274 3902.99140625 0.700847 0.753019 0.761419\n",
      "1172 39029.0 11.4277 3902.90390625 0.700839 0.753019 0.761418\n",
      "1173 39028.2 11.4279 3902.81640625 0.700832 0.753019 0.761418\n",
      "1174 39027.3 11.4282 3902.73125 0.700824 0.753018 0.761417\n",
      "1175 39026.4 11.4285 3902.64414063 0.700816 0.753018 0.761416\n",
      "1176 39025.6 11.4288 3902.55742187 0.700809 0.753018 0.761416\n",
      "1177 39024.7 11.429 3902.46953125 0.700801 0.753017 0.761415\n",
      "1178 39023.8 11.4293 3902.38359375 0.700793 0.753017 0.761415\n",
      "1179 39023.0 11.4296 3902.2953125 0.700785 0.753016 0.761414\n",
      "1180 39022.1 11.4299 3902.21132812 0.700777 0.753016 0.761414\n",
      "1181 39021.2 11.4301 3902.12304688 0.700769 0.753015 0.761413\n",
      "1182 39020.4 11.4304 3902.0375 0.700761 0.753015 0.761412\n",
      "1183 39019.5 11.4307 3901.9515625 0.700754 0.753015 0.761412\n",
      "1184 39018.6 11.431 3901.86484375 0.700746 0.753014 0.761412\n",
      "1185 39017.8 11.4312 3901.7765625 0.700738 0.753014 0.761411\n",
      "1186 39016.9 11.4315 3901.6890625 0.700731 0.753014 0.76141\n",
      "1187 39016.0 11.4318 3901.60234375 0.700723 0.753013 0.76141\n",
      "1188 39015.2 11.4321 3901.51796875 0.700715 0.753013 0.761409\n",
      "1189 39014.3 11.4324 3901.4296875 0.700708 0.753013 0.761409\n",
      "1190 39013.4 11.4326 3901.34296875 0.7007 0.753012 0.761408\n",
      "1191 39012.6 11.4329 3901.25859375 0.700692 0.753012 0.761408\n",
      "1192 39011.7 11.4332 3901.17421875 0.700684 0.753012 0.761408\n",
      "1193 39010.8 11.4334 3901.084375 0.700677 0.753012 0.761407\n",
      "1194 39010.0 11.4337 3901.00078125 0.700669 0.753011 0.761407\n",
      "1195 39009.1 11.434 3900.91328125 0.700661 0.753011 0.761406\n",
      "1196 39008.3 11.4343 3900.82578125 0.700653 0.753011 0.761406\n",
      "1197 39007.4 11.4345 3900.7421875 0.700646 0.753011 0.761406\n",
      "1198 39006.6 11.4348 3900.65625 0.700638 0.753011 0.761405\n",
      "1199 39005.7 11.4351 3900.5671875 0.70063 0.75301 0.761405\n",
      "1200 39004.8 11.4354 3900.48203125 0.700623 0.75301 0.761404\n",
      "1201 39003.9 11.4356 3900.39179688 0.700615 0.75301 0.761404\n",
      "1202 39003.1 11.4359 3900.30859375 0.700607 0.75301 0.761404\n",
      "1203 39002.2 11.4362 3900.22421875 0.7006 0.75301 0.761403\n",
      "1204 39001.4 11.4365 3900.1375 0.700592 0.75301 0.761402\n",
      "1205 39000.5 11.4367 3900.0546875 0.700584 0.75301 0.761402\n",
      "1206 38999.6 11.437 3899.96484375 0.700576 0.75301 0.761402\n",
      "1207 38998.8 11.4373 3899.88007813 0.700568 0.75301 0.761401\n",
      "1208 38998.0 11.4376 3899.79609375 0.700561 0.75301 0.761401\n",
      "1209 38997.1 11.4378 3899.70859375 0.700553 0.75301 0.7614\n",
      "1210 38996.2 11.4381 3899.62421875 0.700545 0.75301 0.7614\n",
      "1211 38995.4 11.4384 3899.53671875 0.700537 0.75301 0.761399\n",
      "1212 38994.5 11.4386 3899.45234375 0.700529 0.75301 0.761399\n",
      "1213 38993.7 11.4389 3899.36679688 0.700522 0.75301 0.761398\n",
      "1214 38992.8 11.4392 3899.28125 0.700514 0.75301 0.761398\n",
      "1215 38991.9 11.4394 3899.19492188 0.700507 0.75301 0.761398\n",
      "1216 38991.1 11.4397 3899.10664063 0.700499 0.75301 0.761398\n",
      "1217 38990.2 11.44 3899.01992187 0.700491 0.753011 0.761397\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-9b5b0ef11eeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_rmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_rmse_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_rmse_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_cost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_cost\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrmse_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrmse_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_rmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_rmse_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_rmse_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Suyixin/anaconda/envs/ws/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Suyixin/anaconda/envs/ws/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Suyixin/anaconda/envs/ws/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/Suyixin/anaconda/envs/ws/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Suyixin/anaconda/envs/ws/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "for i in range(100001):\n",
    "    _, c, r, t, _rmse, _rmse_v, _rmse_t = sess.run([training_step, base_cost, regularizer, total_cost,rmse, rmse_valid, rmse_test])\n",
    "    sess.run(clip)\n",
    "    print(i, c,r,t/10, _rmse, _rmse_v, _rmse_t)\n",
    "    #print(sess.run(md_var)[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.42625999,  0.4261874 ,  0.61788458,  0.42869255,  0.30634779,\n",
       "        0.24180219,  0.52125096,  1.21953642,  0.67584401, -1.21263146,\n",
       "       -0.82477587,  0.90895551, -0.97363335,  1.76878488, -0.06312997,\n",
       "        0.28135145, -0.28535238, -0.35262966, -0.81182677, -0.44890034,\n",
       "       -0.33267298,  1.47805762, -1.37017655,  0.99695224,  0.73363858,\n",
       "        1.15845942, -0.56261462,  1.27127635,  1.63562977, -0.61299998,\n",
       "       -1.04596841,  0.08003101, -0.64074308, -0.85164028, -0.02274869,\n",
       "        0.52204686,  0.00311393, -0.49076593,  0.37734815,  0.62108225,\n",
       "        1.44287658, -0.45581594, -1.42612994,  0.16134748,  0.4529334 ,\n",
       "        0.35469374,  0.43844801, -0.5407331 ,  0.80048341,  0.77998137,\n",
       "       -0.13692749,  0.51383549,  0.05528622, -1.6316644 ,  0.49534857,\n",
       "        0.16590255,  1.68930745,  0.59414142,  0.98762357, -0.95596755,\n",
       "       -0.770724  ,  0.67985415, -0.51150614,  0.22462006, -0.22364046,\n",
       "        0.78469896, -0.22324169, -0.73120481,  0.4400031 ,  1.18174922,\n",
       "        0.62999094,  0.66702223,  0.08272875,  0.35519245, -1.29520524,\n",
       "       -0.72722137, -0.5315488 ,  1.24173248, -0.71368963,  1.00358343,\n",
       "       -0.43494913, -0.48180228, -0.75980169,  0.59769183, -0.912678  ,\n",
       "        1.20326221,  1.38549984, -0.72339624,  0.77603978,  0.07043652,\n",
       "       -0.17227307,  0.1012776 , -0.82400727, -0.2441071 ,  1.28307486,\n",
       "        0.46182096, -0.04825108,  0.84232551,  0.53388774,  0.50418639,\n",
       "        0.50200462,  0.16068298, -0.65489805, -0.46691829, -0.77334523,\n",
       "        0.63444042, -0.45327848, -0.53622782,  0.86699665,  0.77589333,\n",
       "       -0.29701984, -0.98345327, -1.31169295, -0.44022354, -0.79174441,\n",
       "       -0.18580547,  1.3594327 , -0.04032064,  0.16698459, -0.0070427 ,\n",
       "       -0.46925598, -0.05062737,  0.849967  ,  0.22938743,  1.04293013,\n",
       "        0.60860026,  1.13747978, -0.19492814, -0.79621589, -0.20136882,\n",
       "       -0.72526395,  1.17055285, -0.20093043,  0.363363  ,  1.24309361,\n",
       "       -1.61096621, -0.18915448,  1.07173061, -0.27485514,  1.50151062,\n",
       "       -1.17208004, -0.62230742, -0.67804104,  0.35812441,  0.48368824,\n",
       "        0.58964956, -1.77366281, -0.66484702, -1.64930141,  0.50016308,\n",
       "       -0.61628246, -0.07267428,  0.62204742,  1.74293745, -1.27100825,\n",
       "       -0.05627169,  0.58868909,  0.89088148,  0.62500441, -0.85225427,\n",
       "        0.14856932, -0.30281293, -0.23443648, -0.15993226, -0.50198519,\n",
       "       -1.101192  ,  0.98679286, -1.33260858,  0.57929116, -0.49188799,\n",
       "       -0.26567769,  1.50826561,  0.73252857, -0.1672727 , -0.37465358,\n",
       "       -0.92875642, -0.2570329 , -1.18148172, -1.13571167, -0.86249506,\n",
       "       -0.83542275,  0.70538437,  0.14170446,  1.25682902,  0.83539426,\n",
       "        0.11722676, -0.28091916,  0.27052531, -0.30589619,  0.03240581,\n",
       "       -1.32251298,  0.94314975,  0.35420108,  1.89672577], dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(md_var)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id_2_genre = pickle.load(open('../data/'+folder+'/id_2_genre.pkl', 'rb'))\n",
    "id_2_director = pickle.load(open('../data/'+folder+'/id_2_director.pkl', 'rb'))\n",
    "id_2_cast = pickle.load(open('../data/'+folder+'/id_2_cast.pkl', 'rb'))\n",
    "genre_2_id = pickle.load(open('../data/'+folder+'/genre_2_id.pkl', 'rb'))\n",
    "director_2_id = pickle.load(open('../data/'+folder+'/director_2_id.pkl', 'rb'))\n",
    "cast_2_id = pickle.load(open('../data/'+folder+'/cast_2_id.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d_list = sess.run(d_result)\n",
    "mean_list = np.mean(d_list, axis=0)\n",
    "\n",
    "with open('director_dtcg.csv','w') as f:\n",
    "    f_csv = csv.writer(f)\n",
    "    headers = ['name', 'score']\n",
    "    f_csv.writerow(headers)\n",
    "    for i in id_2_director:\n",
    "        rows = [id_2_director[i], mean_list[i]]\n",
    "        f_csv.writerow(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_list = sess.run(c_result)\n",
    "mean_list = np.mean(c_list, axis=0)\n",
    "\n",
    "with open('cast_dtcg.csv','w') as f:\n",
    "    f_csv = csv.writer(f)\n",
    "    headers = ['name', 'score']\n",
    "    f_csv.writerow(headers)\n",
    "    for i in id_2_cast:\n",
    "        rows = [id_2_cast[i], mean_list[i]]\n",
    "        f_csv.writerow(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g_list = sess.run(g_result)\n",
    "mean_list = np.mean(g_list, axis=0)\n",
    "\n",
    "with open('genre_dtcg.csv','w') as f:\n",
    "    f_csv = csv.writer(f)\n",
    "    headers = ['name', 'score']\n",
    "    f_csv.writerow(headers)\n",
    "    for i in id_2_genre:\n",
    "        rows = [id_2_genre[i], mean_list[i]]\n",
    "        f_csv.writerow(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate Training RMSE\n",
    "\"\"\"\n",
    "g_user_test = g_user\n",
    "g_item_test = g_movie\n",
    "g_rate_test = g_rating\n",
    "g_result_flatten = g_result_flatten\n",
    "#genre rmse\n",
    "g_test = tf.gather(g_result_flatten, g_user_test * tf.shape(g_result)[1] + g_item_test, name='g_test')\n",
    "g_rmse = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(g_rate_test, g_test))))\n",
    "\n",
    "d_user_test = d_user\n",
    "d_item_test = d_movie\n",
    "d_rate_test = d_rating\n",
    "d_result_flatten = d_result_flatten\n",
    "#genre rmse\n",
    "d_test = tf.gather(d_result_flatten, d_user_test * tf.shape(d_result)[1] + d_item_test, name='d_test')\n",
    "d_rmse = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(d_rate_test, d_test))))\n",
    "\n",
    "t_user_test = t_user\n",
    "t_item_test = t_movie\n",
    "t_rate_test = t_rating\n",
    "t_result_flatten = t_result_flatten\n",
    "#genre rmse\n",
    "t_test = tf.gather(t_result_flatten, t_user_test * tf.shape(t_result)[1] + t_item_test, name='t_test')\n",
    "t_rmse = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(t_rate_test, t_test))))\n",
    "\n",
    "c_user_test = c_user\n",
    "c_item_test = c_movie\n",
    "c_rate_test = c_rating\n",
    "c_result_flatten = c_result_flatten\n",
    "#genre rmse\n",
    "c_test = tf.gather(c_result_flatten, c_user_test * tf.shape(c_result)[1] + c_item_test, name='c_test')\n",
    "c_rmse = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(c_rate_test, c_test))))\n",
    "\n",
    "\n",
    "rmse = tf.divide((g_rmse + d_rmse + t_rmse + c_rmse), 4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate Validation RMSE\n",
    "\"\"\"\n",
    "g_user_v = g_user_v\n",
    "g_item_v = g_movie_v\n",
    "g_rate_v = g_rating_v\n",
    "g_result_flatten = g_result_flatten\n",
    "#genre rmse\n",
    "g_v = tf.gather(g_result_flatten, g_user_v * tf.shape(g_result)[1] + g_item_v, name='g_v')\n",
    "g_rmse_v = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(g_rate_v, g_v))))\n",
    "\n",
    "d_user_v = d_user_v\n",
    "d_item_v = d_movie_v\n",
    "d_rate_v = d_rating_v\n",
    "d_result_flatten = d_result_flatten\n",
    "#genre rmse\n",
    "d_v = tf.gather(d_result_flatten, d_user_v * tf.shape(d_result)[1] + d_item_v, name='d_v')\n",
    "d_rmse_v = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(d_rate_v, d_v))))\n",
    "\n",
    "t_user_v = t_user_v\n",
    "t_item_v = t_movie_v\n",
    "t_rate_v = t_rating_v\n",
    "t_result_flatten = t_result_flatten\n",
    "#genre rmse\n",
    "t_v = tf.gather(t_result_flatten, t_user_v * tf.shape(t_result)[1] + t_item_v, name='t_v')\n",
    "t_rmse_v = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(t_rate_v, t_v))))\n",
    "\n",
    "c_user_v = c_user_v\n",
    "c_item_v = c_movie_v\n",
    "c_rate_v = c_rating_v\n",
    "c_result_flatten = c_result_flatten\n",
    "#genre rmse\n",
    "c_v = tf.gather(c_result_flatten, c_user_v * tf.shape(c_result)[1] + c_item_v, name='c_v')\n",
    "c_rmse_v = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(c_rate_v, c_v))))\n",
    "\n",
    "\n",
    "rmse_v = tf.divide((g_rmse_v + d_rmse_v + t_rmse_v + c_rmse_v), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#training_trend_f = open('../data/'+folder+'/trends.csv', 'w')\n",
    "#csv_trend = csv.writer(training_trend_f)\n",
    "#headers = ['round','train_total', 'train_g', 'train_d', 'train_t', 'train_c', 'valid_total', 'valid_g', 'valid_d', 'valid_t', 'valid_c']\n",
    "#csv_trend.writerow(headers)\n",
    "lowest_num = 0\n",
    "lowest_rmse = 9999999\n",
    "not_dec_num = 0\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "for i in range(100001):\n",
    "    _, c, r = sess.run([training_step, cost, regularizer])\n",
    "    if i%20 == 0:\n",
    "        _rmse_v, _g_rmse_v, _d_rmse_v, _t_rmse_v, _c_rmse_v  = sess.run([rmse_v, g_rmse_v, d_rmse_v, t_rmse_v, c_rmse_v])\n",
    "        sess.run(clip)\n",
    "        if lowest_rmse > _rmse_v + 0.001:\n",
    "            lowest_rmse = _rmse_v\n",
    "            lowest_num = i\n",
    "            not_dec_num = 0\n",
    "        else:\n",
    "            not_dec_num += 1\n",
    "            if not_dec_num > 10:\n",
    "                break\n",
    "            \n",
    "        print(i, c, \"current rmsr: \", _rmse_v,  \"lowest rmse: \", lowest_rmse, \"lowest num: \", lowest_num)\n",
    "        print(_g_rmse_v, _d_rmse_v, _t_rmse_v, _c_rmse_v)\n",
    "        _rmse, _g_rmse, _d_rmse, _t_rmse, _c_rmse = sess.run([rmse, g_rmse, d_rmse, t_rmse, c_rmse])\n",
    "        #w_row = [i, _rmse, _g_rmse, _d_rmse, _t_rmse, _c_rmse, _rmse_v, _g_rmse_v, _d_rmse_v, _t_rmse_v, _c_rmse_v]\n",
    "        #csv_trend.writerow(w_row)\n",
    "    if i%200 == 0:\n",
    "        print()\n",
    "        print(\"Traing RMSR(total, g, d, t, c):\", i , _rmse, _g_rmse, _d_rmse, _t_rmse, _c_rmse)\n",
    "        print()\n",
    "#training_trend_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_matrix(row_num, col_num, user, movie, rating):\n",
    "    # row is movie infor num\n",
    "    # col is user num\n",
    "    matr = [[0 for i in range(col_num)] for j in range(row_num)]\n",
    "    for i in range(len(user)):\n",
    "        matr[movie[i]][user[i]] = float(rating[i])\n",
    "    \n",
    "    return matr\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_v, director_v, topic_v, genre_v, cast_v = sess.run([U,D,T,G,C])\n",
    "\n",
    "d_mat = generate_matrix(DIRECTOR_NUM, USER_NUM, d_user, d_movie, d_rating)\n",
    "t_mat = generate_matrix(TOPIC_NUM, USER_NUM, t_user, t_movie, t_rating)\n",
    "c_mat = generate_matrix(CAST_NUM, USER_NUM, c_user, c_movie, c_rating)\n",
    "g_mat = generate_matrix(GENRE_NUM, USER_NUM, g_user, g_movie, g_rating)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(d_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id_2_genre = pickle.load(open('../data/'+folder+'/id_2_genre.pkl', 'rb'))\n",
    "id_2_director = pickle.load(open('../data/'+folder+'/id_2_director.pkl', 'rb'))\n",
    "id_2_cast = pickle.load(open('../data/'+folder+'/id_2_cast.pkl', 'rb'))\n",
    "genre_2_id = pickle.load(open('../data/'+folder+'/genre_2_id.pkl', 'rb'))\n",
    "director_2_id = pickle.load(open('../data/'+folder+'/director_2_id.pkl', 'rb'))\n",
    "cast_2_id = pickle.load(open('../data/'+folder+'/cast_2_id.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_rating_list(info_id_list, info_matrix, user_vector, user_id, matrix):\n",
    "    result_list = []\n",
    "    for _id in info_id_list:\n",
    "        if matrix[_id][user_id] > 0:\n",
    "            result_list.append(matrix[_id][user_id]) \n",
    "        else:\n",
    "            \n",
    "            _v = info_matrix[:, int(_id)]\n",
    "            _rating = np.dot(user_vector, _v)\n",
    "            result_list.append(_rating)\n",
    "    return result_list\n",
    "\n",
    "def get_rating_list_train(id_tup_list, matrix):\n",
    "    result_list = []\n",
    "    for tup in id_tup_list:\n",
    "        user_id = tup[0]\n",
    "        info_id = tup[1]\n",
    "        result_list.append(matrix[info_id][user_id])\n",
    "\n",
    "def gen_regr_data(m_id, user_rating_tups, is_training = True):\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    user_id_train = []\n",
    "    for tup in user_rating_tups:\n",
    "        one_user_x = []\n",
    "        rating = float(tup[1])\n",
    "        y_train.append(rating)\n",
    "        \n",
    "        user_id = int(tup[0])\n",
    "        user_id_train.append(user_id)\n",
    "        feature_dict = movie_feature_dict[m_id]\n",
    "        u_v = user_v[user_id]\n",
    "        \n",
    "        d_x = get_rating_list(feature_dict['d'], director_v, u_v,user_id, d_mat)\n",
    "        g_x = get_rating_list(feature_dict['g'], genre_v, u_v, user_id, g_mat)\n",
    "        t_x = get_rating_list(feature_dict['t'], topic_v, u_v, user_id, t_mat)\n",
    "        c_x = get_rating_list(feature_dict['c'], cast_v, u_v, user_id, c_mat)\n",
    "        #print(user_id, d_x, g_x, t_x, c_x)\n",
    "        \n",
    "        \n",
    "        one_user_x = d_x + g_x + t_x + c_x\n",
    "        #one_user_x = d_x + g_x + c_x\n",
    "        x_train.append(one_user_x)\n",
    "        \n",
    "        feature_name = []\n",
    "        feature_name += [\"d_\" + id_2_director[_id] for _id in feature_dict['d']]\n",
    "        feature_name += [\"g_\" + id_2_genre[_id] for _id in feature_dict['g']]\n",
    "        feature_name += [\"t_\" + str(_id) for _id in feature_dict['t']]\n",
    "        feature_name += [\"c_\" + id_2_cast[_id] for _id in feature_dict['c']]\n",
    "        feature_name += [\"bias\"]\n",
    "        \n",
    "        \n",
    "    return x_train, y_train, feature_name, user_id_train\n",
    "        \n",
    "            \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_count_list = pickle.load(open('../data/'+folder+'/user_count_list.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name_list = ['d', 't', 'c', 'g']\n",
    "divid_list = []\n",
    "for name in name_list:\n",
    "    summ = 0\n",
    "    total = 0\n",
    "    for user in user_count_list[name]:\n",
    "        for key in user_count_list[name][user]:\n",
    "            summ += user_count_list[name][user][key]\n",
    "            total += 1\n",
    "\n",
    "    divid_list.append(summ/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store result matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_d_result, _t_result, _g_result, _c_result =  sess.run([d_result, t_result, g_result, c_result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(_d_result, open('../data/'+folder+'/ranking_matrix_director.pkl', 'wb'))\n",
    "pickle.dump(_t_result, open('../data/'+folder+'/ranking_matrix_type.pkl', 'wb'))\n",
    "pickle.dump(_g_result, open('../data/'+folder+'/ranking_matrix_genre.pkl', 'wb'))\n",
    "pickle.dump(_c_result, open('../data/'+folder+'/ranking_matrix_cast.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "import numpy as np\n",
    "\n",
    "def select_feature_index(feature, n):\n",
    "    index_list = [0 for i in range(len(feature))]\n",
    "    temp_f = [abs(feature[i]) for i in range(len(feature))]\n",
    "    for i in (range(n)):\n",
    "        min_index = temp_f.index(max(temp_f))\n",
    "        index_list[min_index] = 1\n",
    "        temp_f[min_index] = 0\n",
    "    \n",
    "    return index_list\n",
    "\n",
    "def get_predict_rating_old(user_rating, features):\n",
    "    return np.mean([user_rating[i] + features[i] for i in range(len(user_rating))])\n",
    "\n",
    "\n",
    "def get_predict_rating(user_rating, features):\n",
    "    weight_n = [float(abs(ur)) for ur in user_rating]\n",
    "    summ = sum(weight_n)\n",
    "    weight = [ur/summ for ur in weight_n]\n",
    "    #weight = [1./len(weight_n) for ur in weight_n]\n",
    "    \n",
    "    weight_total = sum([weight[i]* (user_rating[i] + features[i]) for i in range(len(weight))])\n",
    "    \n",
    "    return weight_total\n",
    "\n",
    "def get_predict_rating_select(user_rating, features, n):\n",
    "    if len(user_rating) < n:\n",
    "        return get_predict_rating_old(user_rating, features)\n",
    "    else:\n",
    "        index_list = select_feature_index(user_rating, n)\n",
    "        account_list = []\n",
    "        for i in range(len(user_rating)):\n",
    "            if index_list[i] == 1:\n",
    "                account_list.append(user_rating[i] + features[i])\n",
    "        return np.mean(account_list)\n",
    "\n",
    "def get_predict_rating_count(user_rating, features, user_id, feature_name):\n",
    "    weight = []\n",
    "    weight_sum = 0\n",
    "    offset = 0.01\n",
    "    for j in range(len(feature_name)-1):\n",
    "        name = feature_name[j]\n",
    "        dict_name = name.split('_')[0]\n",
    "        feature_n = name.split('_')[1]\n",
    "        try:\n",
    "            if dict_name == 'd':\n",
    "                feature_key = director_2_id[feature_n]\n",
    "                offset = 1/divid_list[0]\n",
    "            elif dict_name == 'c':\n",
    "                feature_key = cast_2_id[feature_n]\n",
    "                offset = 1/divid_list[2]\n",
    "            elif dict_name == 'g':\n",
    "                feature_key = genre_2_id[feature_n]\n",
    "                offset = 1/divid_list[3]\n",
    "            elif dict_name == 't':\n",
    "                feature_key = int(feature_n)\n",
    "                offset = 1/divid_list[1]\n",
    "        except:\n",
    "            feature_key = ''\n",
    "        try:\n",
    "            prop = user_count_list[dict_name][user_id][feature_key]\n",
    "        except:\n",
    "            prop = 1\n",
    "        prop = offset * prop\n",
    "        weight_sum += prop\n",
    "        weight.append(prop)      \n",
    "    \n",
    "    weight = [w/float(weight_sum) for w in weight]\n",
    "    result = 0\n",
    "    for i in range(len(user_rating)):\n",
    "        #name = feature_name[i]\n",
    "        #dict_name = name.split('_')[0]\n",
    "        #ppr = (user_rating[i] + features[i]) * weight[i]\n",
    "        result += (user_rating[i] + features[i]) * weight[i]\n",
    "    \"\"\"\n",
    "    print(user_id)\n",
    "    for i in range(len(weight)):\n",
    "        print(feature_name[i], end='')\n",
    "        print(\" %.2f %.2f %.2f\" % (user_rating[i], features[i], weight[i]))\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equal weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_predict = np.array([])\n",
    "total_rating = np.array([])\n",
    "total_predict_v = np.array([])\n",
    "total_rating_v = np.array([])\n",
    "total_predict_tr = np.array([])\n",
    "total_rating_tr = np.array([])\n",
    "#train_rmse_compare_F = []\n",
    "for m in range(0, MOVIE_NUM+1):\n",
    "    if m not in movie_rating:\n",
    "        print(\"not in:\", m)\n",
    "        continue\n",
    "    # tuples is (userid, movierating)\n",
    "    user_rating_tups = movie_rating[m]\n",
    "    x_train, y_train, feature_name, user_id_train = gen_regr_data(int(m), user_rating_tups)\n",
    "    \n",
    "    if len(y_train) < 5:\n",
    "        print('less than 5:', m)\n",
    "        continue\n",
    "    #x_train = x_train[1:2]\n",
    "    #y_train = y_train[1:2]\n",
    "    \n",
    "    #if len(y_train) > 50:\n",
    "     #   continue\n",
    "    \n",
    "    if m in movie_rating_test:     \n",
    "        user_rating_tups_test = movie_rating_test[m]\n",
    "        x_test, y_test, _, user_id_test= gen_regr_data(int(m), user_rating_tups_test)\n",
    "    else:\n",
    "        x_test = []\n",
    "        y_test = []\n",
    "        user_id_test = []\n",
    "    \n",
    "    if m in movie_rating_validation:     \n",
    "        user_rating_tups_validation = movie_rating_validation[m]\n",
    "        x_valid, y_valid, _, user_id_valid= gen_regr_data(int(m), user_rating_tups_validation)\n",
    "    else:\n",
    "        x_valid = []\n",
    "        y_valid = []\n",
    "        user_id_valid = []\n",
    "        \n",
    "    print(len(x_train), len(x_test), len(x_valid))\n",
    "    feature = [0 for i in range(len(feature_name)-1)]\n",
    "    \n",
    "    for j in range(len(feature_name)-1):\n",
    "        name = feature_name[j]\n",
    "        dict_name = name.split('_')[0]\n",
    "        feature_n = name.split('_')[1]\n",
    "\n",
    "        \n",
    "        for i in range(len(x_train)):\n",
    "            feature[j] += (y_train[i] - x_train[i][j])\n",
    "        \n",
    "        feature[j] /= len(x_train)\n",
    "    \n",
    "    #weight = [1 - abs(f) for f in feature] if len(feature) > 0 else []\n",
    "    #select_indexs = select_feature_index(feature, 100)\n",
    "    #select_indexs = [1 for i in range(len(feature))]\n",
    "    #for i in range(len(feature)):\n",
    "        #print(feature_name[i], 1/(1-feature[i]))\n",
    "        #if select_indexs[i] == 1:\n",
    "            #print(feature_name[i], feature[i], weight[i])\n",
    "    #print()\n",
    "    \n",
    "\n",
    "    predict_t = []\n",
    "    predict_v = []\n",
    "    predict_tr = []\n",
    "    \n",
    "    for i in range(len(x_train)):\n",
    "        cc = get_predict_rating_old(x_train[i], feature)\n",
    "        #print(\"%.2f %.2f\" %(cc, y_train[i]))\n",
    "        #print()\n",
    "        predict_tr.append(cc)\n",
    "    \n",
    "    for i in range(len(x_test)):\n",
    "        cc = get_predict_rating_old(x_test[i], feature)\n",
    "        #print(\"%.2f %.2f\" %(cc, y_test[i]))\n",
    "        #print()\n",
    "        predict_t.append(cc)\n",
    "    \n",
    "\n",
    "    for i in range(len(x_valid)):\n",
    "        cc = get_predict_rating_old(x_valid[i], feature)\n",
    "        predict_v.append(cc)\n",
    "        #print(\"%.2f %.2f\" %(cc, y_valid[i]))\n",
    "        #print()\n",
    "     \n",
    "    train_rmse = np.sqrt(np.mean(np.square(np.subtract(predict_tr, y_train))))\n",
    "    valid_rmse = np.sqrt(np.mean(np.square(np.subtract(predict_v, y_valid))))\n",
    "    test_rmse = np.sqrt(np.mean(np.square(np.subtract(predict_t, y_test))))\n",
    "    \n",
    "    #print(m, train_rmse, valid_rmse, test_rmse)\n",
    "    #train_rmse_compare_F.append([m, len(y_train), len(y_valid),len(y_test), train_rmse, valid_rmse, test_rmse])\n",
    "    \n",
    "    total_predict = np.concatenate((total_predict, predict_t), axis = 0)\n",
    "    total_rating = np.concatenate((total_rating, y_test), axis = 0)\n",
    "    total_predict_v = np.concatenate((total_predict_v, predict_v), axis = 0)\n",
    "    total_rating_v = np.concatenate((total_rating_v, y_valid), axis = 0)\n",
    "    total_predict_tr = np.concatenate((total_predict_tr, predict_tr), axis = 0)\n",
    "    total_rating_tr = np.concatenate((total_rating_tr, y_train), axis = 0)\n",
    "    \n",
    "    #print(feature)\n",
    "    #print(feature_name)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rmse = np.sqrt(np.mean(np.square(np.subtract(total_predict, total_rating))))  \n",
    "print(\"train rmse:\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(total_predict)):\n",
    "    if total_predict[i] < 0:\n",
    "        total_predict[i] = 0.\n",
    "    if total_predict[i] > 5:\n",
    "        total_predict[i] = 5.\n",
    "\n",
    "for i in range(len(total_predict_v)):\n",
    "    if total_predict_v[i] < 0:\n",
    "        total_predict_v[i] = 0.\n",
    "    if total_predict_v[i] > 5:\n",
    "        total_predict_v[i] = 5.\n",
    "        \n",
    "for i in range(len(total_predict_tr)):\n",
    "    if total_predict_tr[i] < 0:\n",
    "        total_predict_tr[i] = 0.\n",
    "    if total_predict_tr[i] > 5:\n",
    "        total_predict_tr[i] = 5.\n",
    "\n",
    "        \n",
    "rmse = np.sqrt(np.mean(np.square(np.subtract(total_predict_tr, total_rating_tr))))  \n",
    "print(\"train rmse:\", rmse)\n",
    "\n",
    "mae = mean_absolute_error(total_rating_tr, total_predict_tr)\n",
    "print(\"train mae:\", mae)\n",
    "\n",
    "rmse = np.sqrt(np.mean(np.square(np.subtract(total_predict_v, total_rating_v))))  \n",
    "print(\"valid rmse:\", rmse)\n",
    "\n",
    "mae = mean_absolute_error(total_rating_v, total_predict_v)\n",
    "print(\"valid mae:\", mae)\n",
    "\n",
    "rmse = np.sqrt(np.mean(np.square(np.subtract(total_predict, total_rating))))  \n",
    "print(\"test rmse:\", rmse)\n",
    "\n",
    "mae = mean_absolute_error(total_rating, total_predict)\n",
    "print(\"test mae:\", mae)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# weight with number of featured movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_predict = np.array([])\n",
    "total_rating = np.array([])\n",
    "total_predict_v = np.array([])\n",
    "total_rating_v = np.array([])\n",
    "total_predict_tr = np.array([])\n",
    "total_rating_tr = np.array([])\n",
    "train_rmse_compare_E = []\n",
    "movie_feature = {}\n",
    "feature_name_dict = {}\n",
    "dropped_movie = 0\n",
    "for m in range(0, MOVIE_NUM+1):\n",
    "    if m not in movie_rating:\n",
    "        print('not in', m)\n",
    "        dropped_movie += 1\n",
    "        continue\n",
    "    user_rating_tups = movie_rating[m]\n",
    "    x_train, y_train, feature_name, user_id_train = gen_regr_data(int(m), user_rating_tups)\n",
    "    \n",
    "    if len(y_train) < 5:\n",
    "        print('less than 5:', m)\n",
    "        dropped_movie += 1\n",
    "        continue\n",
    "    #if len(y_train) > 50:\n",
    "     #   continue\n",
    "    \n",
    "    if m in movie_rating_test:     \n",
    "        user_rating_tups_test = movie_rating_test[m]\n",
    "        x_test, y_test, _, user_id_test= gen_regr_data(int(m), user_rating_tups_test)\n",
    "    else:\n",
    "        x_test = []\n",
    "        y_test = []\n",
    "        user_id_test = []\n",
    "    \n",
    "    if m in movie_rating_validation:     \n",
    "        user_rating_tups_validation = movie_rating_validation[m]\n",
    "        x_valid, y_valid, _, user_id_valid= gen_regr_data(int(m), user_rating_tups_validation)\n",
    "    else:\n",
    "        x_valid = []\n",
    "        y_valid = []\n",
    "        user_id_valid = []\n",
    "    \n",
    "    if len(x_test) == 0:\n",
    "        continue\n",
    "        \n",
    "    \n",
    "    feature = [0 for i in range(len(feature_name)-1)]\n",
    "    \n",
    "    for j in range(len(feature_name)-1):\n",
    "        prop_sum = 0\n",
    "        name = feature_name[j]\n",
    "        dict_name = name.split('_')[0]\n",
    "        feature_n = name.split('_')[1]\n",
    "        offset = 0.01\n",
    "        try:\n",
    "            if dict_name == 'd':\n",
    "                feature_key = director_2_id[feature_n]\n",
    "                offset = 1/divid_list[0]\n",
    "            elif dict_name == 'c':\n",
    "                feature_key = cast_2_id[feature_n]\n",
    "                offset = 1/divid_list[2]\n",
    "            elif dict_name == 'g':\n",
    "                feature_key = genre_2_id[feature_n]\n",
    "                offsedivid_listt = 1/divid_list[3]\n",
    "            elif dict_name == 't':\n",
    "                feature_key = int(feature_n)\n",
    "                offset = 1/divid_list[1]\n",
    "        except:\n",
    "            feature_key = ''\n",
    "        \n",
    "        for i in range(len(x_train)):\n",
    "            try:\n",
    "                prop = user_count_list[dict_name][user_id_train[i]][feature_key]\n",
    "            except:\n",
    "                prop = 1\n",
    "            prop = offset * prop\n",
    "            #prop_sum += prop\n",
    "            #feature[j] += (y_train[i] - x_train[i][j]) * prop\n",
    "            prop_sum += 1\n",
    "            feature[j] += (y_train[i] - x_train[i][j])\n",
    "        \n",
    "        feature[j] /= prop_sum\n",
    "    feature_name_dict[m] = feature_name\n",
    "    movie_feature[m] = feature\n",
    "    #weight = [1 - abs(f) for f in feature] if len(feature) > 0 else []\n",
    "    #select_indexs = select_feature_index(feature, 100)\n",
    "    #select_indexs = [1 for i in range(len(feature))]\n",
    "    #for i in range(len(feature)):\n",
    "        #print(feature_name[i], 1/(1-feature[i]))\n",
    "        #if select_indexs[i] == 1:\n",
    "            #print(feature_name[i], feature[i], weight[i])\n",
    "    #print()\n",
    "    \n",
    "\n",
    "    predict_t = []\n",
    "    predict_v = []\n",
    "    predict_tr = []\n",
    "    \n",
    "    for i in range(len(x_train)):\n",
    "        cc = get_predict_rating_count(x_train[i], feature, user_id_train[i], feature_name)\n",
    "        #print(\"%.2f %.2f\" %(cc, y_train[i]))\n",
    "        #print()\n",
    "        predict_tr.append(cc)\n",
    "    \n",
    "    for i in range(len(x_test)):\n",
    "        cc = get_predict_rating_count(x_test[i], feature, user_id_test[i], feature_name)\n",
    "        #print(\"%.2f %.2f\" %(cc, y_test[i]))\n",
    "        #print()\n",
    "        predict_t.append(cc)\n",
    "    \n",
    "\n",
    "    for i in range(len(x_valid)):\n",
    "        cc = get_predict_rating_count(x_valid[i], feature, user_id_valid[i], feature_name )\n",
    "        predict_v.append(cc)\n",
    "        #print(\"%.2f %.2f\" %(cc, y_valid[i]))\n",
    "        #print()\n",
    "        \n",
    "        \n",
    "    #train_rmse = np.sqrt(np.mean(np.square(np.subtract(predict_tr, y_train))))\n",
    "    #valid_rmse = np.sqrt(np.mean(np.square(np.subtract(predict_v, y_valid))))\n",
    "    #test_rmse = np.sqrt(np.mean(np.square(np.subtract(predict_t, y_test))))\n",
    "    \n",
    "    #print(m, train_rmse, valid_rmse, test_rmse)\n",
    "    \n",
    "    #print(m, train_rmse, valid_rmse, test_rmse)\n",
    "    #train_rmse_compare_E.append([m, len(y_train), len(y_valid),len(y_test), train_rmse, valid_rmse, test_rmse])\n",
    "    \n",
    "        \n",
    "    total_predict = np.concatenate((total_predict, predict_t), axis = 0)\n",
    "    total_rating = np.concatenate((total_rating, y_test), axis = 0)\n",
    "    total_predict_v = np.concatenate((total_predict_v, predict_v), axis = 0)\n",
    "    total_rating_v = np.concatenate((total_rating_v, y_valid), axis = 0)\n",
    "    total_predict_tr = np.concatenate((total_predict_tr, predict_tr), axis = 0)\n",
    "    total_rating_tr = np.concatenate((total_rating_tr, y_train), axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(total_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = 3\n",
    "for i in range(len(movie_feature[index])):\n",
    "    print(feature_name_dict[index][i], movie_feature[index][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(total_predict)):\n",
    "    if total_predict[i] < 0:\n",
    "        total_predict[i] = 0.\n",
    "    if total_predict[i] > 5:\n",
    "        total_predict[i] = 5.\n",
    "\n",
    "for i in range(len(total_predict_v)):\n",
    "    if total_predict_v[i] < 0:\n",
    "        total_predict_v[i] = 0.\n",
    "    if total_predict_v[i] > 5:\n",
    "        total_predict_v[i] = 5.\n",
    "        \n",
    "for i in range(len(total_predict_tr)):\n",
    "    if total_predict_tr[i] < 0:\n",
    "        total_predict_tr[i] = 0.\n",
    "    if total_predict_tr[i] > 5:\n",
    "        total_predict_tr[i] = 5.\n",
    "\n",
    "        \n",
    "rmse = np.sqrt(np.mean(np.square(np.subtract(total_predict_tr, total_rating_tr))))  \n",
    "print( rmse)\n",
    "\n",
    "mae = mean_absolute_error(total_rating_tr, total_predict_tr)\n",
    "print(mae)\n",
    "\n",
    "rmse = np.sqrt(np.mean(np.square(np.subtract(total_predict_v, total_rating_v))))  \n",
    "print(rmse)\n",
    "\n",
    "mae = mean_absolute_error(total_rating_v, total_predict_v)\n",
    "print( mae)\n",
    "\n",
    "\n",
    "rmse = np.sqrt(np.mean(np.square(np.subtract(total_predict, total_rating))))  \n",
    "print( rmse)\n",
    "\n",
    "mae = mean_absolute_error(total_rating, total_predict)\n",
    "print(mae)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get one user's information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_id = 1\n",
    "feature_dict = ['d','t','c', 'g']\n",
    "one_user_count_list = {}\n",
    "for type_name in feature_dict:\n",
    "    one_user_count_list[type_name] = {}\n",
    "for i in range(len(feature_dict)):\n",
    "    dict_name = feature_dict[i]\n",
    "    divid_value = divid_list[i]\n",
    "    for feature_key in user_count_list[dict_name][user_id]:\n",
    "        one_user_count_list[dict_name][feature_key] = user_count_list[dict_name][user_id][feature_key]/divid_value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_movie_rating = {}\n",
    "user_movie_info = {}\n",
    "\n",
    "for m in range(0, MOVIE_NUM+1):\n",
    "    if m not in movie_rating:\n",
    "        print('not in', m)\n",
    "        dropped_movie += 1\n",
    "        continue\n",
    "    user_rating_tups = movie_rating[m]\n",
    "    x_train, y_train, feature_name, user_id_train = gen_regr_data(int(m), user_rating_tups)\n",
    "    if len(y_train) < 5:\n",
    "        print('less than 5:', m)\n",
    "        dropped_movie += 1\n",
    "        continue\n",
    "    \n",
    "    one_uesr_ratings = []\n",
    "    one_user_info = []\n",
    "    weight = []\n",
    "    x_pred, _, feature_name, user_id_pred = gen_regr_data(int(m), [(user_id, 0)])\n",
    "    if user_id in user_id_train:\n",
    "        ind = user_id_train.index(user_id)\n",
    "        real_rating = y_train[ind]\n",
    "    else:\n",
    "        real_rating = -1\n",
    "    for j in range(len(feature_name)-1):\n",
    "        prop_sum = 0\n",
    "        name = feature_name[j]\n",
    "        dict_name = name.split('_')[0]\n",
    "        feature_n = name.split('_')[1]\n",
    "        offset = 0.01\n",
    "        try:\n",
    "            if dict_name == 'd':\n",
    "                feature_key = director_2_id[feature_n]\n",
    "            elif dict_name == 'c':\n",
    "                feature_key = cast_2_id[feature_n]\n",
    "            elif dict_name == 'g':\n",
    "                feature_key = genre_2_id[feature_n]\n",
    "            elif dict_name == 't':\n",
    "                feature_key = int(feature_n)\n",
    "        except:\n",
    "            feature_key = ''\n",
    "        try:\n",
    "            weight.append(one_user_count_list[dict_name][feature_key])\n",
    "        except:\n",
    "            # if user have never seen this information\n",
    "            weight.append(0.1)\n",
    "\n",
    "    weight_normal = [w/sum(weight) for w in weight]\n",
    "    print('begin')\n",
    "    try:\n",
    "        one_pred_value = sum([weight_normal[i]*(x_pred[0][i] + movie_feature[m][i]) for i in range(len(x_pred[0]))])\n",
    "    except:\n",
    "        continue\n",
    "    feature_name_dict\n",
    "    \"\"\"\n",
    "    print(\"movie:\", m)\n",
    "    print(\"pred:\", one_pred_value)\n",
    "    print(\"real:\", real_rating)\n",
    "    \n",
    "    for i in range(len(x_pred[0])):\n",
    "        print(feature_name_dict[m][i], x_pred[0][i], movie_feature[m][i], weight_normal[i])\n",
    "    \"\"\"\n",
    "    user_movie_rating[m] = (one_pred_value, real_rating)\n",
    "    user_movie_info[m] = {}\n",
    "    user_movie_info[m]['info_name'] = feature_name_dict[m][0:-1]\n",
    "    user_movie_info[m]['info_rating'] = x_pred[0]\n",
    "    user_movie_info[m]['movie_offset'] = movie_feature[m]\n",
    "    user_movie_info[m]['weight'] = weight_normal\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for m in user_movie_rating:\n",
    "    if user_movie_rating[m][1] > 0:\n",
    "        print(m, user_movie_rating[m][1])\n",
    "        length = len(user_movie_info[m]['info_rating'])\n",
    "        for i in range((length)):\n",
    "            print(user_movie_info[m]['info_name'][i], user_movie_info[m]['info_rating'][i] + user_movie_info[m]['movie_offset'][i], user_movie_info[m]['weight'][i])\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_movie = -1\n",
    "best_rating = 0\n",
    "for m in user_movie_rating:\n",
    "    if user_movie_rating[m][1] < 0 and user_movie_rating[m][0] > best_rating:\n",
    "        best_rating = user_movie_rating[m][0]\n",
    "        best_movie = m\n",
    "\n",
    "best_movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_movie = 1817\n",
    "print('rating:',user_movie_rating[best_movie])\n",
    "length = len(user_movie_info[best_movie]['info_rating'])\n",
    "for i in range((length)):\n",
    "    part = (user_movie_info[best_movie]['info_rating'][i] + user_movie_info[best_movie]['movie_offset'][i]) * user_movie_info[best_movie]['weight'][i]\n",
    "    print(user_movie_info[best_movie]['info_name'][i], user_movie_info[best_movie]['info_rating'][i], user_movie_info[best_movie]['movie_offset'][i], user_movie_info[best_movie]['weight'][i], part)\n",
    "    #print(user_movie_info[best_movie]['info_name'][i])\n",
    "    \n",
    "    #print(part)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ws3.6",
   "language": "python",
   "name": "ws"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
