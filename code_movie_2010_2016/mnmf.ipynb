{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie: 206\n",
      "user: 3550\n",
      "director: 194\n",
      "genre: 20\n",
      "topic: 50\n",
      "cast: 474\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nMOVIE_NUM = 1682\\nDIRECTOR_NUM = 1139\\nGENRE_NUM = 24\\nTOPIC_NUM = 50\\nCAST_NUM = 2894\\nUSER_NUM = 943\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Global values\n",
    "\"\"\"\n",
    "folder = 'boxoffice_10'\n",
    "\n",
    "len_dtcgum = pickle.load(open('../data/'+folder+'/len_dtcgum.pkl', 'rb'))\n",
    "\n",
    "FEATURE_LEN = 5\n",
    "\n",
    "MOVIE_NUM = len_dtcgum[5]\n",
    "DIRECTOR_NUM = len_dtcgum[0]\n",
    "GENRE_NUM = len_dtcgum[3]\n",
    "TOPIC_NUM = 50\n",
    "CAST_NUM = len_dtcgum[2]\n",
    "USER_NUM = len_dtcgum[4]\n",
    "\n",
    "print('movie:', MOVIE_NUM)\n",
    "print('user:', USER_NUM)\n",
    "print('director:', DIRECTOR_NUM)\n",
    "print('genre:', GENRE_NUM)\n",
    "print('topic:', TOPIC_NUM)\n",
    "print('cast:', CAST_NUM)\n",
    "\n",
    "\"\"\"\n",
    "MOVIE_NUM = 1682\n",
    "DIRECTOR_NUM = 1139\n",
    "GENRE_NUM = 24\n",
    "TOPIC_NUM = 50\n",
    "CAST_NUM = 2894\n",
    "USER_NUM = 943\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3550"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USER_NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movie_rating = pickle.load(open('../data/'+folder+'/movie_rating.pkl', 'rb'))\n",
    "movie_rating_test = pickle.load(open('../data/'+folder+'/movie_rating_test.pkl', 'rb'))\n",
    "movie_rating_validation = pickle.load(open('../data/'+folder+'/movie_rating_validation.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(user_name, movie_name, rating_name, last_same):\n",
    "    user = pickle.load(open('../data/'+folder+'/'+user_name+last_same+'.pkl', 'rb'))\n",
    "    movie = pickle.load(open('../data/'+folder+'/'+movie_name+last_same+'.pkl', 'rb'))\n",
    "    rating = pickle.load(open('../data/'+folder+'/'+rating_name+last_same+'.pkl', 'rb'))\n",
    "    return shuffle(user, movie, rating)\n",
    "\n",
    "#load training data\n",
    "d_user, d_movie, d_rating = load_data(\"train_user\", \"train_movie\", \"train_rating\", '_d')\n",
    "t_user, t_movie, t_rating = load_data(\"train_user\", \"train_movie\", \"train_rating\", '_t')\n",
    "g_user, g_movie, g_rating = load_data(\"train_user\", \"train_movie\", \"train_rating\", '_g')   \n",
    "c_user, c_movie, c_rating = load_data(\"train_user\", \"train_movie\", \"train_rating\", '_c')\n",
    "\n",
    "#load validation data\n",
    "d_user_v, d_movie_v, d_rating_v = load_data(\"train_user\", \"train_movie\", \"train_rating\", '_d_v')\n",
    "t_user_v, t_movie_v, t_rating_v = load_data(\"train_user\", \"train_movie\", \"train_rating\", '_t_v')\n",
    "g_user_v, g_movie_v, g_rating_v = load_data(\"train_user\", \"train_movie\", \"train_rating\", '_g_v')        \n",
    "c_user_v, c_movie_v, c_rating_v = load_data(\"train_user\", \"train_movie\", \"train_rating\", '_c_v')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "U = tf.Variable(initial_value=tf.truncated_normal([USER_NUM, FEATURE_LEN]), name='users')\n",
    "D = tf.Variable(initial_value=tf.truncated_normal([FEATURE_LEN, DIRECTOR_NUM]), name='directors')\n",
    "T = tf.Variable(initial_value=tf.truncated_normal([FEATURE_LEN, TOPIC_NUM]), name='topics')\n",
    "G = tf.Variable(initial_value=tf.truncated_normal([FEATURE_LEN, GENRE_NUM]), name='genures')\n",
    "C = tf.Variable(initial_value=tf.truncated_normal([FEATURE_LEN, CAST_NUM]), name='casts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_result = tf.matmul(U, D)\n",
    "t_result = tf.matmul(U, T)\n",
    "g_result = tf.matmul(U, G)\n",
    "c_result = tf.matmul(U, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clip_U = U.assign(tf.maximum(tf.zeros_like(U), U))\n",
    "clip_D = D.assign(tf.maximum(tf.zeros_like(D), D))\n",
    "clip_T = T.assign(tf.maximum(tf.zeros_like(T), T))\n",
    "clip_C = C.assign(tf.maximum(tf.zeros_like(C), C))\n",
    "clip_G = G.assign(tf.maximum(tf.zeros_like(G), G))\n",
    "clip = tf.group(clip_U, clip_D,clip_T,clip_C,clip_G )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MatMul_6:0' shape=(3550, 20) dtype=float32>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_result_flatten = tf.reshape(d_result, [-1])\n",
    "t_result_flatten = tf.reshape(t_result, [-1])\n",
    "g_result_flatten = tf.reshape(g_result, [-1])\n",
    "c_result_flatten = tf.reshape(c_result, [-1])\n",
    "\n",
    "D_R = tf.gather(d_result_flatten, d_user * tf.shape(d_result)[1] + d_movie, name='director_user_rate')\n",
    "T_R = tf.gather(t_result_flatten, t_user * tf.shape(t_result)[1] + t_movie, name='topic_user_rate')\n",
    "G_R = tf.gather(g_result_flatten, g_user * tf.shape(g_result)[1] + g_movie, name='genre_user_rate')\n",
    "C_R = tf.gather(c_result_flatten, c_user * tf.shape(c_result)[1] + c_movie, name='cast_user_rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_diff_op = tf.subtract(D_R, d_rating, name='d_trainig_diff')\n",
    "t_diff_op = tf.subtract(T_R, t_rating, name='t_trainig_diff')\n",
    "g_diff_op = tf.subtract(G_R, g_rating, name='g_trainig_diff')\n",
    "c_diff_op = tf.subtract(C_R, c_rating, name='c_trainig_diff')\n",
    "\n",
    "d_diff_op_squared = tf.abs(d_diff_op, name=\"d_squared_difference\")\n",
    "t_diff_op_squared = tf.abs(t_diff_op, name=\"t_squared_difference\")\n",
    "g_diff_op_squared = tf.abs(g_diff_op, name=\"g_squared_difference\")\n",
    "c_diff_op_squared = tf.abs(c_diff_op, name=\"c_squared_difference\")\n",
    "\n",
    "#\"\"\"\n",
    "d_base_cost = tf.reduce_sum(d_diff_op_squared, name=\"d_sum_squared_error\")/ tf.cast(tf.shape(d_diff_op_squared)[0], tf.float32) * 10000 \n",
    "t_base_cost = tf.reduce_sum(t_diff_op_squared, name=\"t_sum_squared_error\")/ tf.cast(tf.shape(t_diff_op_squared)[0], tf.float32) * 10000\n",
    "g_base_cost = tf.reduce_sum(g_diff_op_squared, name=\"g_sum_squared_error\")/ tf.cast(tf.shape(g_diff_op_squared)[0], tf.float32) * 10000\n",
    "c_base_cost = tf.reduce_sum(c_diff_op_squared, name=\"c_sum_squared_error\")/ tf.cast(tf.shape(c_diff_op_squared)[0], tf.float32) * 10000\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "d_base_cost = tf.reduce_sum(d_diff_op_squared, name=\"d_sum_squared_error\")/30.\n",
    "t_base_cost = tf.reduce_sum(t_diff_op_squared, name=\"t_sum_squared_error\")/30.\n",
    "g_base_cost = tf.reduce_sum(g_diff_op_squared, name=\"g_sum_squared_error\")/30.\n",
    "c_base_cost = tf.reduce_sum(c_diff_op_squared, name=\"c_sum_squared_error\")/30.\n",
    "\"\"\"\n",
    "\n",
    "base_cost = d_base_cost + t_base_cost + g_base_cost + c_base_cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = tf.constant(.001, name='lambda')\n",
    "\n",
    "\"\"\"\n",
    "u_norm = tf.reduce_sum(tf.abs(U, name='user_abs'), name='user_norm') / tf.cast(tf.shape(U)[0], tf.float32)\n",
    "d_norm = tf.reduce_sum(tf.abs(D, name='director_abs'), name='director_norm')/ tf.cast(tf.shape(D)[1], tf.float32)\n",
    "t_norm = tf.reduce_sum(tf.abs(T, name='topic_abs'), name='topic_norm')/ tf.cast(tf.shape(T)[1], tf.float32)\n",
    "g_norm = tf.reduce_sum(tf.abs(G, name='genre_abs'), name='genre_norm')/ tf.cast(tf.shape(G)[1], tf.float32)\n",
    "c_norm = tf.reduce_sum(tf.abs(C, name='cast_abs'), name='cast_norm')/ tf.cast(tf.shape(C)[1], tf.float32)\n",
    "\"\"\"\n",
    "u_norm = tf.reduce_sum(tf.abs(U, name='user_abs'), name='user_norm') \n",
    "d_norm = tf.reduce_sum(tf.abs(D, name='director_abs'), name='director_norm')\n",
    "t_norm = tf.reduce_sum(tf.abs(T, name='topic_abs'), name='topic_norm')\n",
    "g_norm = tf.reduce_sum(tf.abs(G, name='genre_abs'), name='genre_norm')\n",
    "c_norm = tf.reduce_sum(tf.abs(C, name='cast_abs'), name='cast_norm')\n",
    "\n",
    "norm_sums = u_norm + d_norm + t_norm + g_norm + c_norm\n",
    "regularizer = tf.multiply(norm_sums, lda, 'regularizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cost = tf.add(base_cost, regularizer)\n",
    "cost = base_cost + regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = tf.constant(.001, name='learning_rate')\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = tf.train.exponential_decay(lr, global_step, 10000, 0.96, staircase=True)\n",
    "#learning_rate = 0.01\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "training_step = optimizer.minimize(cost, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate Training RMSE\n",
    "\"\"\"\n",
    "g_user_test = g_user\n",
    "g_item_test = g_movie\n",
    "g_rate_test = g_rating\n",
    "g_result_flatten = g_result_flatten\n",
    "#genre rmse\n",
    "g_test = tf.gather(g_result_flatten, g_user_test * tf.shape(g_result)[1] + g_item_test, name='g_test')\n",
    "g_rmse = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(g_rate_test, g_test))))\n",
    "\n",
    "d_user_test = d_user\n",
    "d_item_test = d_movie\n",
    "d_rate_test = d_rating\n",
    "d_result_flatten = d_result_flatten\n",
    "#genre rmse\n",
    "d_test = tf.gather(d_result_flatten, d_user_test * tf.shape(d_result)[1] + d_item_test, name='d_test')\n",
    "d_rmse = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(d_rate_test, d_test))))\n",
    "\n",
    "t_user_test = t_user\n",
    "t_item_test = t_movie\n",
    "t_rate_test = t_rating\n",
    "t_result_flatten = t_result_flatten\n",
    "#genre rmse\n",
    "t_test = tf.gather(t_result_flatten, t_user_test * tf.shape(t_result)[1] + t_item_test, name='t_test')\n",
    "t_rmse = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(t_rate_test, t_test))))\n",
    "\n",
    "c_user_test = c_user\n",
    "c_item_test = c_movie\n",
    "c_rate_test = c_rating\n",
    "c_result_flatten = c_result_flatten\n",
    "#genre rmse\n",
    "c_test = tf.gather(c_result_flatten, c_user_test * tf.shape(c_result)[1] + c_item_test, name='c_test')\n",
    "c_rmse = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(c_rate_test, c_test))))\n",
    "\n",
    "\n",
    "rmse = tf.divide((g_rmse + d_rmse + t_rmse + c_rmse), 4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate Validation RMSE\n",
    "\"\"\"\n",
    "g_user_v = g_user_v\n",
    "g_item_v = g_movie_v\n",
    "g_rate_v = g_rating_v\n",
    "g_result_flatten = g_result_flatten\n",
    "#genre rmse\n",
    "g_v = tf.gather(g_result_flatten, g_user_v * tf.shape(g_result)[1] + g_item_v, name='g_v')\n",
    "g_rmse_v = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(g_rate_v, g_v))))\n",
    "\n",
    "d_user_v = d_user_v\n",
    "d_item_v = d_movie_v\n",
    "d_rate_v = d_rating_v\n",
    "d_result_flatten = d_result_flatten\n",
    "#genre rmse\n",
    "d_v = tf.gather(d_result_flatten, d_user_v * tf.shape(d_result)[1] + d_item_v, name='d_v')\n",
    "d_rmse_v = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(d_rate_v, d_v))))\n",
    "\n",
    "t_user_v = t_user_v\n",
    "t_item_v = t_movie_v\n",
    "t_rate_v = t_rating_v\n",
    "t_result_flatten = t_result_flatten\n",
    "#genre rmse\n",
    "t_v = tf.gather(t_result_flatten, t_user_v * tf.shape(t_result)[1] + t_item_v, name='t_v')\n",
    "t_rmse_v = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(t_rate_v, t_v))))\n",
    "\n",
    "c_user_v = c_user_v\n",
    "c_item_v = c_movie_v\n",
    "c_rate_v = c_rating_v\n",
    "c_result_flatten = c_result_flatten\n",
    "#genre rmse\n",
    "c_v = tf.gather(c_result_flatten, c_user_v * tf.shape(c_result)[1] + c_item_v, name='c_v')\n",
    "c_rmse_v = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(c_rate_v, c_v))))\n",
    "\n",
    "\n",
    "rmse_v = tf.divide((g_rmse_v + d_rmse_v + t_rmse_v + c_rmse_v), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(g_user_v)):\n",
    "    if g_user_v[i] < 0:\n",
    "        print(i, g_item_v[i], g_user_v[i])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 143485.0 current rmsr:  3.99981 lowest rmse:  3.99981 lowest num:  0\n",
      "3.99372 4.00191 3.9591 4.04451\n",
      "\n",
      "Traing RMSR(total, g, d, t, c): 0 3.10277 2.97176 3.16776 3.18583 3.08572\n",
      "\n",
      "20 56095.6 current rmsr:  1.67823 lowest rmse:  1.67823 lowest num:  20\n",
      "1.38114 1.73208 1.40098 2.19874\n",
      "40 31499.1 current rmsr:  1.10693 lowest rmse:  1.10693 lowest num:  40\n",
      "0.939677 1.0801 0.954689 1.45324\n",
      "60 25035.0 current rmsr:  0.935284 lowest rmse:  0.935284 lowest num:  60\n",
      "0.881764 0.895485 0.842716 1.12117\n",
      "80 23080.9 current rmsr:  0.879586 lowest rmse:  0.879586 lowest num:  80\n",
      "0.880512 0.835822 0.815216 0.986793\n",
      "100 22279.8 current rmsr:  0.855711 lowest rmse:  0.855711 lowest num:  100\n",
      "0.882498 0.811601 0.807744 0.921\n",
      "120 21820.4 current rmsr:  0.842458 lowest rmse:  0.842458 lowest num:  120\n",
      "0.882959 0.799008 0.805277 0.882588\n",
      "140 21514.5 current rmsr:  0.833814 lowest rmse:  0.833814 lowest num:  140\n",
      "0.881443 0.79141 0.804312 0.858093\n",
      "160 21279.0 current rmsr:  0.827694 lowest rmse:  0.827694 lowest num:  160\n",
      "0.878932 0.786523 0.803602 0.841721\n",
      "180 21151.6 current rmsr:  0.823754 lowest rmse:  0.823754 lowest num:  180\n",
      "0.878114 0.783224 0.803526 0.830153\n",
      "200 21034.4 current rmsr:  0.820523 lowest rmse:  0.820523 lowest num:  200\n",
      "0.87595 0.780858 0.803466 0.821817\n",
      "\n",
      "Traing RMSR(total, g, d, t, c): 200 0.718165 0.661278 0.72868 0.709692 0.773008\n",
      "\n",
      "220 20905.8 current rmsr:  0.817878 lowest rmse:  0.817878 lowest num:  220\n",
      "0.873685 0.779026 0.803361 0.81544\n",
      "240 20806.2 current rmsr:  0.81541 lowest rmse:  0.81541 lowest num:  240\n",
      "0.870548 0.77759 0.803162 0.810339\n",
      "260 20760.3 current rmsr:  0.81379 lowest rmse:  0.81379 lowest num:  260\n",
      "0.869413 0.776425 0.803217 0.806107\n",
      "280 20668.2 current rmsr:  0.811952 lowest rmse:  0.811952 lowest num:  280\n",
      "0.866674 0.775474 0.803045 0.802613\n",
      "300 20600.2 current rmsr:  0.810154 lowest rmse:  0.810154 lowest num:  300\n",
      "0.862976 0.77476 0.803144 0.799738\n",
      "320 20534.6 current rmsr:  0.808803 lowest rmse:  0.808803 lowest num:  320\n",
      "0.860791 0.774133 0.803057 0.79723\n",
      "340 20488.2 current rmsr:  0.807606 lowest rmse:  0.807606 lowest num:  340\n",
      "0.858728 0.773602 0.803039 0.795056\n",
      "360 20435.3 current rmsr:  0.80661 lowest rmse:  0.807606 lowest num:  340\n",
      "0.856971 0.773225 0.803036 0.793209\n",
      "380 20391.0 current rmsr:  0.805332 lowest rmse:  0.805332 lowest num:  380\n",
      "0.853779 0.772875 0.80314 0.791533\n",
      "400 20327.3 current rmsr:  0.804203 lowest rmse:  0.804203 lowest num:  400\n",
      "0.850952 0.77263 0.803125 0.790103\n",
      "\n",
      "Traing RMSR(total, g, d, t, c): 400 0.699613 0.633929 0.719524 0.709119 0.73588\n",
      "\n",
      "420 20258.6 current rmsr:  0.803011 lowest rmse:  0.803011 lowest num:  420\n",
      "0.847788 0.772402 0.803028 0.788823\n",
      "440 20215.8 current rmsr:  0.801946 lowest rmse:  0.801946 lowest num:  440\n",
      "0.844696 0.772268 0.803111 0.787708\n",
      "460 20159.8 current rmsr:  0.80131 lowest rmse:  0.801946 lowest num:  440\n",
      "0.843392 0.772132 0.802989 0.786726\n",
      "480 20119.6 current rmsr:  0.800369 lowest rmse:  0.800369 lowest num:  480\n",
      "0.840696 0.77199 0.802976 0.785814\n",
      "500 20103.8 current rmsr:  0.799985 lowest rmse:  0.800369 lowest num:  480\n",
      "0.83974 0.77194 0.803198 0.785062\n",
      "520 20062.2 current rmsr:  0.7992 lowest rmse:  0.7992 lowest num:  520\n",
      "0.837409 0.771869 0.80314 0.78438\n",
      "540 20028.4 current rmsr:  0.798619 lowest rmse:  0.7992 lowest num:  520\n",
      "0.835472 0.771955 0.803209 0.783842\n",
      "560 19999.9 current rmsr:  0.797979 lowest rmse:  0.797979 lowest num:  560\n",
      "0.833377 0.77195 0.803286 0.783301\n",
      "580 19951.8 current rmsr:  0.797435 lowest rmse:  0.797979 lowest num:  560\n",
      "0.83154 0.772019 0.803291 0.782887\n",
      "600 19901.1 current rmsr:  0.796595 lowest rmse:  0.796595 lowest num:  600\n",
      "0.82853 0.772072 0.803274 0.782503\n",
      "\n",
      "Traing RMSR(total, g, d, t, c): 600 0.688986 0.607951 0.715761 0.707683 0.72455\n",
      "\n",
      "620 19881.4 current rmsr:  0.796441 lowest rmse:  0.796595 lowest num:  600\n",
      "0.827969 0.772156 0.803475 0.782164\n",
      "640 19831.9 current rmsr:  0.795689 lowest rmse:  0.796595 lowest num:  600\n",
      "0.825245 0.772255 0.803404 0.781851\n",
      "660 19825.3 current rmsr:  0.795355 lowest rmse:  0.795355 lowest num:  660\n",
      "0.82393 0.772323 0.803587 0.78158\n",
      "680 19759.5 current rmsr:  0.794589 lowest rmse:  0.795355 lowest num:  660\n",
      "0.82095 0.772399 0.80363 0.781375\n",
      "700 19699.0 current rmsr:  0.793955 lowest rmse:  0.793955 lowest num:  700\n",
      "0.818873 0.772428 0.803323 0.781198\n",
      "720 19674.8 current rmsr:  0.793562 lowest rmse:  0.793955 lowest num:  700\n",
      "0.817075 0.772609 0.80347 0.781093\n",
      "740 19674.2 current rmsr:  0.793652 lowest rmse:  0.793955 lowest num:  700\n",
      "0.817221 0.772736 0.803728 0.780924\n",
      "760 19611.0 current rmsr:  0.792775 lowest rmse:  0.792775 lowest num:  760\n",
      "0.814168 0.772813 0.803315 0.780803\n",
      "780 19589.0 current rmsr:  0.792643 lowest rmse:  0.792775 lowest num:  760\n",
      "0.813133 0.773076 0.803569 0.780794\n",
      "800 19559.5 current rmsr:  0.792424 lowest rmse:  0.792775 lowest num:  760\n",
      "0.81225 0.773203 0.80356 0.780684\n",
      "\n",
      "Traing RMSR(total, g, d, t, c): 800 0.680693 0.587499 0.711909 0.705006 0.718357\n",
      "\n",
      "820 19536.4 current rmsr:  0.791952 lowest rmse:  0.792775 lowest num:  760\n",
      "0.810128 0.773404 0.803641 0.780634\n",
      "840 19498.6 current rmsr:  0.791632 lowest rmse:  0.791632 lowest num:  840\n",
      "0.808488 0.773576 0.803856 0.780607\n",
      "860 19475.7 current rmsr:  0.791575 lowest rmse:  0.791632 lowest num:  840\n",
      "0.808094 0.773731 0.80392 0.780554\n",
      "880 19456.6 current rmsr:  0.791354 lowest rmse:  0.791632 lowest num:  840\n",
      "0.806777 0.773924 0.804153 0.780564\n",
      "900 19428.3 current rmsr:  0.791254 lowest rmse:  0.791632 lowest num:  840\n",
      "0.806135 0.774085 0.804285 0.780512\n",
      "920 19414.0 current rmsr:  0.791056 lowest rmse:  0.791632 lowest num:  840\n",
      "0.80507 0.774235 0.804433 0.780487\n",
      "940 19385.4 current rmsr:  0.790783 lowest rmse:  0.791632 lowest num:  840\n",
      "0.80375 0.774392 0.804525 0.780465\n",
      "960 19362.0 current rmsr:  0.790798 lowest rmse:  0.791632 lowest num:  840\n",
      "0.803151 0.774652 0.804866 0.780525\n",
      "980 19332.3 current rmsr:  0.790616 lowest rmse:  0.790616 lowest num:  980\n",
      "0.802403 0.774735 0.804842 0.780486\n",
      "1000 19298.6 current rmsr:  0.790315 lowest rmse:  0.790616 lowest num:  980\n",
      "0.800838 0.774926 0.805017 0.780478\n",
      "\n",
      "Traing RMSR(total, g, d, t, c): 1000 0.674306 0.573837 0.707572 0.702621 0.713195\n",
      "\n",
      "1020 19264.1 current rmsr:  0.789999 lowest rmse:  0.790616 lowest num:  980\n",
      "0.799291 0.775097 0.805145 0.780463\n",
      "1040 19227.4 current rmsr:  0.789823 lowest rmse:  0.790616 lowest num:  980\n",
      "0.798151 0.775281 0.805358 0.780501\n",
      "1060 19210.1 current rmsr:  0.789695 lowest rmse:  0.790616 lowest num:  980\n",
      "0.797391 0.775435 0.805463 0.780492\n",
      "1080 19192.2 current rmsr:  0.789632 lowest rmse:  0.790616 lowest num:  980\n",
      "0.796665 0.775623 0.805685 0.780556\n",
      "1100 19172.7 current rmsr:  0.78961 lowest rmse:  0.78961 lowest num:  1100\n",
      "0.796245 0.775783 0.80584 0.780571\n",
      "1120 19153.6 current rmsr:  0.789445 lowest rmse:  0.78961 lowest num:  1100\n",
      "0.795508 0.775835 0.805915 0.780523\n",
      "1140 19099.3 current rmsr:  0.78916 lowest rmse:  0.78961 lowest num:  1100\n",
      "0.794009 0.775991 0.806077 0.780563\n",
      "1160 19104.6 current rmsr:  0.789252 lowest rmse:  0.78961 lowest num:  1100\n",
      "0.793935 0.776135 0.806332 0.780607\n",
      "1180 19047.1 current rmsr:  0.788695 lowest rmse:  0.78961 lowest num:  1100\n",
      "0.791452 0.776266 0.806441 0.780619\n",
      "1200 19025.9 current rmsr:  0.788782 lowest rmse:  0.78961 lowest num:  1100\n",
      "0.791255 0.776452 0.806738 0.780682\n",
      "\n",
      "Traing RMSR(total, g, d, t, c): 1200 0.667614 0.56126 0.702085 0.69938 0.707732\n",
      "\n",
      "1220 18997.5 current rmsr:  0.788401 lowest rmse:  0.788401 lowest num:  1220\n",
      "0.789899 0.776465 0.806659 0.78058\n",
      "1240 18996.6 current rmsr:  0.788894 lowest rmse:  0.788401 lowest num:  1220\n",
      "0.791458 0.776626 0.806866 0.780626\n",
      "1260 18952.5 current rmsr:  0.78866 lowest rmse:  0.788401 lowest num:  1220\n",
      "0.790033 0.776809 0.807121 0.780677\n",
      "1280 18940.9 current rmsr:  0.78861 lowest rmse:  0.788401 lowest num:  1220\n",
      "0.789628 0.776903 0.807264 0.780645\n",
      "1300 18911.1 current rmsr:  0.788489 lowest rmse:  0.788401 lowest num:  1220\n",
      "0.788786 0.777067 0.807406 0.780697\n",
      "1320 18891.7 current rmsr:  0.788614 lowest rmse:  0.788401 lowest num:  1220\n",
      "0.78875 0.777258 0.807693 0.780756\n",
      "1340 18849.9 current rmsr:  0.788404 lowest rmse:  0.788401 lowest num:  1220\n",
      "0.7876 0.777391 0.807848 0.780776\n",
      "1360 18841.8 current rmsr:  0.788297 lowest rmse:  0.788401 lowest num:  1220\n",
      "0.787061 0.777506 0.807863 0.780758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1380 18804.0 current rmsr:  0.788175 lowest rmse:  0.788401 lowest num:  1220\n",
      "0.786243 0.777628 0.808046 0.780784\n",
      "1400 18767.7 current rmsr:  0.788077 lowest rmse:  0.788401 lowest num:  1220\n",
      "0.785294 0.777824 0.808345 0.780844\n",
      "\n",
      "Traing RMSR(total, g, d, t, c): 1400 0.66105 0.55203 0.695481 0.695149 0.701538\n",
      "\n",
      "1420 18753.8 current rmsr:  0.788208 lowest rmse:  0.788401 lowest num:  1220\n",
      "0.785668 0.777923 0.808405 0.780838\n"
     ]
    }
   ],
   "source": [
    "#training_trend_f = open('../data/'+folder+'/trends.csv', 'w')\n",
    "#csv_trend = csv.writer(training_trend_f)\n",
    "#headers = ['round','train_total', 'train_g', 'train_d', 'train_t', 'train_c', 'valid_total', 'valid_g', 'valid_d', 'valid_t', 'valid_c']\n",
    "#csv_trend.writerow(headers)\n",
    "lowest_num = 0\n",
    "lowest_rmse = 9999999\n",
    "not_dec_num = 0\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "for i in range(100001):\n",
    "    _, c, r = sess.run([training_step, cost, regularizer])\n",
    "    if i%20 == 0:\n",
    "        _rmse_v, _g_rmse_v, _d_rmse_v, _t_rmse_v, _c_rmse_v  = sess.run([rmse_v, g_rmse_v, d_rmse_v, t_rmse_v, c_rmse_v])\n",
    "        sess.run(clip)\n",
    "        if lowest_rmse > _rmse_v + 0.001:\n",
    "            lowest_rmse = _rmse_v\n",
    "            lowest_num = i\n",
    "            not_dec_num = 0\n",
    "        else:\n",
    "            not_dec_num += 1\n",
    "            if not_dec_num > 10:\n",
    "                break\n",
    "            \n",
    "        print(i, c, \"current rmsr: \", _rmse_v,  \"lowest rmse: \", lowest_rmse, \"lowest num: \", lowest_num)\n",
    "        print(_g_rmse_v, _d_rmse_v, _t_rmse_v, _c_rmse_v)\n",
    "        _rmse, _g_rmse, _d_rmse, _t_rmse, _c_rmse = sess.run([rmse, g_rmse, d_rmse, t_rmse, c_rmse])\n",
    "        #w_row = [i, _rmse, _g_rmse, _d_rmse, _t_rmse, _c_rmse, _rmse_v, _g_rmse_v, _d_rmse_v, _t_rmse_v, _c_rmse_v]\n",
    "        #csv_trend.writerow(w_row)\n",
    "    if i%200 == 0:\n",
    "        print()\n",
    "        print(\"Traing RMSR(total, g, d, t, c):\", i , _rmse, _g_rmse, _d_rmse, _t_rmse, _c_rmse)\n",
    "        print()\n",
    "#training_trend_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movie_feature_dict = pickle.load(open('../data/'+folder+'/movie_feature_dict.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_matrix(row_num, col_num, user, movie, rating):\n",
    "    # row is movie infor num\n",
    "    # col is user num\n",
    "    matr = [[0 for i in range(col_num)] for j in range(row_num)]\n",
    "    for i in range(len(user)):\n",
    "        matr[movie[i]][user[i]] = float(rating[i])\n",
    "    \n",
    "    return matr\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_v, director_v, topic_v, genre_v, cast_v = sess.run([U,D,T,G,C])\n",
    "\n",
    "d_mat = generate_matrix(DIRECTOR_NUM, USER_NUM, d_user, d_movie, d_rating)\n",
    "t_mat = generate_matrix(TOPIC_NUM, USER_NUM, t_user, t_movie, t_rating)\n",
    "c_mat = generate_matrix(CAST_NUM, USER_NUM, c_user, c_movie, c_rating)\n",
    "g_mat = generate_matrix(GENRE_NUM, USER_NUM, g_user, g_movie, g_rating)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id_2_genre = pickle.load(open('../data/'+folder+'/id_2_genre.pkl', 'rb'))\n",
    "id_2_director = pickle.load(open('../data/'+folder+'/id_2_director.pkl', 'rb'))\n",
    "id_2_cast = pickle.load(open('../data/'+folder+'/id_2_cast.pkl', 'rb'))\n",
    "genre_2_id = pickle.load(open('../data/'+folder+'/genre_2_id.pkl', 'rb'))\n",
    "director_2_id = pickle.load(open('../data/'+folder+'/director_2_id.pkl', 'rb'))\n",
    "cast_2_id = pickle.load(open('../data/'+folder+'/cast_2_id.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_rating_list(info_id_list, info_matrix, user_vector, user_id, matrix):\n",
    "    result_list = []\n",
    "    for _id in info_id_list:\n",
    "        if matrix[_id][user_id] > 0:\n",
    "            result_list.append(matrix[_id][user_id]) \n",
    "        else:\n",
    "            \n",
    "            _v = info_matrix[:, int(_id)]\n",
    "            _rating = np.dot(user_vector, _v)\n",
    "            result_list.append(_rating)\n",
    "    return result_list\n",
    "\n",
    "def get_rating_list_train(id_tup_list, matrix):\n",
    "    result_list = []\n",
    "    for tup in id_tup_list:\n",
    "        user_id = tup[0]\n",
    "        info_id = tup[1]\n",
    "        result_list.append(matrix[info_id][user_id])\n",
    "\n",
    "def gen_regr_data(m_id, user_rating_tups, is_training = True):\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    user_id_train = []\n",
    "    for tup in user_rating_tups:\n",
    "        one_user_x = []\n",
    "        rating = float(tup[1])\n",
    "        y_train.append(rating)\n",
    "        \n",
    "        user_id = int(tup[0])\n",
    "        user_id_train.append(user_id)\n",
    "        feature_dict = movie_feature_dict[m_id]\n",
    "        u_v = user_v[user_id]\n",
    "        \n",
    "        d_x = get_rating_list(feature_dict['d'], director_v, u_v,user_id, d_mat)\n",
    "        g_x = get_rating_list(feature_dict['g'], genre_v, u_v, user_id, g_mat)\n",
    "        t_x = get_rating_list(feature_dict['t'], topic_v, u_v, user_id, t_mat)\n",
    "        c_x = get_rating_list(feature_dict['c'], cast_v, u_v, user_id, c_mat)\n",
    "        #print(user_id, d_x, g_x, t_x, c_x)\n",
    "        \n",
    "        \n",
    "        one_user_x = d_x + g_x + t_x + c_x\n",
    "        #one_user_x = d_x + g_x + c_x\n",
    "        x_train.append(one_user_x)\n",
    "        \n",
    "        feature_name = []\n",
    "        feature_name += [\"d_\" + id_2_director[_id] for _id in feature_dict['d']]\n",
    "        feature_name += [\"g_\" + id_2_genre[_id] for _id in feature_dict['g']]\n",
    "        feature_name += [\"t_\" + str(_id) for _id in feature_dict['t']]\n",
    "        feature_name += [\"c_\" + id_2_cast[_id] for _id in feature_dict['c']]\n",
    "        feature_name += [\"bias\"]\n",
    "        \n",
    "        \n",
    "    return x_train, y_train, feature_name, user_id_train\n",
    "        \n",
    "            \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_count_list = pickle.load(open('../data/'+folder+'/user_count_list.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name_list = ['d', 't', 'c', 'g']\n",
    "divid_list = []\n",
    "for name in name_list:\n",
    "    summ = 0\n",
    "    total = 0\n",
    "    for user in user_count_list[name]:\n",
    "        for key in user_count_list[name][user]:\n",
    "            summ += user_count_list[name][user][key]\n",
    "            total += 1\n",
    "\n",
    "    divid_list.append(summ/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store result matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_d_result, _t_result, _g_result, _c_result =  sess.run([d_result, t_result, g_result, c_result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(_d_result, open('../data/'+folder+'/ranking_matrix_director.pkl', 'wb'))\n",
    "pickle.dump(_t_result, open('../data/'+folder+'/ranking_matrix_type.pkl', 'wb'))\n",
    "pickle.dump(_g_result, open('../data/'+folder+'/ranking_matrix_genre.pkl', 'wb'))\n",
    "pickle.dump(_c_result, open('../data/'+folder+'/ranking_matrix_cast.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "import numpy as np\n",
    "\n",
    "def select_feature_index(feature, n):\n",
    "    index_list = [0 for i in range(len(feature))]\n",
    "    temp_f = [abs(feature[i]) for i in range(len(feature))]\n",
    "    for i in (range(n)):\n",
    "        min_index = temp_f.index(max(temp_f))\n",
    "        index_list[min_index] = 1\n",
    "        temp_f[min_index] = 0\n",
    "    \n",
    "    return index_list\n",
    "\n",
    "def get_predict_rating_old(user_rating, features):\n",
    "    return np.mean([user_rating[i] + features[i] for i in range(len(user_rating))])\n",
    "\n",
    "\n",
    "def get_predict_rating(user_rating, features):\n",
    "    weight_n = [float(abs(ur)) for ur in user_rating]\n",
    "    summ = sum(weight_n)\n",
    "    weight = [ur/summ for ur in weight_n]\n",
    "    #weight = [1./len(weight_n) for ur in weight_n]\n",
    "    \n",
    "    weight_total = sum([weight[i]* (user_rating[i] + features[i]) for i in range(len(weight))])\n",
    "    \n",
    "    return weight_total\n",
    "\n",
    "def get_predict_rating_select(user_rating, features, n):\n",
    "    if len(user_rating) < n:\n",
    "        return get_predict_rating_old(user_rating, features)\n",
    "    else:\n",
    "        index_list = select_feature_index(user_rating, n)\n",
    "        account_list = []\n",
    "        for i in range(len(user_rating)):\n",
    "            if index_list[i] == 1:\n",
    "                account_list.append(user_rating[i] + features[i])\n",
    "        return np.mean(account_list)\n",
    "\n",
    "def get_predict_rating_count(user_rating, features, user_id, feature_name):\n",
    "    type_prop = {}\n",
    "    weight = []\n",
    "    weight_sum = 0\n",
    "    offset = 0.01\n",
    "    for j in range(len(feature_name)-1):\n",
    "        name = feature_name[j]\n",
    "        dict_name = name.split('_')[0]\n",
    "        feature_n = name.split('_')[1]\n",
    "        try:\n",
    "            if dict_name == 'd':\n",
    "                feature_key = director_2_id[feature_n]\n",
    "                offset = 1/divid_list[0]\n",
    "            elif dict_name == 'c':\n",
    "                feature_key = cast_2_id[feature_n]\n",
    "                offset = 1/divid_list[2]\n",
    "            elif dict_name == 'g':\n",
    "                feature_key = genre_2_id[feature_n]\n",
    "                offset = 1/divid_list[3]\n",
    "            elif dict_name == 't':\n",
    "                feature_key = int(feature_n)\n",
    "                offset = 1/divid_list[1]\n",
    "        except:\n",
    "            feature_key = ''\n",
    "        try:\n",
    "            prop = user_count_list[dict_name][user_id][feature_key]\n",
    "        except:\n",
    "            prop = 1\n",
    "        prop = offset * prop\n",
    "        weight_sum += prop\n",
    "        weight.append(prop)      \n",
    "    \n",
    "    weight = [w/float(weight_sum) for w in weight]\n",
    "    result = 0\n",
    "    ppr_list = []\n",
    "    for i in range(len(user_rating)):\n",
    "        ppr = (user_rating[i] + features[i]) * weight[i]\n",
    "        ppr_list.append(ppr)\n",
    "        result += ppr\n",
    "        # add each kind information proportion to calculate average proportion of each information in movie \n",
    "    \n",
    "    for i in range(len(ppr_list)):\n",
    "        name = feature_name[i]\n",
    "        dict_name = name.split('_')[0]\n",
    "        if dict_name not in type_prop:\n",
    "            type_prop[dict_name] = [ppr_list[i]/result]\n",
    "        else:\n",
    "            type_prop[dict_name] = type_prop[dict_name] + [ppr_list[i]/result]\n",
    "    \"\"\"\n",
    "    print(user_id)\n",
    "    for i in range(len(weight)):\n",
    "        print(feature_name[i], end='')\n",
    "        print(\" %.2f %.2f %.2f\" % (user_rating[i], features[i], weight[i]))\n",
    "    \n",
    "    \"\"\"\n",
    "    type_mean = {}\n",
    "    for key in type_prop:\n",
    "        type_mean[key] = np.sum(type_prop[key])\n",
    "    \n",
    "    return result, type_mean\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equal weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1427 175 172\n",
      "759 107 88\n",
      "1106 148 163\n",
      "543 72 80\n",
      "208 28 26\n",
      "692 71 84\n",
      "519 57 61\n",
      "1002 132 151\n",
      "371 42 51\n",
      "1119 151 165\n",
      "200 24 24\n",
      "1168 134 132\n",
      "352 45 47\n",
      "378 53 59\n",
      "1000 131 89\n",
      "530 51 52\n",
      "1366 157 160\n",
      "873 104 104\n",
      "273 38 33\n",
      "539 86 58\n",
      "383 51 41\n",
      "1104 125 136\n",
      "868 122 115\n",
      "516 53 63\n",
      "787 89 99\n",
      "313 39 31\n",
      "639 72 85\n",
      "346 36 54\n",
      "480 44 69\n",
      "193 20 25\n",
      "50 7 5\n",
      "109 14 20\n",
      "238 33 25\n",
      "13 3 0\n",
      "176 23 23\n",
      "657 70 88\n",
      "412 55 50\n",
      "53 7"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Suyixin/anaconda/envs/ws/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2889: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/Suyixin/anaconda/envs/ws/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3\n",
      "289 36 42\n",
      "335 45 40\n",
      "1258 142 165\n",
      "135 17 16\n",
      "789 104 104\n",
      "1149 139 154\n",
      "1330 173 178\n",
      "912 102 125\n",
      "493 55 49\n",
      "97 15 12\n",
      "742 84 84\n",
      "1136 138 156\n",
      "43 6 13\n",
      "642 69 84\n",
      "164 13 17\n",
      "1242 155 186\n",
      "1045 123 114\n",
      "810 92 103\n",
      "less than 5: 56\n",
      "265 36 32\n",
      "235 35 30\n",
      "1560 207 202\n",
      "410 54 52\n",
      "46 6 7\n",
      "322 37 44\n",
      "31 7 3\n",
      "32 4 7\n",
      "250 42 28\n",
      "1177 165 150\n",
      "61 7 12\n",
      "300 37 45\n",
      "less than 5: 69\n",
      "254 33 32\n",
      "217 29 38\n",
      "374 54 51\n",
      "16 4 1\n",
      "447 53 54\n",
      "731 87 81\n",
      "743 95 90\n",
      "664 92 79\n",
      "66 7 14\n",
      "71 5 12\n",
      "335 50 41\n",
      "113 13 14\n",
      "231 26 35\n",
      "70 3 9\n",
      "9 0 1\n",
      "1009 110 118\n",
      "38 3 2\n",
      "37 4 7\n",
      "648 70 72\n",
      "748 108 84\n",
      "90 11 12\n",
      "58 8 9\n",
      "666 83 84\n",
      "186 37 28\n",
      "71 8 15\n",
      "266 43 33\n",
      "445 36 48\n",
      "1070 133 109\n",
      "440 54 56\n",
      "240 24 29\n",
      "271 29 29\n",
      "535 77 64\n",
      "58 7 11\n",
      "402 46 54\n",
      "96 14 10\n",
      "244 32 27\n",
      "94 10 11\n",
      "275 27 39\n",
      "less than 5: 108\n",
      "117 20 13\n",
      "133 16 12\n",
      "212 30 21\n",
      "103 9 10\n",
      "387 43 43\n",
      "165 19 21\n",
      "89 13 19\n",
      "195 23 25\n",
      "131 14 9\n",
      "367 61 44\n",
      "488 58 62\n",
      "105 14 17\n",
      "663 88 73\n",
      "895 105 114\n",
      "55 4 4\n",
      "50 9 8\n",
      "216 30 30\n",
      "728 106 77\n",
      "571 73 68\n",
      "1064 130 146\n",
      "93 13 13\n",
      "632 91 95\n",
      "244 27 32\n",
      "60 7 9\n",
      "86 17 11\n",
      "331 35 35\n",
      "44 6 2\n",
      "1224 168 173\n",
      "219 40 26\n",
      "26 5 3\n",
      "138 16 9\n",
      "239 25 29\n",
      "111 17 16\n",
      "212 18 32\n",
      "40 8 4\n",
      "20 3 0\n",
      "258 32 38\n",
      "102 8 9\n",
      "10 0 3\n",
      "1006 127 115\n",
      "156 27 18\n",
      "391 55 45\n",
      "45 11 3\n",
      "102 16 12\n",
      "238 23 32\n",
      "130 22 12\n",
      "343 47 34\n",
      "1509 182 188\n",
      "377 42 39\n",
      "948 117 117\n",
      "102 13 11\n",
      "621 74 69\n",
      "328 40 33\n",
      "65 5 5\n",
      "107 20 10\n",
      "18 4 0\n",
      "577 88 83\n",
      "140 18 14\n",
      "168 19 23\n",
      "537 75 71\n",
      "905 122 142\n",
      "400 45 46\n",
      "less than 5: 171\n",
      "87 12 9\n",
      "457 55 57\n",
      "479 65 63\n",
      "773 110 105\n",
      "230 39 35\n",
      "58 8 2\n",
      "174 33 31\n",
      "593 73 61\n",
      "236 27 26\n",
      "12 0 0\n",
      "98 12 23\n",
      "47 8 11\n",
      "568 77 70\n",
      "763 89 80\n",
      "not in: 186\n",
      "154 25 12\n",
      "213 22 21\n",
      "12 0 4\n",
      "230 27 24\n",
      "340 34 36\n",
      "less than 5: 192\n",
      "1010 112 114\n",
      "26 4 4\n",
      "61 5 10\n",
      "76 4 15\n",
      "71 3 10\n",
      "77 8 8\n",
      "31 3 2\n",
      "21 5 4\n",
      "517 65 51\n",
      "273 31 38\n",
      "14 3 3\n",
      "12 4 1\n",
      "26 1 6\n",
      "283 36 29\n"
     ]
    }
   ],
   "source": [
    "total_predict = np.array([])\n",
    "total_rating = np.array([])\n",
    "total_predict_v = np.array([])\n",
    "total_rating_v = np.array([])\n",
    "total_predict_tr = np.array([])\n",
    "total_rating_tr = np.array([])\n",
    "#train_rmse_compare_F = []\n",
    "for m in range(0, MOVIE_NUM+1):\n",
    "    if m not in movie_rating:\n",
    "        print(\"not in:\", m)\n",
    "        continue\n",
    "    user_rating_tups = movie_rating[m]\n",
    "    x_train, y_train, feature_name, user_id_train = gen_regr_data(int(m), user_rating_tups)\n",
    "    \n",
    "    if len(y_train) < 5:\n",
    "        print('less than 5:', m)\n",
    "        continue\n",
    "    #x_train = x_train[1:2]\n",
    "    #y_train = y_train[1:2]\n",
    "    \n",
    "    #if len(y_train) > 50:\n",
    "     #   continue\n",
    "    \n",
    "    if m in movie_rating_test:     \n",
    "        user_rating_tups_test = movie_rating_test[m]\n",
    "        x_test, y_test, _, user_id_test= gen_regr_data(int(m), user_rating_tups_test)\n",
    "    else:\n",
    "        x_test = []\n",
    "        y_test = []\n",
    "        user_id_test = []\n",
    "    \n",
    "    if m in movie_rating_validation:     \n",
    "        user_rating_tups_validation = movie_rating_validation[m]\n",
    "        x_valid, y_valid, _, user_id_valid= gen_regr_data(int(m), user_rating_tups_validation)\n",
    "    else:\n",
    "        x_valid = []\n",
    "        y_valid = []\n",
    "        user_id_valid = []\n",
    "        \n",
    "    print(len(x_train), len(x_test), len(x_valid))\n",
    "    feature = [0 for i in range(len(feature_name)-1)]\n",
    "    \n",
    "    for j in range(len(feature_name)-1):\n",
    "        name = feature_name[j]\n",
    "        dict_name = name.split('_')[0]\n",
    "        feature_n = name.split('_')[1]\n",
    "\n",
    "        \n",
    "        for i in range(len(x_train)):\n",
    "            feature[j] += (y_train[i] - x_train[i][j])\n",
    "        \n",
    "        feature[j] /= len(x_train)\n",
    "    \n",
    "    #weight = [1 - abs(f) for f in feature] if len(feature) > 0 else []\n",
    "    #select_indexs = select_feature_index(feature, 100)\n",
    "    #select_indexs = [1 for i in range(len(feature))]\n",
    "    #for i in range(len(feature)):\n",
    "        #print(feature_name[i], 1/(1-feature[i]))\n",
    "        #if select_indexs[i] == 1:\n",
    "            #print(feature_name[i], feature[i], weight[i])\n",
    "    #print()\n",
    "    \n",
    "\n",
    "    predict_t = []\n",
    "    predict_v = []\n",
    "    predict_tr = []\n",
    "    \n",
    "    for i in range(len(x_train)):\n",
    "        cc = get_predict_rating_old(x_train[i], feature)\n",
    "        #print(\"%.2f %.2f\" %(cc, y_train[i]))\n",
    "        #print()\n",
    "        predict_tr.append(cc)\n",
    "    \n",
    "    for i in range(len(x_test)):\n",
    "        cc = get_predict_rating_old(x_test[i], feature)\n",
    "        #print(\"%.2f %.2f\" %(cc, y_test[i]))\n",
    "        #print()\n",
    "        predict_t.append(cc)\n",
    "    \n",
    "\n",
    "    for i in range(len(x_valid)):\n",
    "        cc = get_predict_rating_old(x_valid[i], feature)\n",
    "        predict_v.append(cc)\n",
    "        #print(\"%.2f %.2f\" %(cc, y_valid[i]))\n",
    "        #print()\n",
    "     \n",
    "    train_rmse = np.sqrt(np.mean(np.square(np.subtract(predict_tr, y_train))))\n",
    "    valid_rmse = np.sqrt(np.mean(np.square(np.subtract(predict_v, y_valid))))\n",
    "    test_rmse = np.sqrt(np.mean(np.square(np.subtract(predict_t, y_test))))\n",
    "    \n",
    "    #print(m, train_rmse, valid_rmse, test_rmse)\n",
    "    #train_rmse_compare_F.append([m, len(y_train), len(y_valid),len(y_test), train_rmse, valid_rmse, test_rmse])\n",
    "    \n",
    "    total_predict = np.concatenate((total_predict, predict_t), axis = 0)\n",
    "    total_rating = np.concatenate((total_rating, y_test), axis = 0)\n",
    "    total_predict_v = np.concatenate((total_predict_v, predict_v), axis = 0)\n",
    "    total_rating_v = np.concatenate((total_rating_v, y_valid), axis = 0)\n",
    "    total_predict_tr = np.concatenate((total_predict_tr, predict_tr), axis = 0)\n",
    "    total_rating_tr = np.concatenate((total_rating_tr, y_train), axis = 0)\n",
    "    \n",
    "    #print(feature)\n",
    "    #print(feature_name)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse: 0.7638355321\n"
     ]
    }
   ],
   "source": [
    "rmse = np.sqrt(np.mean(np.square(np.subtract(total_predict, total_rating))))  \n",
    "print(\"train rmse:\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse: 0.29143128588\n",
      "train mae: 0.21081254878\n",
      "valid rmse: 0.755190356617\n",
      "valid mae: 0.555964326455\n",
      "test rmse: 0.763469932085\n",
      "test mae: 0.558734388503\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(total_predict)):\n",
    "    if total_predict[i] < 0:\n",
    "        total_predict[i] = 0.\n",
    "    if total_predict[i] > 5:\n",
    "        total_predict[i] = 5.\n",
    "\n",
    "for i in range(len(total_predict_v)):\n",
    "    if total_predict_v[i] < 0:\n",
    "        total_predict_v[i] = 0.\n",
    "    if total_predict_v[i] > 5:\n",
    "        total_predict_v[i] = 5.\n",
    "        \n",
    "for i in range(len(total_predict_tr)):\n",
    "    if total_predict_tr[i] < 0:\n",
    "        total_predict_tr[i] = 0.\n",
    "    if total_predict_tr[i] > 5:\n",
    "        total_predict_tr[i] = 5.\n",
    "\n",
    "        \n",
    "rmse = np.sqrt(np.mean(np.square(np.subtract(total_predict_tr, total_rating_tr))))  \n",
    "print(\"train rmse:\", rmse)\n",
    "\n",
    "mae = mean_absolute_error(total_rating_tr, total_predict_tr)\n",
    "print(\"train mae:\", mae)\n",
    "\n",
    "rmse = np.sqrt(np.mean(np.square(np.subtract(total_predict_v, total_rating_v))))  \n",
    "print(\"valid rmse:\", rmse)\n",
    "\n",
    "mae = mean_absolute_error(total_rating_v, total_predict_v)\n",
    "print(\"valid mae:\", mae)\n",
    "\n",
    "rmse = np.sqrt(np.mean(np.square(np.subtract(total_predict, total_rating))))  \n",
    "print(\"test rmse:\", rmse)\n",
    "\n",
    "mae = mean_absolute_error(total_rating, total_predict)\n",
    "print(\"test mae:\", mae)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# weight with number of featured movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not in 186\n"
     ]
    }
   ],
   "source": [
    "total_predict = np.array([])\n",
    "total_rating = np.array([])\n",
    "total_predict_v = np.array([])\n",
    "total_rating_v = np.array([])\n",
    "total_predict_tr = np.array([])\n",
    "total_rating_tr = np.array([])\n",
    "train_rmse_compare_E = []\n",
    "movie_feature = {}\n",
    "feature_name_dict = {}\n",
    "dropped_movie = 0\n",
    "info_prop = {}\n",
    "info_prop['t'] = 0\n",
    "info_prop['g'] = 0\n",
    "info_prop['c'] = 0\n",
    "info_prop['d'] = 0\n",
    "for m in range(0, MOVIE_NUM+1):\n",
    "    if m not in movie_rating:\n",
    "        print('not in', m)\n",
    "        dropped_movie += 1\n",
    "        continue\n",
    "    user_rating_tups = movie_rating[m]\n",
    "    x_train, y_train, feature_name, user_id_train = gen_regr_data(int(m), user_rating_tups)\n",
    "    \n",
    "    if len(y_train) < 0:\n",
    "        print('less than 5:', m)\n",
    "        dropped_movie += 1\n",
    "        continue\n",
    "    #if len(y_train) > 50:\n",
    "     #   continue\n",
    "    \n",
    "    if m in movie_rating_test:     \n",
    "        user_rating_tups_test = movie_rating_test[m]\n",
    "        x_test, y_test, _, user_id_test= gen_regr_data(int(m), user_rating_tups_test)\n",
    "    else:\n",
    "        x_test = []\n",
    "        y_test = []\n",
    "        user_id_test = []\n",
    "    \n",
    "    if m in movie_rating_validation:     \n",
    "        user_rating_tups_validation = movie_rating_validation[m]\n",
    "        x_valid, y_valid, _, user_id_valid= gen_regr_data(int(m), user_rating_tups_validation)\n",
    "    else:\n",
    "        x_valid = []\n",
    "        y_valid = []\n",
    "        user_id_valid = []\n",
    "    \n",
    "    if len(x_test) == 0:\n",
    "        continue\n",
    "        \n",
    "    \n",
    "    feature = [0 for i in range(len(feature_name)-1)]\n",
    "    \n",
    "    for j in range(len(feature_name)-1):\n",
    "        prop_sum = 0\n",
    "        name = feature_name[j]\n",
    "        dict_name = name.split('_')[0]\n",
    "        feature_n = name.split('_')[1]\n",
    "        offset = 0.01\n",
    "        try:\n",
    "            if dict_name == 'd':\n",
    "                feature_key = director_2_id[feature_n]\n",
    "                offset = 1/divid_list[0]\n",
    "            elif dict_name == 'c':\n",
    "                feature_key = cast_2_id[feature_n]\n",
    "                offset = 1/divid_list[2]\n",
    "            elif dict_name == 'g':\n",
    "                feature_key = genre_2_id[feature_n]\n",
    "                offsedivid_listt = 1/divid_list[3]\n",
    "            elif dict_name == 't':\n",
    "                feature_key = int(feature_n)\n",
    "                offset = 1/divid_list[1]\n",
    "        except:\n",
    "            feature_key = ''\n",
    "        \n",
    "        for i in range(len(x_train)):\n",
    "            try:\n",
    "                prop = user_count_list[dict_name][user_id_train[i]][feature_key]\n",
    "            except:\n",
    "                prop = 1\n",
    "            prop = offset * prop\n",
    "            #prop_sum += prop\n",
    "            #feature[j] += (y_train[i] - x_train[i][j]) * prop\n",
    "            prop_sum += 1\n",
    "            feature[j] += (y_train[i] - x_train[i][j])\n",
    "        \n",
    "        feature[j] /= prop_sum\n",
    "    feature_name_dict[m] = feature_name\n",
    "    movie_feature[m] = feature\n",
    "    #weight = [1 - abs(f) for f in feature] if len(feature) > 0 else []\n",
    "    #select_indexs = select_feature_index(feature, 100)\n",
    "    #select_indexs = [1 for i in range(len(feature))]\n",
    "    #for i in range(len(feature)):\n",
    "        #print(feature_name[i], 1/(1-feature[i]))\n",
    "        #if select_indexs[i] == 1:\n",
    "            #print(feature_name[i], feature[i], weight[i])\n",
    "    #print()\n",
    "    \n",
    "\n",
    "    predict_t = []\n",
    "    predict_v = []\n",
    "    predict_tr = []\n",
    "    for i in range(len(x_train)):\n",
    "        cc,info_prop_s = get_predict_rating_count(x_train[i], feature, user_id_train[i], feature_name)\n",
    "        #print(\"%.2f %.2f\" %(cc, y_train[i]))\n",
    "        #print()\n",
    "        for key in info_prop:\n",
    "            if key in info_prop_s:\n",
    "                info_prop[key] = info_prop[key] + info_prop_s[key]\n",
    "        predict_tr.append(cc)\n",
    "    \n",
    "    for i in range(len(x_test)):\n",
    "        cc,info_prop_s = get_predict_rating_count(x_test[i], feature, user_id_test[i], feature_name)\n",
    "        #print(\"%.2f %.2f\" %(cc, y_test[i]))\n",
    "        #print()\n",
    "        for key in info_prop:\n",
    "            if key in info_prop_s:\n",
    "                info_prop[key] = info_prop[key] + info_prop_s[key]\n",
    "        predict_t.append(cc)\n",
    "    \n",
    "\n",
    "    for i in range(len(x_valid)):\n",
    "        cc,info_prop_s = get_predict_rating_count(x_valid[i], feature, user_id_valid[i], feature_name )\n",
    "        for key in info_prop:\n",
    "            if key in info_prop_s:\n",
    "                info_prop[key] = info_prop[key] + info_prop_s[key]\n",
    "        predict_v.append(cc)\n",
    "        #print(\"%.2f %.2f\" %(cc, y_valid[i]))\n",
    "        #print()\n",
    "        \n",
    "        \n",
    "    #train_rmse = np.sqrt(np.mean(np.square(np.subtract(predict_tr, y_train))))\n",
    "    #valid_rmse = np.sqrt(np.mean(np.square(np.subtract(predict_v, y_valid))))\n",
    "    #test_rmse = np.sqrt(np.mean(np.square(np.subtract(predict_t, y_test))))\n",
    "    \n",
    "    #print(m, train_rmse, valid_rmse, test_rmse)\n",
    "    \n",
    "    #print(m, train_rmse, valid_rmse, test_rmse)\n",
    "    #train_rmse_compare_E.append([m, len(y_train), len(y_valid),len(y_test), train_rmse, valid_rmse, test_rmse])\n",
    "    \n",
    "        \n",
    "    total_predict = np.concatenate((total_predict, predict_t), axis = 0)\n",
    "    total_rating = np.concatenate((total_rating, y_test), axis = 0)\n",
    "    total_predict_v = np.concatenate((total_predict_v, predict_v), axis = 0)\n",
    "    total_rating_v = np.concatenate((total_rating_v, y_valid), axis = 0)\n",
    "    total_predict_tr = np.concatenate((total_predict_tr, predict_tr), axis = 0)\n",
    "    total_rating_tr = np.concatenate((total_rating_tr, y_train), axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9932"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t : 0.101244689706\n",
      "g : 0.522195997063\n",
      "c : 0.275096052917\n",
      "d : 0.101463260314\n"
     ]
    }
   ],
   "source": [
    "summ = 0\n",
    "for key in info_prop:\n",
    "    summ += info_prop[key]\n",
    "\n",
    "for key in info_prop:\n",
    "    print(key, ':', info_prop[key]/summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_Michael Bay 0.03360957642725598\n",
      "g_Action -0.4520801335041177\n",
      "g_Adventure -0.5222022198128377\n",
      "g_Sci-Fi -0.5247564581382188\n",
      "t_0 -0.07597237047091458\n",
      "c_Shia LaBeouf -0.04929876753081174\n",
      "c_Rosie Huntington-Whiteley -7.68773217788322e-17\n",
      "c_Josh Duhamel 0.0\n"
     ]
    }
   ],
   "source": [
    "index = 3\n",
    "for i in range(len(movie_feature[index])):\n",
    "    print(feature_name_dict[index][i], movie_feature[index][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.402485198126\n",
      "0.288492403305\n",
      "0.751154040526\n",
      "0.553690955189\n",
      "0.759157228415\n",
      "0.556456986302\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(total_predict)):\n",
    "    if total_predict[i] < 0:\n",
    "        total_predict[i] = 0.\n",
    "    if total_predict[i] > 5:\n",
    "        total_predict[i] = 5.\n",
    "\n",
    "for i in range(len(total_predict_v)):\n",
    "    if total_predict_v[i] < 0:\n",
    "        total_predict_v[i] = 0.\n",
    "    if total_predict_v[i] > 5:\n",
    "        total_predict_v[i] = 5.\n",
    "        \n",
    "for i in range(len(total_predict_tr)):\n",
    "    if total_predict_tr[i] < 0:\n",
    "        total_predict_tr[i] = 0.\n",
    "    if total_predict_tr[i] > 5:\n",
    "        total_predict_tr[i] = 5.\n",
    "\n",
    "        \n",
    "rmse = np.sqrt(np.mean(np.square(np.subtract(total_predict_tr, total_rating_tr))))  \n",
    "print( rmse)\n",
    "\n",
    "mae = mean_absolute_error(total_rating_tr, total_predict_tr)\n",
    "print(mae)\n",
    "\n",
    "rmse = np.sqrt(np.mean(np.square(np.subtract(total_predict_v, total_rating_v))))  \n",
    "print(rmse)\n",
    "\n",
    "mae = mean_absolute_error(total_rating_v, total_predict_v)\n",
    "print( mae)\n",
    "\n",
    "\n",
    "rmse = np.sqrt(np.mean(np.square(np.subtract(total_predict, total_rating))))  \n",
    "print( rmse)\n",
    "\n",
    "mae = mean_absolute_error(total_rating, total_predict)\n",
    "print(mae)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get one user's information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_id = 5\n",
    "feature_dict = ['d','t','c', 'g']\n",
    "one_user_count_list = {}\n",
    "for type_name in feature_dict:\n",
    "    one_user_count_list[type_name] = {}\n",
    "for i in range(len(feature_dict)):\n",
    "    dict_name = feature_dict[i]\n",
    "    divid_value = divid_list[i]\n",
    "    for feature_key in user_count_list[dict_name][user_id]:\n",
    "        one_user_count_list[dict_name][feature_key] = user_count_list[dict_name][user_id][feature_key]/divid_value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "less than 5: 56\n",
      "less than 5: 69\n",
      "less than 5: 108\n",
      "less than 5: 171\n",
      "not in 186\n",
      "less than 5: 192\n"
     ]
    }
   ],
   "source": [
    "user_movie_rating = {}\n",
    "user_movie_info = {}\n",
    "\n",
    "for m in range(0, MOVIE_NUM+1):\n",
    "    if m not in movie_rating:\n",
    "        print('not in', m)\n",
    "        dropped_movie += 1\n",
    "        continue\n",
    "    user_rating_tups = movie_rating[m]\n",
    "    x_train, y_train, feature_name, user_id_train = gen_regr_data(int(m), user_rating_tups)\n",
    "    if len(y_train) < 5:\n",
    "        print('less than 5:', m)\n",
    "        dropped_movie += 1\n",
    "        continue\n",
    "    \n",
    "    one_uesr_ratings = []\n",
    "    one_user_info = []\n",
    "    weight = []\n",
    "    x_pred, _, feature_name, user_id_pred = gen_regr_data(int(m), [(user_id, 0)])\n",
    "    if user_id in user_id_train:\n",
    "        ind = user_id_train.index(user_id)\n",
    "        real_rating = y_train[ind]\n",
    "    else:\n",
    "        real_rating = -1\n",
    "    for j in range(len(feature_name)-1):\n",
    "        prop_sum = 0\n",
    "        name = feature_name[j]\n",
    "        dict_name = name.split('_')[0]\n",
    "        feature_n = name.split('_')[1]\n",
    "        offset = 0.01\n",
    "        try:\n",
    "            if dict_name == 'd':\n",
    "                feature_key = director_2_id[feature_n]\n",
    "            elif dict_name == 'c':\n",
    "                feature_key = cast_2_id[feature_n]\n",
    "            elif dict_name == 'g':\n",
    "                feature_key = genre_2_id[feature_n]\n",
    "            elif dict_name == 't':\n",
    "                feature_key = int(feature_n)\n",
    "        except:\n",
    "            feature_key = ''\n",
    "        try:\n",
    "            weight.append(one_user_count_list[dict_name][feature_key])\n",
    "        except:\n",
    "            # if user have never seen this information\n",
    "            weight.append(0.1)\n",
    "\n",
    "    weight_normal = [w/sum(weight) for w in weight]\n",
    "    #print('begin')\n",
    "    try:\n",
    "        one_pred_value = sum([weight_normal[i]*(x_pred[0][i] + movie_feature[m][i]) for i in range(len(x_pred[0]))])\n",
    "    except:\n",
    "        continue\n",
    "    feature_name_dict\n",
    "    \"\"\"\n",
    "    print(\"movie:\", m)\n",
    "    print(\"pred:\", one_pred_value)\n",
    "    print(\"real:\", real_rating)\n",
    "    \n",
    "    for i in range(len(x_pred[0])):\n",
    "        print(feature_name_dict[m][i], x_pred[0][i], movie_feature[m][i], weight_normal[i])\n",
    "    \"\"\"\n",
    "    user_movie_rating[m] = (one_pred_value, real_rating)\n",
    "    user_movie_info[m] = {}\n",
    "    user_movie_info[m]['info_name'] = feature_name_dict[m][0:-1]\n",
    "    user_movie_info[m]['info_rating'] = x_pred[0]\n",
    "    user_movie_info[m]['movie_offset'] = movie_feature[m]\n",
    "    user_movie_info[m]['weight'] = weight_normal\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor m in user_movie_rating:\\n    if user_movie_rating[m][1] > 0:\\n        print(m, user_movie_rating[m][1])\\n        length = len(user_movie_info[m]['info_rating'])\\n        for i in range((length)):\\n            print(user_movie_info[m]['info_name'][i], user_movie_info[m]['info_rating'][i] + user_movie_info[m]['movie_offset'][i], user_movie_info[m]['weight'][i])\\n\\n\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "for m in user_movie_rating:\n",
    "    if user_movie_rating[m][1] > 0:\n",
    "        print(m, user_movie_rating[m][1])\n",
    "        length = len(user_movie_info[m]['info_rating'])\n",
    "        for i in range((length)):\n",
    "            print(user_movie_info[m]['info_name'][i], user_movie_info[m]['info_rating'][i] + user_movie_info[m]['movie_offset'][i], user_movie_info[m]['weight'][i])\n",
    "\n",
    "\"\"\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_movie = -1\n",
    "best_rating = 0\n",
    "for m in user_movie_rating:\n",
    "    if user_movie_rating[m][1] < 0 and user_movie_rating[m][0] > best_rating:\n",
    "        best_rating = user_movie_rating[m][0]\n",
    "        best_movie = m\n",
    "\n",
    "best_movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1817",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-e469be7b0d14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbest_movie\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1817\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rating:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muser_movie_rating\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_movie\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_movie_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_movie\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'info_rating'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0muser_movie_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_movie\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'info_rating'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0muser_movie_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_movie\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'movie_offset'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0muser_movie_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_movie\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1817"
     ]
    }
   ],
   "source": [
    "best_movie = 1817\n",
    "print('rating:',user_movie_rating[best_movie])\n",
    "length = len(user_movie_info[best_movie]['info_rating'])\n",
    "for i in range((length)):\n",
    "    part = (user_movie_info[best_movie]['info_rating'][i] + user_movie_info[best_movie]['movie_offset'][i]) * user_movie_info[best_movie]['weight'][i]\n",
    "    print(user_movie_info[best_movie]['info_name'][i], user_movie_info[best_movie]['info_rating'][i], user_movie_info[best_movie]['movie_offset'][i], user_movie_info[best_movie]['weight'][i], part)\n",
    "    #print(user_movie_info[best_movie]['info_name'][i])\n",
    "    \n",
    "    #print(part)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## test cold start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_predict_rating_0(user_rating, features):\n",
    "    return np.mean([user_rating[i] for i in range(len(user_rating))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_predict = np.array([])\n",
    "total_rating = np.array([])\n",
    "total_predict_v = np.array([])\n",
    "total_rating_v = np.array([])\n",
    "total_predict_tr = np.array([])\n",
    "total_rating_tr = np.array([])\n",
    "train_rmse_compare_E = []\n",
    "movie_feature = {}\n",
    "feature_name_dict = {}\n",
    "dropped_movie = 0\n",
    "total_predicted = 0\n",
    "info_prop = {}\n",
    "info_prop['t'] = 0\n",
    "info_prop['g'] = 0\n",
    "info_prop['c'] = 0\n",
    "info_prop['d'] = 0\n",
    "largest_train = 10\n",
    "_RMSE_list = []\n",
    "for largest_train in range(0, 200, 3):\n",
    "    for m in range(0, MOVIE_NUM+1):\n",
    "        have_train = False\n",
    "        x_train = []\n",
    "        y_train = []\n",
    "        if m in movie_rating:\n",
    "            user_rating_tups = movie_rating[m]\n",
    "            x_train, y_train, feature_name, user_id_train = gen_regr_data(int(m), user_rating_tups)\n",
    "            have_train = True\n",
    "\n",
    "        if len(y_train) > largest_train:\n",
    "            #print('larger:', m)\n",
    "            #dropped_movie += 1\n",
    "            continue\n",
    "        #if len(y_train) > 50:\n",
    "         #   continue\n",
    "\n",
    "        if m in movie_rating_test:     \n",
    "            user_rating_tups_test = movie_rating_test[m]\n",
    "            x_test, y_test, _, user_id_test= gen_regr_data(int(m), user_rating_tups_test)\n",
    "        else:\n",
    "            x_test = []\n",
    "            y_test = []\n",
    "            user_id_test = []\n",
    "        \"\"\"\n",
    "        if m in movie_rating_validation:     \n",
    "            user_rating_tups_validation = movie_rating_validation[m]\n",
    "            x_valid, y_valid, _, user_id_valid= gen_regr_data(int(m), user_rating_tups_validation)\n",
    "        else:\n",
    "            x_valid = []\n",
    "            y_valid = []\n",
    "            user_id_valid = []\n",
    "        \"\"\"\n",
    "        if len(x_test) == 0:\n",
    "            continue\n",
    "\n",
    "\n",
    "        feature = [1.0/(len(feature_name)-1) for i in range(len(feature_name)-1)]\n",
    "\n",
    "        for j in range(len(feature_name)-1):\n",
    "            prop_sum = 0\n",
    "            name = feature_name[j]\n",
    "            dict_name = name.split('_')[0]\n",
    "            feature_n = name.split('_')[1]\n",
    "            offset = 0.01\n",
    "            try:\n",
    "                if dict_name == 'd':\n",
    "                    feature_key = director_2_id[feature_n]\n",
    "                    offset = 1/divid_list[0]\n",
    "                elif dict_name == 'c':\n",
    "                    feature_key = cast_2_id[feature_n]\n",
    "                    offset = 1/divid_list[2]\n",
    "                elif dict_name == 'g':\n",
    "                    feature_key = genre_2_id[feature_n]\n",
    "                    offsedivid_listt = 1/divid_list[3]\n",
    "                elif dict_name == 't':\n",
    "                    feature_key = int(feature_n)\n",
    "                    offset = 1/divid_list[1]\n",
    "            except:\n",
    "                feature_key = ''\n",
    "            #print(len(x_train))\n",
    "            for i in range(len(x_train)):\n",
    "                try:\n",
    "                    prop = user_count_list[dict_name][user_id_train[i]][feature_key]\n",
    "                except:\n",
    "                    prop = 1\n",
    "                prop = offset * prop\n",
    "                #prop_sum += prop\n",
    "                #feature[j] += (y_train[i] - x_train[i][j]) * prop\n",
    "                prop_sum += 1\n",
    "                feature[j] += (y_train[i] - x_train[i][j])\n",
    "\n",
    "            if len(x_train) > 0:\n",
    "                feature[j] /= prop_sum\n",
    "\n",
    "        feature_name_dict[m] = feature_name\n",
    "        movie_feature[m] = feature\n",
    "        #weight = [1 - abs(f) for f in feature] if len(feature) > 0 else []\n",
    "        #select_indexs = select_feature_index(feature, 100)\n",
    "        #select_indexs = [1 for i in range(len(feature))]\n",
    "        #for i in range(len(feature)):\n",
    "            #print(feature_name[i], 1/(1-feature[i]))\n",
    "            #if select_indexs[i] == 1:\n",
    "                #print(feature_name[i], feature[i], weight[i])\n",
    "        #print()\n",
    "\n",
    "\n",
    "        predict_t = []\n",
    "        predict_v = []\n",
    "        predict_tr = []\n",
    "\n",
    "        \"\"\"\n",
    "        for i in range(len(x_train)):\n",
    "            cc,info_prop_s = get_predict_rating_count(x_train[i], feature, user_id_train[i], feature_name)\n",
    "            #print(\"%.2f %.2f\" %(cc, y_train[i]))\n",
    "            #print()\n",
    "            for key in info_prop:\n",
    "                if key in info_prop_s:\n",
    "                    info_prop[key] = info_prop[key] + info_prop_s[key]\n",
    "            predict_tr.append(cc)\n",
    "\n",
    "        for i in range(len(x_valid)):\n",
    "            cc,info_prop_s = get_predict_rating_count(x_valid[i], feature, user_id_valid[i], feature_name )\n",
    "            for key in info_prop:\n",
    "                if key in info_prop_s:\n",
    "                    info_prop[key] = info_prop[key] + info_prop_s[key]\n",
    "            predict_v.append(cc)\n",
    "        \"\"\"\n",
    "        for i in range(len(x_test)):\n",
    "            if have_train:\n",
    "                cc,info_prop_s = get_predict_rating_count(x_test[i], feature, user_id_test[i], feature_name)\n",
    "            else:\n",
    "                cc = get_predict_rating_0(x_test[i], feature)\n",
    "            #print(\"%.2f %.2f\" %(cc, y_test[i]))\n",
    "            #print()\n",
    "            total_predicted += 1\n",
    "            predict_t.append(cc)\n",
    "\n",
    "        #train_rmse = np.sqrt(np.mean(np.square(np.subtract(predict_tr, y_train))))\n",
    "        #valid_rmse = np.sqrt(np.mean(np.square(np.subtract(predict_v, y_valid))))\n",
    "        #test_rmse = np.sqrt(np.mean(np.square(np.subtract(predict_t, y_test))))\n",
    "\n",
    "        #print(m, train_rmse, valid_rmse, test_rmse)\n",
    "\n",
    "        #print(m, train_rmse, valid_rmse, test_rmse)\n",
    "        #train_rmse_compare_E.append([m, len(y_train), len(y_valid),len(y_test), train_rmse, valid_rmse, test_rmse])\n",
    "\n",
    "\n",
    "        total_predict = np.concatenate((total_predict, predict_t), axis = 0)\n",
    "        total_rating = np.concatenate((total_rating, y_test), axis = 0)\n",
    "        total_predict_v = np.concatenate((total_predict_v, predict_v), axis = 0)\n",
    "        total_rating_v = np.concatenate((total_rating_v, y_valid), axis = 0)\n",
    "        total_predict_tr = np.concatenate((total_predict_tr, predict_tr), axis = 0)\n",
    "        total_rating_tr = np.concatenate((total_rating_tr, y_train), axis = 0)\n",
    "    rmse = np.sqrt(np.mean(np.square(np.subtract(total_predict, total_rating)))) \n",
    "    _RMSE_list.append((largest_train, rmse))\n",
    "    print(largest_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(np.mean(np.square(np.subtract(total_predict, total_rating))))  \n",
    "print( rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(total_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_RMSE_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('MMF_RMSE.csv','w') as f:\n",
    "    f_csv = csv.writer(f)\n",
    "    headers = ['num, RMSE']\n",
    "    f_csv.writerow(headers)\n",
    "    for p in _RMSE_list:\n",
    "        row = [p[0], p[1]]\n",
    "        f_csv.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ws3.6",
   "language": "python",
   "name": "ws"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
