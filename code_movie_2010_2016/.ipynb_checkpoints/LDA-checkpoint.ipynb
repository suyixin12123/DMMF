{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "import pickle\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "FOLDER = 'year2010_movie'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'finish'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "en_stop = stopwords.words('english')\n",
    "add_list = ['sp', 'n', 'take', 'get', 'stopwordssymble']\n",
    "en_stop += add_list\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\"finish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the movie id used set\n",
    "f = open(\"../data/movie_plot.txt\", 'rb')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'finish'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_set = []\n",
    "index_set = []\n",
    "f = open(\"../data/movie_plot.txt\", 'rb')\n",
    "for line in f:\n",
    "    index_set.append(str(line).split('<sss>')[0])\n",
    "    doc_set.append(str(line).split('<sss>')[1])\n",
    "\"finish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# create sample documents\\ndoc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\\ndoc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\\ndoc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\\ndoc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\\ndoc_e = \"Health professionals say that brocolli is good for your health.\" \\n\\n# compile sample documents into a list\\ndoc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]\\n# list for tokenized documents in loop\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# create sample documents\n",
    "doc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
    "doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
    "doc_e = \"Health professionals say that brocolli is good for your health.\" \n",
    "\n",
    "# compile sample documents into a list\n",
    "doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]\n",
    "# list for tokenized documents in loop\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "def remove_frequent_words(texts, num):\n",
    "    remove_word = set()\n",
    "    result_texts = []\n",
    "    word_dict = {}\n",
    "    for text in texts:\n",
    "        for word in text:\n",
    "            if word in word_dict:\n",
    "                word_dict[word] += 1\n",
    "            else:\n",
    "                word_dict[word] = 1\n",
    "    \n",
    "    sorted_x = sorted(word_dict.items(), key=operator.itemgetter(1))\n",
    "    length = len(sorted_x)\n",
    "    for i in range(length-num,length):\n",
    "        key = sorted_x[i][0]\n",
    "        remove_word.add(key)\n",
    "    \n",
    "    for i in range(len(texts)):\n",
    "        result_text = []\n",
    "        for j in range(len(texts[i])):\n",
    "            if texts[i][j] not in remove_word:\n",
    "                result_text.append(texts[i][j])\n",
    "        \n",
    "        result_texts.append(result_text)\n",
    "                \n",
    "    return texts, remove_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'finish'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = []\n",
    "# loop through document list\n",
    "index = 0\n",
    "for i in doc_set:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    \n",
    "    tokens = tokenizer.tokenize(i)\n",
    "    for j in range(len(tokens)):\n",
    "        token = tokens[j]\n",
    "        if token[0][0].isupper():\n",
    "            tokens[j] = 'stopwordssymble'\n",
    "        else:\n",
    "            tokens[j] = token.lower()\n",
    "    \n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    lemmed_tokens = [lmtzr.lemmatize(i) for i in stopped_tokens]\n",
    "    \n",
    "    lemmed_tokens = [i for i in lemmed_tokens if not i in en_stop]\n",
    "    \"\"\"\n",
    "    while 'sp' in stemmed_tokens:\n",
    "        stemmed_tokens.remove('sp')\n",
    "    while 'n' in stemmed_tokens:\n",
    "        stemmed_tokens.remove('n')\n",
    "    \"\"\"\n",
    "    # add tokens to list\n",
    "    texts.append(lemmed_tokens)\n",
    "    index += 1\n",
    "\n",
    "\n",
    "texts, remove_word = remove_frequent_words(texts, 100)\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "\"finish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'finish'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle.dump(corpus, open('../data/'+FOLDER+'/corpus.pkl', 'wb'))\n",
    "\"finish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'finish'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle.dump(index_set, open('../data/'+FOLDER+'/index_set.pkl', 'wb'))\n",
    "\"finish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'finish'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=50, id2word = dictionary, passes=20,iterations=300)\n",
    "\"finish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'finish'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle.dump(ldamodel, open('../data/'+FOLDER+'/LDA_model_50.pkl', 'wb'))\n",
    "\"finish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'finish'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.print_topics(num_topics=50, num_words=10)\n",
    "\"finish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'finish'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_list = []\n",
    "for corp in corpus:\n",
    "    lad_corpu = ldamodel.get_document_topics(corp, minimum_probability=0.00001)\n",
    "    l = [0 for i in range(50)]\n",
    "    for tup in lad_corpu:\n",
    "        l[tup[0]] = tup[1]\n",
    "    \n",
    "    corpus_list.append(l)\n",
    "\n",
    "\"finish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3876"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "similarity_dict = {}\n",
    "c_len = len(corpus_list)\n",
    "for i in range(c_len):\n",
    "    for j in range(c_len):\n",
    "        if i < j:\n",
    "            simi = 1 - spatial.distance.cosine(corpus_list[i], corpus_list[j])\n",
    "            similarity_dict[(i,j)] = simi\n",
    "\"\"\"\n",
    "\"finish\"           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for key in similarity_dict:\n",
    "    if similarity_dict[key] >0.999:\n",
    "        print(key, similarity_dict[key])\n",
    "\"\"\"\n",
    "\"finish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index = 0\n",
    "for corp in corpus:\n",
    "    lad_corpu = ldamodel.get_document_topics(corp)\n",
    "    for tup in lad_corpu:\n",
    "        if tup[0] == 2 and tup[1] > 0.5:\n",
    "            doc = doc_set[index]\n",
    "            print(index, lad_corpu)\n",
    "    index += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ldamodel = pickle.load(open('../data/'+FOLDER+'/LDA_model_50.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.032*\"child\" + 0.017*\"life\" + 0.016*\"mother\" + 0.016*\"boy\" + 0.014*\"young\" + 0.013*\"baby\" + 0.012*\"woman\" + 0.010*\"father\" + 0.009*\"parent\" + 0.009*\"find\"'),\n",
       " (1,\n",
       "  '0.018*\"find\" + 0.015*\"evil\" + 0.012*\"must\" + 0.011*\"world\" + 0.010*\"help\" + 0.008*\"save\" + 0.008*\"power\" + 0.007*\"back\" + 0.006*\"year\" + 0.006*\"time\"'),\n",
       " (2,\n",
       "  '0.029*\"gold\" + 0.018*\"treasure\" + 0.013*\"map\" + 0.012*\"tribe\" + 0.009*\"find\" + 0.009*\"cave\" + 0.009*\"ancient\" + 0.008*\"lost\" + 0.008*\"mountain\" + 0.007*\"old\"'),\n",
       " (3,\n",
       "  '0.015*\"life\" + 0.011*\"money\" + 0.010*\"friend\" + 0.010*\"love\" + 0.009*\"one\" + 0.008*\"man\" + 0.006*\"want\" + 0.006*\"day\" + 0.006*\"go\" + 0.006*\"woman\"'),\n",
       " (4,\n",
       "  '0.010*\"film\" + 0.010*\"ad\" + 0.010*\"life\" + 0.009*\"world\" + 0.007*\"music\" + 0.006*\"dream\" + 0.005*\"sea\" + 0.005*\"lifeboat\" + 0.005*\"man\" + 0.005*\"work\"'),\n",
       " (5,\n",
       "  '0.008*\"airport\" + 0.007*\"plane\" + 0.007*\"lead\" + 0.007*\"firm\" + 0.006*\"team\" + 0.005*\"run\" + 0.005*\"belief\" + 0.005*\"another\" + 0.005*\"law\" + 0.005*\"political\"'),\n",
       " (6,\n",
       "  '0.014*\"prison\" + 0.011*\"bank\" + 0.011*\"two\" + 0.009*\"time\" + 0.009*\"go\" + 0.008*\"find\" + 0.007*\"robbery\" + 0.007*\"money\" + 0.006*\"car\" + 0.006*\"way\"'),\n",
       " (7,\n",
       "  '0.018*\"singer\" + 0.011*\"brother\" + 0.010*\"woman\" + 0.010*\"spider\" + 0.008*\"together\" + 0.007*\"life\" + 0.007*\"female\" + 0.007*\"club\" + 0.007*\"business\" + 0.006*\"opera\"'),\n",
       " (8,\n",
       "  '0.011*\"monster\" + 0.008*\"one\" + 0.008*\"creature\" + 0.007*\"come\" + 0.006*\"life\" + 0.006*\"wish\" + 0.005*\"time\" + 0.005*\"first\" + 0.005*\"woman\" + 0.005*\"make\"'),\n",
       " (9,\n",
       "  '0.021*\"party\" + 0.017*\"apartment\" + 0.015*\"building\" + 0.014*\"guest\" + 0.012*\"friend\" + 0.009*\"night\" + 0.009*\"dinner\" + 0.008*\"roommate\" + 0.007*\"professor\" + 0.006*\"invite\"'),\n",
       " (10,\n",
       "  '0.013*\"town\" + 0.012*\"murder\" + 0.012*\"find\" + 0.010*\"woman\" + 0.009*\"man\" + 0.009*\"kill\" + 0.009*\"death\" + 0.009*\"dead\" + 0.008*\"young\" + 0.008*\"body\"'),\n",
       " (11,\n",
       "  '0.040*\"killer\" + 0.027*\"serial\" + 0.012*\"victim\" + 0.011*\"mental\" + 0.009*\"one\" + 0.008*\"doll\" + 0.008*\"body\" + 0.007*\"institution\" + 0.007*\"mind\" + 0.007*\"time\"'),\n",
       " (12,\n",
       "  '0.089*\"ship\" + 0.047*\"crew\" + 0.021*\"sea\" + 0.019*\"board\" + 0.017*\"submarine\" + 0.012*\"boat\" + 0.012*\"ocean\" + 0.010*\"vessel\" + 0.010*\"puppet\" + 0.010*\"art\"'),\n",
       " (13,\n",
       "  '0.039*\"alien\" + 0.025*\"planet\" + 0.018*\"space\" + 0.017*\"di\" + 0.014*\"find\" + 0.012*\"e\" + 0.011*\"human\" + 0.010*\"earth\" + 0.008*\"year\" + 0.007*\"comet\"'),\n",
       " (14,\n",
       "  '0.025*\"school\" + 0.016*\"friend\" + 0.011*\"girl\" + 0.011*\"student\" + 0.010*\"high\" + 0.010*\"year\" + 0.009*\"boy\" + 0.009*\"find\" + 0.009*\"father\" + 0.009*\"old\"'),\n",
       " (15,\n",
       "  '0.015*\"life\" + 0.011*\"research\" + 0.009*\"begin\" + 0.008*\"one\" + 0.007*\"find\" + 0.007*\"people\" + 0.007*\"man\" + 0.006*\"world\" + 0.006*\"something\" + 0.006*\"see\"'),\n",
       " (16,\n",
       "  '0.012*\"black\" + 0.009*\"make\" + 0.008*\"gangster\" + 0.007*\"white\" + 0.007*\"group\" + 0.007*\"team\" + 0.006*\"daughter\" + 0.006*\"local\" + 0.005*\"old\" + 0.005*\"family\"'),\n",
       " (17,\n",
       "  '0.034*\"team\" + 0.016*\"coach\" + 0.016*\"player\" + 0.013*\"game\" + 0.011*\"basketball\" + 0.010*\"two\" + 0.009*\"union\" + 0.007*\"baseball\" + 0.006*\"strike\" + 0.005*\"find\"'),\n",
       " (18,\n",
       "  '0.018*\"ape\" + 0.012*\"jungle\" + 0.011*\"pimp\" + 0.010*\"two\" + 0.009*\"film\" + 0.008*\"night\" + 0.008*\"back\" + 0.007*\"time\" + 0.007*\"want\" + 0.006*\"nun\"'),\n",
       " (19,\n",
       "  '0.013*\"martial\" + 0.011*\"brain\" + 0.011*\"show\" + 0.011*\"art\" + 0.011*\"tournament\" + 0.008*\"warrior\" + 0.008*\"movie\" + 0.008*\"troupe\" + 0.007*\"world\" + 0.007*\"one\"'),\n",
       " (20,\n",
       "  '0.040*\"computer\" + 0.037*\"train\" + 0.013*\"system\" + 0.012*\"satellite\" + 0.011*\"world\" + 0.008*\"butler\" + 0.008*\"control\" + 0.008*\"hacker\" + 0.007*\"agent\" + 0.007*\"security\"'),\n",
       " (21,\n",
       "  '0.010*\"one\" + 0.009*\"night\" + 0.007*\"world\" + 0.007*\"friend\" + 0.007*\"love\" + 0.007*\"back\" + 0.006*\"life\" + 0.006*\"carnival\" + 0.006*\"cousin\" + 0.005*\"day\"'),\n",
       " (22,\n",
       "  '0.023*\"police\" + 0.016*\"cop\" + 0.015*\"murder\" + 0.013*\"find\" + 0.012*\"man\" + 0.011*\"one\" + 0.011*\"kill\" + 0.011*\"drug\" + 0.011*\"crime\" + 0.010*\"criminal\"'),\n",
       " (23,\n",
       "  '0.011*\"store\" + 0.009*\"one\" + 0.009*\"casino\" + 0.009*\"job\" + 0.008*\"two\" + 0.007*\"day\" + 0.007*\"game\" + 0.007*\"town\" + 0.006*\"girlfriend\" + 0.006*\"want\"'),\n",
       " (24,\n",
       "  '0.038*\"love\" + 0.031*\"life\" + 0.017*\"sister\" + 0.015*\"brother\" + 0.014*\"fall\" + 0.014*\"woman\" + 0.013*\"young\" + 0.011*\"family\" + 0.010*\"man\" + 0.008*\"one\"'),\n",
       " (25,\n",
       "  '0.025*\"war\" + 0.012*\"men\" + 0.009*\"life\" + 0.007*\"officer\" + 0.007*\"camp\" + 0.007*\"battle\" + 0.007*\"military\" + 0.006*\"one\" + 0.006*\"escape\" + 0.006*\"fight\"'),\n",
       " (26,\n",
       "  '0.019*\"weekend\" + 0.017*\"spy\" + 0.013*\"agent\" + 0.010*\"wagon\" + 0.009*\"father\" + 0.008*\"bird\" + 0.008*\"life\" + 0.007*\"comic\" + 0.006*\"find\" + 0.005*\"autistic\"'),\n",
       " (27,\n",
       "  '0.016*\"friend\" + 0.015*\"love\" + 0.011*\"life\" + 0.010*\"fall\" + 0.010*\"meet\" + 0.009*\"woman\" + 0.009*\"show\" + 0.008*\"find\" + 0.008*\"work\" + 0.007*\"two\"'),\n",
       " (28,\n",
       "  '0.013*\"memory\" + 0.010*\"wife\" + 0.009*\"ghost\" + 0.006*\"love\" + 0.006*\"find\" + 0.006*\"new\" + 0.006*\"fisherman\" + 0.005*\"end\" + 0.005*\"man\" + 0.005*\"see\"'),\n",
       " (29,\n",
       "  '0.025*\"life\" + 0.018*\"music\" + 0.011*\"musical\" + 0.009*\"musician\" + 0.008*\"song\" + 0.008*\"concert\" + 0.007*\"love\" + 0.006*\"documentary\" + 0.006*\"many\" + 0.006*\"culture\"'),\n",
       " (30,\n",
       "  '0.014*\"dog\" + 0.012*\"life\" + 0.010*\"friend\" + 0.010*\"young\" + 0.010*\"boy\" + 0.010*\"find\" + 0.009*\"town\" + 0.006*\"wolf\" + 0.006*\"help\" + 0.005*\"home\"'),\n",
       " (31,\n",
       "  '0.021*\"case\" + 0.013*\"village\" + 0.012*\"lawyer\" + 0.011*\"defend\" + 0.009*\"border\" + 0.009*\"three\" + 0.009*\"judge\" + 0.009*\"trial\" + 0.008*\"story\" + 0.008*\"samurai\"'),\n",
       " (32,\n",
       "  '0.010*\"crash\" + 0.009*\"plane\" + 0.008*\"team\" + 0.007*\"rescue\" + 0.007*\"survivor\" + 0.006*\"passenger\" + 0.006*\"crew\" + 0.006*\"pilot\" + 0.005*\"scientist\" + 0.005*\"escape\"'),\n",
       " (33,\n",
       "  '0.019*\"camp\" + 0.015*\"bus\" + 0.010*\"escape\" + 0.010*\"shark\" + 0.008*\"try\" + 0.008*\"con\" + 0.008*\"truck\" + 0.007*\"group\" + 0.007*\"young\" + 0.007*\"50\"'),\n",
       " (34,\n",
       "  '0.039*\"family\" + 0.029*\"son\" + 0.022*\"father\" + 0.013*\"daughter\" + 0.011*\"life\" + 0.009*\"town\" + 0.009*\"year\" + 0.009*\"wife\" + 0.008*\"house\" + 0.008*\"mother\"'),\n",
       " (35,\n",
       "  '0.026*\"story\" + 0.017*\"tale\" + 0.011*\"fairy\" + 0.011*\"prince\" + 0.009*\"meet\" + 0.009*\"land\" + 0.008*\"father\" + 0.008*\"life\" + 0.007*\"based\" + 0.007*\"follows\"'),\n",
       " (36,\n",
       "  '0.078*\"gang\" + 0.017*\"robot\" + 0.013*\"leader\" + 0.012*\"member\" + 0.011*\"street\" + 0.010*\"group\" + 0.010*\"new\" + 0.009*\"drug\" + 0.007*\"production\" + 0.006*\"join\"'),\n",
       " (37,\n",
       "  '0.023*\"band\" + 0.016*\"rock\" + 0.015*\"slave\" + 0.011*\"singer\" + 0.008*\"film\" + 0.008*\"day\" + 0.007*\"year\" + 0.006*\"group\" + 0.006*\"town\" + 0.005*\"find\"'),\n",
       " (38,\n",
       "  '0.021*\"human\" + 0.010*\"scientist\" + 0.008*\"mutant\" + 0.008*\"base\" + 0.008*\"people\" + 0.007*\"device\" + 0.007*\"child\" + 0.006*\"disease\" + 0.006*\"virus\" + 0.006*\"find\"'),\n",
       " (39,\n",
       "  '0.013*\"mission\" + 0.011*\"terrorist\" + 0.011*\"one\" + 0.009*\"sent\" + 0.009*\"nuclear\" + 0.008*\"must\" + 0.008*\"agent\" + 0.008*\"war\" + 0.007*\"soldier\" + 0.007*\"kill\"'),\n",
       " (40,\n",
       "  '0.022*\"witch\" + 0.018*\"immortal\" + 0.016*\"gorilla\" + 0.009*\"magic\" + 0.008*\"settler\" + 0.008*\"sailor\" + 0.008*\"spell\" + 0.008*\"dream\" + 0.008*\"power\" + 0.007*\"magical\"'),\n",
       " (41,\n",
       "  '0.014*\"news\" + 0.011*\"program\" + 0.009*\"network\" + 0.008*\"one\" + 0.007*\"two\" + 0.005*\"rating\" + 0.005*\"virginity\" + 0.004*\"poacher\" + 0.004*\"build\" + 0.004*\"high\"'),\n",
       " (42,\n",
       "  '0.018*\"vampire\" + 0.012*\"accused\" + 0.012*\"murder\" + 0.011*\"man\" + 0.009*\"innocence\" + 0.009*\"one\" + 0.009*\"year\" + 0.008*\"find\" + 0.008*\"death\" + 0.007*\"story\"'),\n",
       " (43,\n",
       "  '0.054*\"island\" + 0.030*\"park\" + 0.018*\"food\" + 0.017*\"dinosaur\" + 0.014*\"company\" + 0.014*\"ant\" + 0.009*\"find\" + 0.008*\"group\" + 0.007*\"go\" + 0.007*\"eating\"'),\n",
       " (44,\n",
       "  '0.013*\"un\" + 0.009*\"film\" + 0.009*\"fear\" + 0.006*\"rabbit\" + 0.006*\"non\" + 0.006*\"writer\" + 0.005*\"young\" + 0.004*\"garbage\" + 0.004*\"charity\" + 0.004*\"corpse\"'),\n",
       " (45,\n",
       "  '0.054*\"drug\" + 0.012*\"pool\" + 0.011*\"life\" + 0.010*\"dealer\" + 0.009*\"clinic\" + 0.009*\"addict\" + 0.009*\"heroin\" + 0.007*\"pill\" + 0.006*\"trapped\" + 0.006*\"earthquake\"'),\n",
       " (46,\n",
       "  '0.010*\"film\" + 0.008*\"farmer\" + 0.008*\"farm\" + 0.007*\"mouse\" + 0.007*\"tomb\" + 0.006*\"demon\" + 0.006*\"field\" + 0.006*\"generation\" + 0.006*\"hell\" + 0.005*\"ball\"'),\n",
       " (47,\n",
       "  '0.022*\"movie\" + 0.019*\"film\" + 0.017*\"life\" + 0.010*\"director\" + 0.008*\"star\" + 0.007*\"time\" + 0.007*\"book\" + 0.007*\"work\" + 0.007*\"job\" + 0.007*\"sex\"'),\n",
       " (48,\n",
       "  '0.011*\"life\" + 0.010*\"wife\" + 0.008*\"man\" + 0.007*\"one\" + 0.007*\"two\" + 0.007*\"year\" + 0.007*\"also\" + 0.006*\"want\" + 0.006*\"time\" + 0.006*\"affair\"'),\n",
       " (49,\n",
       "  '0.015*\"daughter\" + 0.011*\"house\" + 0.009*\"de\" + 0.008*\"husband\" + 0.008*\"find\" + 0.008*\"gay\" + 0.008*\"young\" + 0.008*\"mansion\" + 0.007*\"help\" + 0.007*\"home\"')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.print_topics(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lmid_2_id = pickle.load(open('../data/'+FOLDER+'/lmid_2_id.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'finish'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_dict = {}\n",
    "for i in range(len(index_set)):\n",
    "    try:\n",
    "        real_id = lmid_2_id[re.split('\\W+', index_set[i])[1]]\n",
    "        topic_dict[real_id] = ldamodel[corpus[i]]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "\"finish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'finish'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle.dump(topic_dict, open('../data/'+FOLDER+'/topic_dict.pkl', 'wb'))\n",
    "\"finish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ws3.6",
   "language": "python",
   "name": "ws"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
